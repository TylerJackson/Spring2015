{
 "metadata": {
  "name": "",
  "signature": "sha256:a08632d396c5888c45e1ad6e7709f402adbbeaee64f151ff74952757cb1be868"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Enter Team Member Names here (double click to edit):\n",
      "\n",
      "- Name 1: Tyler Jackson\n",
      "- Name 2:\n",
      "- Name 3:\n",
      "\n",
      "________\n",
      "\n",
      "#In Class Assignment Two\n",
      "In the following assignment you will be asked to fill in python code and derivations for a number of different problems. Please read all instructions carefully and turn in the rendered notebook (.ipynb file, remember to save it!!) or HTML of the rendered notebook to blackboard before the end of class.\n",
      "\n",
      "**Distance Students**: please finish this assignment in 1 hour and 30 minutes. Turn in before next class per the instructions on blackboard.\n",
      "\n",
      "________________________________________________________________________________________________________\n",
      "\n",
      "##Loading the Classification Data\n",
      "Please run the following code to read in the \"digits\" dataset from sklearn's data loading module."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import load_digits\n",
      "import numpy as np\n",
      "\n",
      "ds = load_digits()\n",
      "\n",
      "# this holds the continuous feature data\n",
      "print 'features shape:', ds.data.shape # there are 1797 instances and 64 features per instance\n",
      "print 'target shape:', ds.target.shape \n",
      "print 'range of target:', np.min(ds.target),np.max(ds.target)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "features shape: (1797, 64)\n",
        "target shape: (1797,)\n",
        "range of target: 0 9\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "________________________________________________________________________________________________________\n",
      "\n",
      "##Using Decision Trees\n",
      "In the videos, we talked about the splitting conditions for different attributes. Specifically, we discussed the number of ways in which it is possible to split a node, depending on the attribute types. To understand the possible splits, we need to understand the attributes. You might find the description in the `ds['DESCR']` field to be useful. You can see the field using `print ds['DESCR'] `\n",
      "\n",
      "**Question 1:** For the digits dataset, what are the type(s) of the attributes? How many attributes are there? What do they represent?\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Enter your comments here\n",
      "print ds['DESCR']\n",
      " \n",
      "#These attributes are \n",
      "#These attributes are numerical and nominal.  They are discrete, and the order does not matter.  I was not completely sure if\n",
      "#order mattered here, but I was under the impression that the integer values in the range 0 to 16 represented there place in\n",
      "#the 4x4 grid.  There place is important, but one spot doesn't rank higher than another.  There are 64 attributes.  \n",
      "## Enter comments here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Optical Recognition of Handwritten Digits Data Set\n",
        "\n",
        "Notes\n",
        "-----\n",
        "Data Set Characteristics:\n",
        "    :Number of Instances: 5620\n",
        "    :Number of Attributes: 64\n",
        "    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\n",
        "    :Missing Attribute Values: None\n",
        "    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n",
        "    :Date: July; 1998\n",
        "\n",
        "This is a copy of the test set of the UCI ML hand-written digits datasets\n",
        "http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n",
        "\n",
        "The data set contains images of hand-written digits: 10 classes where\n",
        "each class refers to a digit.\n",
        "\n",
        "Preprocessing programs made available by NIST were used to extract\n",
        "normalized bitmaps of handwritten digits from a preprinted form. From a\n",
        "total of 43 people, 30 contributed to the training set and different 13\n",
        "to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n",
        "4x4 and the number of on pixels are counted in each block. This generates\n",
        "an input matrix of 8x8 where each element is an integer in the range\n",
        "0..16. This reduces dimensionality and gives invariance to small\n",
        "distortions.\n",
        "\n",
        "For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\n",
        "T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\n",
        "L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n",
        "1994.\n",
        "\n",
        "References\n",
        "----------\n",
        "  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n",
        "    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n",
        "    Graduate Studies in Science and Engineering, Bogazici University.\n",
        "  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n",
        "  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n",
        "    Linear dimensionalityreduction using relevance weighted LDA. School of\n",
        "    Electrical and Electronic Engineering Nanyang Technological University.\n",
        "    2005.\n",
        "  - Claudio Gentile. A New Approximate Maximal Margin Classification\n",
        "    Algorithm. NIPS. 2000.\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "___\n",
      "##Using the gini coefficient\n",
      "We talked about the gini index in the videos.  The gini coefficient for a given split is given by:\n",
      "$$Gini=\\sum_{t=1}^T \\frac{n_t}{N}gini(t)$$\n",
      "where $T$ is the total number splits (2 for binary attributes), $n_t$ is the number of instances in node $t$ after splitting, and $N$ is the total number of instances in the parent node. $gini(t)$ is the gini index for the resulting node and is given by:\n",
      "$$gini(t)=1-\\sum_{j=0}^{C-1} p(j|t)^2$$\n",
      "where $C$ is the total number of possible classes and $p(j|t)$ is the count of instances belonging to class $j$ in node $t$, normalized by the total number of instances in node $t$ (i.e., the probability of class $j$ in node $t$).\n",
      "\n",
      "For the given dataset, $gini(t)$ has been programmed for you in the function `gini_index`. To use the function, pass in a `numpy` array of the class values for a given node and the gini will be returned. In the example below, the function is used calculate the gini for splitting the dataset on feature 28, with value 2.5. \n",
      "- First all the target classes for a split are calculated using `numpy` indexing `ds.target[feature28>2.5]` \n",
      " - Note: this grabs all the classes which have feature28 greater than 2.5\n",
      "- Second, those classes are passed into the function to get the gini for going right in the tree (i.e., feature 28 being greater than the threshold 2.5). \n",
      " - `gini_r = gini_index(ds.target[feature28>2.5])`\n",
      "- Third, the gini is calculated for going left in the tree. \n",
      "     - `gini_l = gini_index(ds.target[feature28<=2.5])`\n",
      "- Combining the gini indices is left as an exercise in the next section"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# compute the gini of several examples for the starting dataset\n",
      "\n",
      "def gini_index(classes_in_split):\n",
      "    # pay no attention to this code -- it just computes the gini for a given split \n",
      "    classes_in_split = np.reshape(classes_in_split,(len(classes_in_split),-1))\n",
      "    unique_classes = np.unique(classes_in_split)\n",
      "    gini = 1\n",
      "    for c in unique_classes:\n",
      "        gini -= (np.sum(classes_in_split==c) / float(len(classes_in_split)))**2\n",
      "        \n",
      "    return gini\n",
      "    \n",
      "# get the value for this feature as a column vector \n",
      "# (this is like grabbing one column of the record table)\n",
      "feature28 = ds.data[:,28]\n",
      "\n",
      "# if we split on the value of 2.5, then this is the gini for each resulting node:\n",
      "gini_r = gini_index(ds.target[feature28>2.5])\n",
      "gini_l = gini_index(ds.target[feature28<=2.5])\n",
      "\n",
      "# compute gini example. This splits on attribute '28' with a value of 2.5\n",
      "print 'gini for right node of split:', gini_r\n",
      "print 'gini for left node of split:', gini_l"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "gini for right node of split: 0.884585786767\n",
        "gini for left node of split: 0.711540756654\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Question 2:** Now, using the above values `gini_r` and `gini_l`. Calculate the combined Gini for the entire split. You will need to write the weighted summation (based upon the number of instances inside each node). To count the number of instances greater than a value using numpy, you can write `sum(some_array>5)` where `some_array` is an array of values and we want to now how many are greater than `5`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Enter your code here\n",
      "n_r = float(sum(ds.target[feature28>2.5]))\n",
      "n_l = float(sum(ds.target[feature28<=2.5]))\n",
      "N = n_l+n_r\n",
      "Gini_split = n_r/N*gini_r + (n_l/N)*gini_l   \n",
      "print 'Instances in left:', n_l\n",
      "print 'Instances in right:', n_r\n",
      "print 'Instances in parent Node:', N\n",
      "print 'Gini_split:', Gini_split\n",
      "\n",
      "## Enter your code here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Instances in left: 1157.0\n",
        "Instances in right: 6913.0\n",
        "Instances in parent Node: 8070.0\n",
        "Gini_split: 0.859776232883\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "___\n",
      "**Question 3:** Now we want to know which is a better split, `feature28 -> 2.5` or `feature28 -> 10`.  Enter your code to find the total gini of splitting on the threshold of 10 and compare it to the threshold for 2.5 for feature 28 (saved to variable `feature28`). According to gini, which threshold is better, 2.5 or 10.0?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Enter your code here\n",
      "\n",
      "# if we split on the value of 10, then this is the gini for each resulting node:\n",
      "gini_r_10 = gini_index(ds.target[feature28>10.0])\n",
      "gini_l_10 = gini_index(ds.target[feature28<=10.0])\n",
      "# compute gini example. This splits on attribute '28' with a value of 10.0\n",
      "print 'gini for right node of split:', gini_r\n",
      "print 'gini for left node of split:', gini_l\n",
      "\n",
      "n_r_10 = float(sum(ds.target[feature28>2.5]))\n",
      "n_l_10 = float(sum(ds.target[feature28<=2.5]))\n",
      "N_10 = n_l_10+n_r_10\n",
      "Gini_split_10 = n_r_10/N_10*gini_r_10 + n_l_10/N_10*gini_l_10   \n",
      "print 'Instances in left:', n_l_10\n",
      "print 'Instances in right:', n_r_10\n",
      "print 'Instances in parent Node:', N_10\n",
      "print 'Gini_split on 10:', Gini_split_10\n",
      "print 'Gini_split on 2.5:', Gini_split\n",
      "\n",
      "# The gini_split value that is lower is better because this means the data is more impure in that split.  Therefore splitting\n",
      "# with the threshold at 2.5 is slightly better according to gini.\n",
      "# Enter your code here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "gini for right node of split: 0.884585786767\n",
        "gini for left node of split: 0.711540756654\n",
        "Instances in left: 1157.0\n",
        "Instances in right: 6913.0\n",
        "Instances in parent Node: 8070.0\n",
        "Gini_split on 10: 0.870265016944\n",
        "Gini_split on 2.5: 0.859776232883\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "___\n",
      "## Entropy based splitting\n",
      "We discussed entropy as well in the video as another means of splitting. We calculated entropy for a node $t$ by:\n",
      "$$ Entropy(t) = -\\sum p(j|t) \\log p(j|t) $$\n",
      "where $p(j|t)$ is the same as above. To combine Entropy measures from a set of nodes, t = {1,...,T} we use: \n",
      "$$Entropy_{split}=\\sum_{t=1}^T \\frac{n_t}{N}Entropy(t)$$ \n",
      "where $n_t$ and $N$ are the same as defined above for the $Gini$. Information gain is calculated by subtracting the Entropy of the split from the Entropy of the parent node before splitting:\n",
      "$$InfoGain = Entropy(p)-Entropy_{split}$$\n",
      "where $p$ is the parent node before splitting. You are given an equation for calculating the $Entropy(t)$ of  node $t$. It works exactly like the `gini_index` function above, but is named `entropy_value`. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def entropy_value(classes_in_split):\n",
      "    # pay no attention to this code -- it just computes the gini for a given split \n",
      "    classes_in_split = np.reshape(classes_in_split,(len(classes_in_split),-1))\n",
      "    unique_classes = np.unique(classes_in_split)\n",
      "    ent = 0\n",
      "    for c in unique_classes:\n",
      "        p = (np.sum(classes_in_split==c) / float(len(classes_in_split)))\n",
      "        ent += p * np.log(p)\n",
      "        \n",
      "    return -ent\n",
      "\n",
      "ent_r = entropy_value(ds.target[feature28>2.5])\n",
      "ent_l = entropy_value(ds.target[feature28<=2.5])\n",
      "\n",
      "# compute entropy example. This splits on attribute '28' with a value of 2.5\n",
      "print 'entropy for right node of split:', ent_r\n",
      "print 'entropy for left node of split:', ent_l"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "entropy for right node of split: 2.18369753782\n",
        "entropy for left node of split: 1.48988814128\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "___\n",
      "**Question 4:** Calculate the information gain of the split when the threshold is 2.5 on `feature28`. What is the value of the information gain?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Enter your code here\n",
      "ent_parent = entropy_value(ds.target)\n",
      "ent_split = n_l/N*ent_l + n_r/N*ent_r\n",
      "Info_gain = ent_parent - ent_split\n",
      "\n",
      "print 'Entropy Split:', ent_split\n",
      "print 'Entropy parent:', ent_parent\n",
      "print 'InfoGain:', Info_gain\n",
      "\n",
      "# Enter your code here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Entropy Split: 2.08422573215\n",
        "Entropy parent: 2.30247922097\n",
        "InfoGain: 0.218253488822\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Question 5:** What is the information gain if the threshold is 10.0 on `feature28`? According to information gain, is it better to split on a threshold of 2.5 or 10? Does entropy give the same decision as gini for this example?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Enter your code here\n",
      "ent_r_10 = entropy_value(ds.target[feature28>10])\n",
      "ent_l_10 = entropy_value(ds.target[feature28<=10])\n",
      "\n",
      "# Enter your code here\n",
      "ent_parent_10 = entropy_value(ds.target)\n",
      "ent_split_10 = n_l_10/N_10*ent_l_10 + n_r_10/N_10*ent_r_10\n",
      "Info_gain_10 = ent_parent_10 - ent_split_10\n",
      "\n",
      "print 'Entropy Split on 10:', ent_split_10\n",
      "print 'Entropy parent on 10:', ent_parent_10\n",
      "print 'InfoGain on 10:', Info_gain_10\n",
      "print 'InfoGain on 2.5:', Info_gain\n",
      "\n",
      "# Enter your code here\n",
      "#According to information gain, it is better to split on 2.5.  We want to choose the split that results in the most information\n",
      "# gained as this implies a larger reduction.  In this case entropy and gini give the same decision.\n",
      "\n",
      "\n",
      "# Enter your code here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Entropy Split on 10: 2.10574108975\n",
        "Entropy parent on 10: 2.30247922097\n",
        "InfoGain on 10: 0.196738131219\n",
        "InfoGain on 2.5: 0.218253488822\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "___\n",
      "##Information gain and multi-way splitting\n",
      "Now assume that we can use not just a binary split, but a three way split. \n",
      "\n",
      "**Question 6** What is the information gain if we split feature28 on two thesholds (three separate nodes corresponding to three branches from one node) \n",
      "- node left: `feature28<2.5`, \n",
      "- node middle: `2.5<=feature28<10`, and \n",
      "- node right: `10<=feature28`? \n",
      "\n",
      "Is the information gain better? \n",
      "\n",
      "***Note***: You can index into a numpy array with the following notation: `some_array[(2.5<=feature28) & (feature28<10.0)]`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Enter your code here\n",
      "# Enter your code here\n",
      "ent_first = entropy_value(ds.target[feature28<2.5])\n",
      "ent_second = entropy_value(ds.target[(2.5<=feature28) & (feature28<10.0)])\n",
      "ent_third = entropy_value(ds.target[10<=feature28])\n",
      "# Enter your code here\n",
      "\n",
      "n_first = float(sum(ds.target[feature28<2.5]))\n",
      "n_second = float(sum(ds.target[(2.5<=feature28) & (feature28<10.0)]))\n",
      "n_third = float(sum(ds.target[10<=feature28]))\n",
      "\n",
      "ent_parent_multi = entropy_value(ds.target)\n",
      "ent_split_multi = n_first/N*ent_first + n_second/N*ent_second + n_third/N*ent_third\n",
      "Info_gain_multi = ent_parent_multi - ent_split_multi\n",
      "\n",
      "print 'Entropy Multi split:', ent_split_multi\n",
      "print 'Entropy parent:', ent_parent_multi\n",
      "print 'InfoGain on multi split:', Info_gain_multi\n",
      "print 'InfoGain on 10:', Info_gain_10\n",
      "print 'InfoGain on 2.5:', Info_gain\n",
      "\n",
      "\n",
      "#As you can see the information gain is much better on a multi way split.  As mentioned earlier we want to maximize the\n",
      "#information gain.\n",
      "\n",
      "\n",
      "# Enter your code here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Entropy Multi split: 2.03549670063\n",
        "Entropy parent: 2.30247922097\n",
        "InfoGain on multi split: 0.266982520339\n",
        "InfoGain on 10: 0.196738131219\n",
        "InfoGain on 2.5: 0.218253488822\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Question 7**: Should we normalize the quantity that we just calculated if we want to compare it to the information gain of a binary split? Why or Why not?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Enter your code here\n",
      "#If we normalized it that would penalize the information gain of the three way split which tends to favor more partitions.  \n",
      "#This doesn't seem like it would be too necessary here as we are only talking about one more split compared to binary.  If\n",
      "#we had a lot of splits then it would probably be more practical.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Enter your code here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "___\n",
      "##Decision Trees in scikit-learn\n",
      "Scikit-learn also has an implementation of decision trees. Its available here:\n",
      "- http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\n",
      "\n",
      "**Question 8**: What algorithm does scikit-learn use for creating decision trees (i.e., ID3, C4.5, C5.0, CART, MARS, CHAID, etc.)? "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Enter your answer here\n",
      "#It uses an optimized version of the CART algorithm\n",
      "\n",
      "# Enter your answer here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "___\n",
      "**Question 9**: Using the documentation, use scikit-learn to train a decision tree on the digits data. Calculate the accuracy on the training data. What is the accuracy? Did you expect the decision tree to have this kind of accuracy? Why or Why not?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# use scikit learn to train a decision tree\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "# enter your code below here to train and predict on the same data\n",
      "clf = DecisionTreeClassifier()\n",
      "clf = clf.fit(ds.data, ds.target)\n",
      "yhat = clf.predict(ds.data)\n",
      "\n",
      "# enter your code above here\n",
      "\n",
      "from sklearn.metrics import accuracy_score \n",
      "\n",
      "# enter your code below here to calculate accuracy\n",
      "acc = accuracy_score(ds.target,yhat)\n",
      "\n",
      "print 'accuracy:', acc\n",
      "print 'I did/did not expect... it to have perfect accuracy.  Maybe I did something wrong?'\n",
      "# enter your code above here\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "accuracy: 1.0\n",
        "I did/did not expect... it to have perfect accuracy.  Maybe I did something wrong?\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "___\n",
      "**Question 10**: Look at the other input variables to the function `DecisionTreeClassifier` could any of them be used to help prevent the decision tree from overlearning on the data? Which variables might we use to control overfitting and how (explain each)?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Enter your answer here\n",
      "#Setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree can be used to prevent \n",
      "#overlearning.  This prevents the tree from creating a really complex model where every box has only one class.  These \n",
      "#attributes are max_depth and min_samples_leaf.  By default max_depth is set to none and min_samples_leaf is set to 1.  So \n",
      "#on second thought it actually makes perfect sense that the model would way overfit and create a model with accuracy 1.\n",
      "\n",
      "\n",
      "# Enter your answer here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "________________________________________________________________________________________________________\n",
      "\n",
      "That's all! Please **upload your rendered notebook to blackboard** and please include **team member names** in the notebook submission."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}