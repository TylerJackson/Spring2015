CSE 7331 Data Mining Lecture Notes

------------------------------------------------------

Data Mining Lecture 1 Notes

Labs = 75% of grade
-when labs are due, no class that week, but you have 2 periods to just work on the lab
-high expectations
-comments, analysis, conclusion
-no late labs
In Class Assignment = 25% of grade
-5 assignments at 5% each

-Dr Larson, Professor Larson, prefers Eric

-Email Name, department, grad/ugrad
-something true/false

What is Data Mining?
-non-trivial extraction of implicit, previously unknown, and potentially useful information from data
-Exploration and analysis by automatic or semi-automatic means over large quantities of data in order to discover meaningful patterns

What is not Data Mining?
-looking up phone number in directory, or querying a DB or web search engine

What is Data Mining?
-certain names are more prevalent in certain US locations (O'brien, O'Rurke, O'Reilly... in Boston area)
-group together similar documents returned by search engine according to their context (e.g. Amazon rainforest)
-an algorithm that can group articles by looking at text and throwing them in a different bin accordingly


Data Mining Tasks
-Prediction
	-use some variables to predict unknown or future values of other variables
-Description
	-find human-interpretable patterns that describe the data
-Tasks
	-Classification (predictive)
	-Regression (predictive)
	-Deviation Detection (predictive)
	-Clustering (descriptive)
	-Association Rule Discovery (descriptive)
	-Sequential Pattern Discovery (descriptive)

icaggle

Classification
-Given a collection of records (training set)
	-each record contains a set of attributes, one of the attributes is the class
-find a model for class attribute as a function of the values of other attributes
-Goal: previously unseen records should be assigned a class as accurately as possible

Classification: Application 1
-Direct Marketing
-Goal: Reduce cost of mailing by targeting a set of consumers likely to buy a new cell-phone product
Approach:
	-use the data for a similar product introduced before
	-(buy, dont buy) decision forms the class attribute
	-Collect various demographic, lifestyle, and company-interaction related information about all such customers
-More data the better

Classification: Application 2
-Sky Survery Cataloging
-Goal: To predict class (star or galaxy) of sky objects, especially visually faint ones, based on the telescopic survey images (from Palomar observatory)
	-3000 images with 23,040 x 23,040 pixel/image
Approach:
	-segment the image
	-measure image attributes (features) - 40/object
	-model this class based on these features
	-success story: could find 16 new high red-shift quasars, some of the farthest objects that are difficult to find

Regression
-predict a value of a given continuous valued variable based on the values of other variables
-Examples:
	-predicting sales amounts of new product based on advertising expenditure
	-predicting wind velocities as a function of temperature, humidity, air pressure, etc.
	-predicting lung function as a function of gender, weight, height

Clustering Definition
-given a set of data points, each having a set of attributes, and a similarity measure among them, find clusters such that
	-data points in one cluster are more similar to one another
	-data points in separate clusters are less similar to one another
-similarity measures:
	-Euclidean Distance if attributes are continuous.
	-Other Problem: specific measures

Clustering: Application
-Document Clustering:
	-Goal: To find groups of documents that are similar to each other based on the important terms appearing in them
	-Approach: to identify frequently occurring terms in each document. Form a similarity measure based on the frequencies of different terms.
	-Gain: Information retrieval can utilize the clusters to relate a new document or search term to clustered documents
-Clustering Points:
	-3204 Articles of Los Angeles Times
-Similarity Measure
	-How many words are common in these documents (after some word filtering)

Association Rule Discovery: Definition
-Given a set of records each of which contain some number of items from a given collection:
	-produce dependency rules which will predict occurrence of an item based on occurrences of other items
-supermarket shelf management
	-Goal: Identify items that are bought together by sufficiently many customers .
	-Approach: Process the point-of-sale data collection with barcode scanners to find dependencies among items.
		-if a customer buys diapers and milk, then is very likely to buy beer
		-so, don't be surprised if you find six-packs stacked next to diapers

Types of Data and Categorization

What is data?
-Collection of objects and their attributes
-an attribute is a property or characteristic of an object
	Examples: eye color of a person, temperature, etc.
-a collection of attributes describe an object

Table Data
Rows=> referred to as objects, records, points, samples, cases, entities, instances, etc.

Cols=> referred to as Attributes, variables, fields, characteristics, features

Types of Attributes
-Different types of attributes
	Nominal
		Examples: ID numbers, eye color, zip codes
		-like categories, there isn't an order
	Ordinal
		Examples: rankings (e.g. taste of potato chips on a scale from 1-10), grades, height in (tall, medium, short)
		-order
	-Interval
		-Examples: calendar dates, temperatures in Celsius or Farenheit
	-Ratio
		-Examples: temperature in Kelvin, length, time, counts

Properties of Attribute Values
-the type of an attribute depends on which of the following properties it posseses
	-Distinctness:  =, !=
	-Order: 		< >
	-Addition: 		+ -
	-Multiplication: */

	Nominal- distinctness
	Ordinal- distinctness and order
	Interval- distinctness, order, and addition
	Ratio- all 4 properties

			Attribute
			Level		Tranformation		Comments
		 	
		  _	Nominal		Any Permutation		Would 
		 |				of values 			reassigning
Discrete-								all employee ID
		 |								# make a diff.?
		 |								
		  - Ordinal		an order of 	An attribute
		  				preserving 		encompassing
		  				change of 		notion of good,
		  				values. i.e.	better, best can
		  				new = f(old)	be represented
		  				where f is a 	equally well by
		  				monotonic 		the values (1,2,
		  				function		3) or (.5,1,10)

		  	_Interval	new_value = a*	Thus, farenheit
		   |			old_value + b 	and Celsius 
Continuous-				where a,b are 	temperature 
		   |			constants		scales differ in
		   |							terms of where
		   |							their zero value
		   |							is and the size
		   |							of a unit (
		   |							degree)
		   |
		   	-Ratio		new_value=a*	Length can be 
		   				old_value		measured in 
		   								meters or feet

Types of data sets
-Record
	-tables, frequency lists
-Graph
-Ordered
	-spatial data, temporal data, sequential data

----------------------------------------------------------------------------------------------------------------

Data Mining Lecture 2 Notes

1.5 hours for in class assignment.  Turn in what you got

Labels and Feature Vectors-> MachineLearning Algorithm

Data Quality Problems
-noise and outliers
	-remove as much as possible
-missing values
	-replace or ignore
-duplicate data
	-clean entries or merge

Outliers
	-outliers are data objects with characteristics that are considerably different than most of the other data objects in the data set

Missing Values
	-Reasons for missing values
		-information is not collected (e.g. people decline to give their age and weight)
		-Attributes may not be applicable to all cases (e.g. annual income for children)
		-UCI ML Repository: 90% of repositories have missing data
	-Handling missing values
		-eliminate Data objects
		-impute missing values
			-Stats-means,median,mode
			-cluster data without values and then take means,medians,and modes of the clusters
		-ignore the missing value during analysis
		-replace with all possible values (talk about later)

Data Representation
-intervals -> floats
-ratio -> float
-ordinal -> integer
-nominal -> boolean/integer (binary nominal),
			dictionary, one hot encoding

Bag of Words model
-separate out words as their own separate column and include term frequency, or inverse doc frequency

Feature hashing
-have a hashing function h(x) = y
-multiple words mapped to one feature (want to minimize collisions)
-example- take sume of all letters and modulo it by 8

-slide 43 lecture 2 has Panda tutorials

Make sure to have these installed
-pandas, anaconda or whatever
-matplotlib
-seaborn
-mpld3

--------------------------------------------------------

Data Mining Lecture 3 Notes

What is data exploration?
-A preliminary exploration of the data to better understand its characteristics
-key motivations of data exploration include
	-Helping to select the right tool for preprocessing or analysis
	-making use of human's abilities to recognize patterns
		-fact: people can recognize patterns not captured by data analysis tools. Visualization is the key

Techniques used in Data exploration
-in Exploratory Data Analysis (John Tukey)
	-the focus was visualization 
	-Clustering and anomaly detection were viewed as exploratory techniques 
	-In data mining, clustering anomaly detection are major areas of interest, and not thought of as just exploratory
-in our discussion of data exp, focus on 
	1. Summary Statistics
	2. Visualizations

Summary Statistics
	-numbers that summarize data
		mean -location
		standard deviation - spread
	-most summary stats can be calculated in a single pass

Measures of location
-mean and median
-mean is most common used
-mean is sensitive to outliers
-thus, median or trimmed mean is also commonly used

Measures of Spread: Range and Variance
-Range is the difference b.w. the max and min
-variance or st. dev. is the most common measure of spread
	variance(x) = (s_x)^2 = 1/(m-1)SUM(x_i - x_bar)^2
-however, this is also sensitive to outliers, so that other measures are often used
	AAD(x) = 1/m * SUM(abs_val(x_i - x_bar))

Skewness, kurtosis
-Comparison of the tails of a distribution
-negative skew is skew left
-positive skew is skew right

skewness(x) = 1/N * SUM((x_i - x_bar)/st.dev)^3

kurtosis -> skewness but the 4th power instead of the third
		 -> how peaked your data is

Frequency and Mode
-the frequency of an attribute value is the percentage of time the value occurs in the data set
	attribute gender, and a representative population,
	the gender female occurs 48.2% of the time
-the mode of an attribute is the most frequent attribute value
-the notions of frequency and mode are typically used with categorical data

Percentiles
-for continuous data percentiles is usually more meaningful
-give an ordinal or continuous attribute 
	the pth percentile is x_p means that p percent of the data is < x_p
-50th percentile x_50% is the median
-how efficient is one percentile calculation
	sorted array = O(1) unsorted array = O(N*logN)

Percentiles: approximate
-many times it is more useful to use an approximation of the percentile to avoid sorting time
-for data that fits in memory this usually is overkill: just sort
-1 solt-> sub sample the data
	-not good for percentiles close to 0 or 100
-for data that is too large to fit in memory, look at T-Digest

Data Preprocessing
-aggregation
	-combining 2 or more attributes (or objects) into a single attribute (or object)
-purpose
	-data reduction
		-reduce the # of attributs or objects
	-change of scale
		-cities aggregated into regions, states, countries, etc.
	-more "stable" data
		-aggregated data tends to have less variability

Mapping data to a new space
	-fourier transform
	-wavelet transform
-map to a different space 
	-take it to a space with different representation on the axes
	-instead of time vs. value, change to freq. vs value

Discretization using class labels
-minimize within interval entropy of class distributes
-break up data based on groupings and assign a new attribute to each group -- probably nominal if no order

Discretization without using Class labels
-can use intervals, which would turn continuous data into discrete data
-can do this in pandas with cut command
	cut(dataframe,var,[intervals])

Attribute Transformation
-a function that maps the entire set of values of a given attribute to a new set of replacement values s.t. each old value can be identified with one of the new values
	-simple functions x^k, log(x), e^x, abs_val(x)
	-standardization and normalization
	-polynomial and interaction variables

attribute transformation in python slide 21 lecture 3

Data visualization
-Visualization
	-conversion of data into a visual or tabular format so that the characteristics of the data and the relationships among data items or attributes can be analyzed or reported
-visualization of data is one of the most powerful and appealing techniques for data exploration
	-humans have developed ability to analyze large amounts of information that is presented visually
	-can detect general patterns and trends
	-can detect outliers and unusual patterns

Representation
-need to map information to a visual format 
-data objects, their attribtues, and the relationships among data objects are translated into graphical elements such as points, lines, shapes, and colors.
-Example:
	-objects are often represented as points
	-their attribute values can be represented as the position of the points or the characteristics of the points (e.g. color, size, shape)
	-if position is used, then the relationships of points, i.e. whether they form groups or a point is an outlier, is easily perceived

Arrangement is really important for humans
-is the placement of visual elements within a display
-can make a large difference in how easy it is to understand the data

Selection (people do not think in > 3D)
-need to eliminate or de-emphasize certain objects or attribtues
-selection may involve choosing a subset of attributes
	-dimensionality reduction is often used to reduce the number of dimensions to 2 or 3 
	-alternatively, pairs of attribtues can be aggregated
-selection may also involve choosing a subset of objects
	-a region of the screen can only show so many points
	-can sample, but want to preserve points in sparse areas

Visualization Techniques: Histograms
-Histogram
	-distributions of values of a single variable
	-divide the values into bins and show a bar plot of the # of objects in each bin
	-the height of each bar indicates the number of objects
	-shape of histogram depends on the number of the bins
	-basically a frequency plot

Two-dimensional histograms
-estimate the joint distribution of the values of 2 attributes
-ex: petal width and petal length
-tells you correlation

Visualization Techniques: Box plots
-box plots
	-John Tukey
	-another way of displaying the distribution of data
	-outliers as dots, whiskers at 10th and 90th percentiles, bottom box at 25th percentile, line in box at 50th percentile, and top box at 75th percentile
	-connected to violin plot
	-box plot will hide bimodality
-can have multiple box plots on same graph
	-allows you to compare attributes

Visualization Techniques: Scatter Plots
-Scatter plots
	-2D scatter plots most common, but can have 3D scatter plots
	-often additional attributes can be displayed by using the size, shape, and color of the markers that represent the objects
	-it is useful to have arrays of scatter plots can compactly summarize the relationships of several pairs of attributes

Edward Tufty
Hans Rosling

Matplotlib
-python plotting utility
	-has low level plotting functionality
	-highly similar to Matlab and R for plotting
-extended for visually be more beautiful by 
	-Seaborn: standard data visualization group

-read about PCA in your book (appendix B)


--------------------------------------------------------

Data Mining Lecture 4 Notes

groupings used in lecture 3
-class (1,2,3)
-age (child, adult, senior)
-gender( male, female)

seaborn demo
-correlation plot b.w. siblings and spouses
-violin plot -shows distribution
-box plot - survival with each plot being a category
-pair grid - smooth version of histogram, joint distribution 
-facet grid
-correlation plot organized by survival

integrating with D3
-makes plots html interactive
	-zoom in, move around in plot
	-javascript code generated in python
-if you can use mplD3 then use it

Curse of Dimensionality
-when dimensionality increases (# of attributes increases), data becomes increasingly sparse in the space that it occupies
-definitions of density and distance b.w. points, which is critical for clustering and outlier detection, becomes less meaningful
	-more features, means distance b.w. points is less meaningful
-exponential more points as you increase dimensions

Dimensionality Reduction
-purpose: -avoid curse of dimensionality
		  -reduce amount of time and memory required by data mining algorithms
		  -allow data to be more easily visualized
-techniques: -principle component analysis (Karl Pearson)
			 -discriminant analysis
			 -Others: supervised and non-linear techniques

Dimensionality Reduction: PCA
-goal is to find a projection that captures the largest amount of variation in data
-plot 1 attribute vs. other
-find an attribute that would follow closely to something like a best fit line of the plot
-eigenvectors of covariance matrix
-make data set zero mean and then take covariance between attributes
-higher covariance means more correlation
-take first eigenvector of covariance matrix (corresponds to the highest eigen value)
-then project the data points onto the eigen vector
	-represent the data as if it had one attribute
	-if EigenVector = [.85 .52] then .85*att1 + .52*att2

Karhunen-Loeve Transform
-take data points and multiply them by eigenvectors in PCA

Dimensionality Reduction: Randomized PCA
-problem: PCA on all that data can take a while to compute 
	-what if the # of dimensions is gigantic?
		-Actually that's okay: there are iterative algorithms for finding the largest eigenvalues that scales well with the number of data dimensions, but not the number of instances
		-except iterative solutions doesn't scale if the amount of data you have is huge
	-what if the # of instances is gigantic?
-what if we construct the covariance matrix with a subsample of the data?
	-by randomly sampling from the dataset, most of the time we get something representative of the PCA for the entire dataset

Dimensionality Reduction: LDA
-PCA tell us variance explained by the data in different directions, but it ignores class labels
-is there a way to find "components" that will help with discriminate b.w. the classes?

	arg max  SUM differences b.w. classes
	   comp. ----------------------------
			 SUM variance w.in classes

-called Fisher's discriminant
-...but we need to solve this using Lagrange multipliers and gradient-based optimization
-which we wont get to for 2-3 weeks

-PCA tries to give you a component in the direction of the greatest variance
-LDA tries to look at the classes
-differences b.w. classes is calculated by trying to separate the mean value of each feature in each class
-linear discriminant analysis:
	-assume the covariance in each class is the same
-Quadrature DA:
	-estimate the covariance for each class

Dimensionality Reduction: non-linear
-sometimes a linear transform is not enough
-but nonlinear algorithms tend to be pretty unstable or slow
-though many exist: non-linear PCA, ISOMAP, Manifold
-A powerful non-linear transform has seen a resurgence in past decade: kernel PCA
	-apply a transformation on each of your data points, take them into this higher dimensional space, and then do PCA on it

-------------------------------------------------------

Reviewed this material a second time for Assignment 1
-mostly syntax from demos


Assignment 1 Notes

Summary Statistics using Pandas

//import pandas

import pandas as pd

//load in the data to the data frame
df = pd.read_csv('csv path')

//grabs first 5 rows 
df.head()

//grabs last 5 rows
df.tail()

//can also load in from a database
import sqlite3
con = sqlite3.connect('data/heart_disease_sql')
df = pd.read_sql('SELECT * FROM heart_disease', con)
df.head()

//to access a column in dataframe
df.age
df('age')

//to remove a column
del df('site')

df.describe()
-summary statistics
	-count (# of records)
	-unique (# of unique)
	-top (most frequent thing occurring)
	-freq (freq of most common)
-treats all of these as nominal values

//to replace any ? marks in the data set with NaN (not a number)
//use numpy here just to get that nan value
import numpy as np
df = df.replace(to_replace = '?', value=np.nan)

//impute the values for everything that was missing
//fillina means fill everything thats NaN
//df.median creates a new dataframe with the median of each column
//so this replaces all NaNs with the corresponding median
df_imputed = df.fillina(df.median())

//some of these are numeric
numeric_features = ('age','rest_blood_press','cholesterol')

//go through each numeric feature columns
//get the values in matrix form and then return them as a float
for feat in numeric_features:
	df[feat] = df[feat].values.astype(np.float)

//if there is numeric data then it will take precedence over categorical for describe
df.describe()

//can now do something similar with categorical data
//this will take number data and make sure it is categorical
categ_features = ['is_male','chest_pain', 'high_blood_sugar'(etc)]

for feat in categ_features:
	df[feat] = pd.Categorical(df[feat].values.astype(np.float))


//make one series nbased on the data
//df[numeric_features] will create a new data frame 
//with only those attributes
//then get the mean of all those and save it to 
//series_mean

series_mean = df[numeric_features].mean()

//do same with categorical for median

series_median = df[categ_features].median()
cat_series = dp.concat((series_median,series_mean))

cat_series

//now it will replace the categ attributes with the median and the numeric values will be replaced with the mean
df_imputed = df.fillna(value=cat_series)

//can grab particular data really easy
//if class value is missing throw that record out
//can get some very quick summary statistics for someone who doesn't have heart disease
df_imputed[df_imputed.has_heart_disease==0].describe()

//can also group by with pandas
//says when someone has heart disease value = x then the median of the other attributes are y
df_imputed.groupby(by='has_heart_disease').median()

//this basically groups 1-4 together
//either has heart disease or doesn't and find the averages of all the other values based on if they do or don't have heart disease
df_imputed.groupby(by=df_imputed.has_heart_disease>0).mean()

//could do a group by major blood vessels then look at has_heart_disease to see if it is a classifier for someone who has heart_disease
df_imputed.groupby(by=df_imputed.major_vessels>2).mean()

//one hot encoded variables using get_dummies
tmpdf = pd.get_dummies(df_imputed['chest_pain'],prefix='chest')
tmpdf.head()

//get one hot encoding for all categorical variables
//pd.concat([*]) will concatenate all values inside *
//[** for col in categ_features] ->says for each col in categ_features do **
one_hot_df = pd.concat([pd.get_dummies(df_imputed[col],prefix=col) for col in categ_features], axis=1)


Data Visualization using Pandas


Histograms
-petal width, petal length, sepal width
-split continuous variables into bins on x-axis

2D Histograms
-have one attribute on x, another on z, and count on y
-shows correlation between 2 attributes

Box Plots
-have a plot for each attribute on x axis and values on y

violin plots
-estimation of distribution
-more meaningful for data if data has 2 values with high frequency
-box plot hides bimodality

Scatter plots
-2D scatter plots most common
-could also do scatter plot matrices
-show scatter plot of 5 attributes vs. 5 attributes

Visualization Iris Correlation Matrix
-arranged by class

Parallel Coordinates 
-take attributes on the x axis and values on y axis and connect between classes with a line
-then can have each class use a different color
-can see how attributes are correlated to their class and how each class is correlated to the other classes

//get all data types
df.types
df.info

//percent of individuals that died on the titanic
float(len(df[df.Survived==0]))/len(df)*100

//gets number of people that survived in each class
// number of people in each class
// percentage of people that survived in each class
df_grouped = df.groupedby(by='Pclass')
print df_grouped.Survived.sum()
print '------------------------'
print df_grouped.Survived.count()
print '------------------------'
print df_grouped.Survived.sum() / df_grouped.Survived.count()

//break up a continuous variable into intervals
//creates a new attribute in df called age_range
//rows with NaN will get skipped by aggregate stats
 df['age_range'] = pd.cut(df.Age,[0,16,65,1e6],3,labels=['child','adult','senior'])

//can do df.age_range.describe()

//group by class and age range
//print out % of people that survived based on their class and age range
df_grouped = df.groupby(by=['Pclass','age_range'])
print "Percentage of survivors in each group:"
print df_grouped.Survived.sum() / df_grouped.Survived.count() * 100

//use aggregation to impute values

df_grouped = df.grouby(by=['Pclass','SibSp'])

//takes the median value for each attribute when it is grouped by class and sibling and fills in the NaN with that value
df_imputed = df_grouped.transform(lambda grp: grp.fillina(grp.median()))
df_imputed[['Pclass','SibSp']] = df[['Pclass','SibSp']]

%matplotlib inline


BAR GRAPH
//plot the survival rate based on groups
import matplotlib.pyplot as plt
df_grouped = df_imputed.grouby(by=['Pclass','age_range'])
survival_rate = df_grouped.Survived.sum() / df_grouped.Survived.count()
survival_rate.plot(kind='barh')

//cross-tabulate is good to get groupings
//cross tabulate the df_imputed by the class and 
//age_range
//breaks down class and age range based on if you 
//survived or not
survival = pd.crosstab([df_imputed['Pclass'],df_imputed['age_range']],df_imputed.Survived.astype(bool))

//Can see a stacked bar graph
//divides count by sum to get rate
survival_rate = survival.div(survival.sum(1).astype(float),axis=0)
survival_rate.plot(kind='barh', stacked=True, color=['black','gold'])

//above was for the survived rate
//can do count of people that survived instead with sex 
//and class groupings
//if your yellow bar is big then a lot of your group 
//survived
survival_counts = pd.crosstab([df_imputed['Pclass'],df_imputed['Sex']],df_imputed.Survived.astype(bool))
survival_counts.plot(kind='bar',stacked=True, color=['black','gold'])


BOX PLOT

//plot price of fare based on class
//have a box plot for each value of class, with the range of fares shown on the y axis
df_imputed.boxplot(column='Fare', by= 'Pclass')

//subplots
vars_to_plot_separate = [['Survived','SibSp','Pclass'],['Age'],['Fare']]
plt.figure(figsize=(10,6))

for index, plot_vars in enumerate(vars_to_plot_separate):
	plt.subplot(len(vars_to_plot_separate)/2,2,index+1)
	ax = df_imputed.boxplot(column=plot_vars)
plt.show()


Scatter Matrix

from pandas.tools.plotting import scatter_matrix

ax = scatter_matrix(df,figsize=(15,10))
-diagonal is histogram

Parallel Coordinates

from pandas.tools.plotting import parallel_coordinates

df_sub = df[['Is_Grtr_50','Age','Hrs_Per_Week']]
df_normalized = (df_sub-df_sub.min())/(df_sub.max()-df_sub.min())
parallel_coordinates(df_normalized,'Is_Grtr_50')

Simplifying with Seaborn

import seaborn as sns
cmap = sns.diverging_palette(220, 10, as_cmap=True)

Correlation Plot
sns.set(style='darkgrid')

f, ax = plt.subplots(figsize=(9,9))

sns.corrplot(df,
			annot=True,
			sig_stars=True,
			diag_names=True,
			cmap=cmap,
			ax=ax)
f.tight_layout()

Violin Plot

pal = sns.cubehelix_palette(40, rot=-.5, dark=.3)
sns.violinplot(df_imputed[['Age','Hrs_Per_Week']], color=pal, groupby=df.Is_Grtr_50.sdfadaf)

Factor Plot

sns.factorplot('Age_range','Fare','Survived',df,
				kind="box",
				palette="PRGn")

Pair Grid

sns.set(style="white")

-uses kernel density estimation
g = sns.PairGrid(df,diag_sharey=False)
g.map_lower(sns.kdsplot, cmap="Blues_d")
g.map_upper(plt.scatter)
g.map_diag(sns.kdsplot,lw=3)

Facet Grid

sns.set(styles='darkgrid')
 
g= sns.FacetGrid(df, col="age_range", row="Sex", margin_titles=True)
g.map(plt.hist, "Survived", color="steelblue", lw=0)


Integrating with D3

import mpld3

sns.set(style="darkgrid")

g = sns.FacetGrid(df, col="age_range", row="Sex", margin_titles=True)
g.map(plt.hist, "Survived", color="steelblue", lw=0)

mpld3.display()
-don't use it for box plots or violin plots

-------------------------------------------------------

Data Mining Flipped Module 1 notes

Linear Regression

set of instances X used to predict output y

vectors
X(1)	X(2)	 X(3)  .... X(M)
x(1)_1						x(M)_1
x(1)_2 ...
x(1)_3						.
.							.
.
.
x(1)_N 						x(M)_N

trying to predict y(1) y(2)...y(M)
-try to find eq. that maps y to x
-in linear regression this is a linear eq.

yhat(1) = W'X(1) -> dot product of these 2 variables

W = w_1
	w_2
	w_3
	w_4

add in bias term  w_0
-data becomes 

X(1)	X(2)	 X(3)  .... X(M)
x(1)_0						x(M)_0
x(1)_1						x(M)_1
x(1)_2 ...
x(1)_3						.
.							.
.
.
x(1)_N 						x(M)_N

W = w_0
	w_1
	w_2
	w_3
	w_4

set all of x(.)_0 = 1

want to make yhat(1) as close to y(1) as possible

sum_i(y(i) - yhat(i))^2 = J -> objective function
-want to minimize this
-adding squared difference minimizes the error if we assume it only has Gaussian error

J(w) = sum_i(y(i) - W'x(i))^2

y(i) = W'x(i) + eps_x
				(error term)
				assume its Gaussian

if Gaussian it will have normal distribution 
-0 mean and some variance sigma^2
	N(0,sigma^2)

if we add a Gaussian distribution on then y(i) now also follows a Gaussian distribution with
N(W'x(i),sigma^2)
-average is increased variance stays the same

P(y(i) | x(i),W,sigma) = 
	
		  1 					(y(i) - W'x(i))^2
  __________________  * exp(-  ___________________ )
  sqrt(2*pi*sigma^2) 				2* sigma^2

Tot Prob = Prod_i(P(y(i) | x(i),W,sigma))

-want to maximize the Tot Prob	
-take ln of tot prob. It will change tot prob, but W will stay the same

					  	1
ArgMax_W( sum_i ( ln( _______))                
                )
 			
 ArgMax_W(
 		Sum_i(
 		
 		    1 	 			  (y(i) - W'x(i))^2
 ln( __________________ ) -  ___________________ 
      sqrt(2*pi*sigma^2) 		2* sigma^2

      		 )
      	)

Argmax_W does not depend on term with ln so we can just ignore that term completely

 		  		   1			
 ArgMax_W( -  ____________ * sum_i (y(i) - W'x(i))^2
      		   2* sigma^2
      	 )

      	 or 
      	 	   1
ArgMin_W( ____________ * sum_i (y(i) - W'x(i))^2
      	   2* sigma^2
      	 )

Maximizing total probability is Max likelihood estimation (MLE)
-assumes noise source is Gaussian distributed noise

Regression Objective Function Solution

Added the 1/2 in that we took from ArgMin 
	-we don't know anything about sigma so ignore it?

J(w) = 1/2*sum_i(y(i) - W'x(i))^2

y = y_1    X = <------(x(1))'-------->
	y_2		   <------(x(2))'-------->
	y_2		   <------(x(3))'-------->
	.		   			.
	.		   			.
	.		   			.
	y_M		   <------(x(M))'-------->
					N+1 columns
	
	W = w_0
		w_1
		.
		.
		.
		w_m

		XW = W'x(1)  = yhat
			 W'x(2)
			 W'x(3)
			 .
			 .
			 .
			 W'x(M)

remember want to minimize error between y and yhat

rewrite J(w) as

	J(w) = .5 * ( XW - y)'(XW - y)

normally to minimize a function you take derivative and set to 0

matrix equivalent of taking derivative is taking gradient
	-partial derivatives of J w.r.t w

Aside---------
d.dw((alpha*W -y)(alpha*W-y))
	-use product rule
	= 2*alpha*(alphaW - y)

matrix equivalent

grad.W(XW -y)'(XW-y) = 2X'(XW-y)

End Aside-------

grad.W (J(W)) = .5*2*X'(XW-y)

set = to 0

	0  = X'XW - X'y
	X'XW = X'y
	multiple both sides by inverse of X'XW

	W = (X'X)^-1 * X'y

	-this is the optimal sol't for our W vector for linear regression


X is matrix M x (N+1)

X'X  is matrix N+1 x N+1
-as # of instances grows, the complexity of taking inverse doesn't matter (still have a matrix of N+1 x N+1)
-as N gets really big this calculation gets really complicated
-N is attributes, M is rows

Steepest Descent

W = w_0
	w_1
	.
	.
	.
	.
	w_N
	a W for each attribute 

	lets say W = w_0
				 w_1

	draw a 2d representation of what a 3d vector would look like in space

Steepest Descent
	dot with circles around it
	-from the dot anywhere is uphill
	-we have a guess outside the circles
	-we want to make a decision s.t. each jump is the steepest direction down in relation to next inner circle
	-iteratively update w to be in the steepest direction

W_j := W_j + alpha d.dW_j (J(w))

-alpha adjusts size of each jump in valley
-want to calculate d.dW_j (J(w))

d.dW_j (J(w)) = .5 * sum_i ( d.dW_j((y(i) - W'x(i))^2))
-take derivate ...

	= sum_i (y(i)-W'x(i))x(i)_j

insert into W_j

W_j := W_j - alpha * sum_i (y(i)-W'x(i))x(i)_j

alpha = learning rate
-in code
W = W - alpha * sum_i (y(i)-W'x(i))x(i)
	 
-can use steepest descent to optimize J(W) to solve for coefficients of W

if W starts overlearning
	-meaning highly dependent on 1 or 2 parameters of X
	-overlearning based on sample and it won't generalize as well as it could
	-can adjust by also minimizing sum_j((W_j)^2)

Ridge Regression
J(w) = 1/2*sum_i(y(i) - W'x(i))^2 + roe*sum_j((W_j)^2)

slightly different minimum value by using smaller W


Linear Regression to Classification

-redefine y values from continuous numbers to a set
	y = {0,1} binary

binary classification problem

now
	yhat(i) = g(W'x(i)) => {0,1}
	-g just maps those values into 0 and 1
	-dirac delta

	if g(W'x) < 0 then yhat = 0
	   g(W'x) >= 0 then yhat = 1

J(w) = 1/2*sum_i(y(i) - g(W'x(i)))^2

-update function is same

W_j := W_j + alpha d.dW_j (J(w))


d.dW_j (J(w)) = sum_i((y(i) - g(W'x(i))) 
									*d.dW_j(g(W'x(i)))
					 )
	= sum_i((y(i) - g(W'x(i))) * 			   |
						x(i)_j(d.dthet(g(thet))|      )
			)								   |
											thet=W'x(i)
-saying find the derivate of the dirac delta function
-normally just ignore the d.dthet in this function
	-which is the dirac delta

d.dW_j (J(w)) = sum_i((y(i) - g(W'x(i))) * x(i)_j


Logistic Function

yhat(i) = g(W'x(i))
-use a better equation for g in g(theta)
				1
g(theta) = ________________
			1+ exp(-theta)
-logistic function
-theta is a scalar

				1
g(W'x) = ________________
			1+ exp(-W'x)

d.dW_j(g(W'x)) = -g(W'x(i))(1 - g(W'x(i)))x(i)_j


				learning rule
			 |---------------------|	

W_j := W_j - sum_i((y(i) - g(W'x(i))) *

				g(W'x(i))(1-g(W'x(i)))x(i)_j
				|--------------------------|
					back propagation

optimal values of W when g is sigmoid function
-easier to use with Steepest Desecent


Logistic Regression
-Classification algorithm
-not regression

still got a binary classification problem
yhat = {0,1}
						1
P(yhat =1 | x,W) = _______________
					1 + exp(-W'x)

							1
P(yhat=0 | x,W) = 1 -  _______________
						1 + exp(-W'x)

Want to maximize probabilities given instances we have
-similar to before

Define total probability given our instances and the model parameters

L(W) = prod_for all y=1 (P(y(i)=1 | x(i),W)) * 
	   prod_for all y=0 (P(y(i)=0 | x(i),W))

MLE

ARGMAX_W (L(W))



-------------------------------------------------------

Data Mining Lecture 8 Notes

-lec 5,6 he was gone
-lec 7 was jus inclass assignment


W' is the line mapping x(1) to y

then we used g(W'x(i)) > alpha
with alpha being the splitting point- median maybe

-how do we figure out what weights are so that this is a good classification

Demo Logistic Regression

import numpy as np

#vector from -5 to 5 with 100 points
x1 = np.linspace(-5,5,100)
x2 = np.linspace(-5,5,100)

#turnthese into a grid so you can graph them
x1, x2 = np.meshgrid(x1,x2)

# w for this case is [4 2]
yreg = 1 / (1 + np.exp(-(4*x1+2*x2)))

#when features are axes you're in the feature space
#drawing a line -> < line is class 0, > is class 1
#alpha here is .5

-------------------------
-the weights tell you where the cliff is 

L(W) = Prod_y(i)=1 (P (y(i)=1 | x(i),W)) *
	   Prod_y(i)=0 (P (y(i)=0 | x(i),W))

MLE	 = Prod_i ( P(y(i)=1| x(i),W)^y(i) *
	 			P(y(i)=0| x(i),W)^(1-y(i))
	 		  )

ln(L(W)) = l(W) = sum_i( y(i)*ln(P(y(i)=1)) + 
						(1-y(i))*ln(P(y(i)=0))
						)


l(W) is a contour plot
-choose initial value for w and choose direction of steepest descent (gradient)
-don't need to worry about local minima with logistic regression - there is 1 optimal value

grad_W (l(W)) = d.dW_1(l)
				d.dW_2(l)

:= is preceded by
W := W + alpha * grad_W (l(W))

l(W) = sum_i (y(i)* ln(g(x(i))) + (1-y(i))*ln(1-g(x(i))
											 )
			 )
			 			   1
d.dW_j(l) = sum_i(y(i)* _______ * d.dW_j(g(x(i))) +
						g(x(i))

				  	  (1-y(i))
					____________ *d.dW_j(1-g(x(i)))

				 	 (1-g(x(i)))
				 )
			    1
g(x(i)) = ______________  
		  1+exp(-W'x(i))

d.dW_j(g(x(i))) = g(x(i))*(1-g(x(i)))*(-x(i)_j)

plug these into above equation and cancel

d.dW_j(l) = sum_i ( y(i)*(1-g(x(i)))*(-x(i)_j) + 
					(1 - y(i))*g(x(i))*x(i)_j
				  )

= sum_i( (g(x(i)) - y(i))x(i)_j )

-this is the direction we need to move with each jump

-now plug into update equation

W := W + alpha sum_i( (g(x(i)) - y(i))x(i)_1 )
			   sum_i( (g(x(i)) - y(i))x(i)_2 )
			   			.
			   			.
			   			.
			   sum_i( (g(x(i)) - y(i))x(i)_N )

W := W + alpha* X'(g - y)

g is the vector of outputs when x(i) is used

y is what you are trying to predict (target)

g and y are column vectors

X' is the stacked instances (each instance is a row)
-> X'(g-y) is a column vector


c = {1 ..... C_T}

saying c can go from 1 to C_T -1
P(y(i)=c | x(i),W) =  exp(W'_c x(i))
					 _________________
					  1 + sum_(c'->C_T-1)of 
					  			(W'_c'*x(i))
					  		  )

					W_(c=1)
					W_(c=2)
				W =	.			each W_c_i is N long
					.
					.W_(c=C_T-1)

p(y(i)=C_T | x(i),W) = 				1
						___________________________
						1+ sum_(c=1->c_T-1) of
						exp(W'_c * x(i))

l_new(W) = l(W) - b*sum_j( (W_j)^2)
					|--------------|
					  Regularize


W := W + alpha*X'*(g(x) - y) 		BATCH

W := W + alpha*(g(x(i)) - y(i))x(i)

Stochastic gradient descent
-keep trying jumps till you get a correct one, and will slowly zero in on minimum
-combine with batch to make more stable

Mini-Batch
 W:= W + alpha* X'_sub(g(x_sub) - y(sub))

More Demo-------------------

#class weight is how important a class is
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import load_iris

ds = load_iris()

#default c = 1
obj = LogisticRegression()

obj.fit(ds.data[:75],ds.target[:75])

ysub = ds.target[76:len(ds.data)
feat_sub = ds.data[76:len(ds.data)
yhat = obj.predict(feat_sub)

sum(yhat == ysub)




-----------------------------

Assignment 2 Notes
-take data set we already had 
-use logistic regression use fort vector machine on it
-vary parameters in each and see how well you can classify the data and interpret it


-------------------------------------------------------

Data Mining Lecture 9 Notes


Support Vector Machines

x(i) = x(i)_1
	   x(i)_2

Want to find the equation for the line that falls in the middle of the 2 clusters

Have W that will be perpendicular to this line and will go through the data

-take the dot product of a data point and W and if the scalar value is < C then 0, if it is >= C then 1
w . x(i) >= c then class + 
			  else class -

w . x(i) + b >= 0
y(i) = + => 1
y(i) = - => -1

w . x(i) + b >= 0, when  y(i) = 1
			 < 	0, when y(i) = -1

y(i)*(w . x(i) + b) >= 0

w has 1 entry for each attribute 

w = w_1
	w_2
						   W
Margin = (x(+) - x(-)) . _____ 
						 ||W||


x(+)W + b >= 0
x(+)W + b >= alpha
-(x(-)W + b) = alpha
x(+)W = alpha -b
-x(-)W = alpha + b

Plug into margin
				  1
(x(+)W - x(-)W)* ____
				 ||W||

alpha-b + alpha + b

  						1
(alpha-b + alpha + b)* _____
				 	  ||W||

 2*alpha
_________ = MARGIN
 ||w||

 MINIMIZE ||W|| 

 Want this MINIMIZE .5||W||^2 to be true s.t. 
 -y(i)*(Wx(i) + b) <= 0


min f(W) s.t. g_i(w) <= 0

Primal

min_W Max_alpha_i f(W) + sum_i (alpha_i*g_i(W))
-maximize with respect to alpha_i and then minimize w.r.t W


DUAL
max_alpha_i Min_W f(W) + sum_i (alpha_i*g_i(W))
-minimize w.r.t W and then maximize w.r.t alpha_i


Dual = max_alpha_i Min_W .5||W||^2 - 
					sum_i(alpha_i*y(i)*(W*x(i) + b))

grad_b(duel) = sum_i(alpha_i * y(i)) = 0
grad_w(duel) = 0 = W - sum_i(alpha_i * y(i) * x(i))
				W = sum_i(alpha_i * y(i) * x(i))

DUAL = .5(sum_i(alpha_i * y(i) * x(i)))*
		 (sum_j(alpha_j * y(j) * x(j))) - 
		 (
		  (sum_i(alpha_i * y(i)*b)) +
		  (sum_i(alpha_i * y(i) * x(i))) *
		  ((sum_j(alpha_j * y(j) * x(j))))
		 )

	 = .5 sum_i (sum_j( 
	 			  alpha_i*alpha_j*y(i)*y(j)*x(i).x(j))
	 			      )
	 			)
	 	- sum_i (sum_j( 
	 			  alpha_i*alpha_j*y(i)*y(j)*x(i).x(j))
	 			      )
	 			) 
	 = - .5sum_i (sum_j( 
	 			  alpha_i*alpha_j*y(i)*y(j)*x(i).x(j))
	 			      )
	 			) - sum_i( alpha_i^2)

-most of the alphas = 0
-once you know alphas you can figure out W and then you can figure out b

W*x(test) + b = sum_i (alpha_i * y(i)*<x(i),x(test)>)+b
-don't need to know x's
-just need to know what the <x(i),x(test)> is 

zeta(i) is distance from W that a data point is

Dual + C* sum_i ( (zeta_i^2) )
					|
				  slack
want to minimize slack as much as possible

zeta(i)*(x(i)W + b) >= 0
-set the value of C -> says how much you value the zetas
-zeta is some value greater than 0 for each instance


phi(x(i)) takes x(i) from one dimension and puts it in another

= -.5 sum_i(sum_j(alpha_i*alpha_j*y(i)*y(j)*
					<phi(x(i)),phi(x(j))> + epsilon

DEC = sum_i(alpha_i*y(i)*<phi(x(i)),phi(x(test))> + b)
Func

Aside----------------
q = 1
	sqrt(2)*q
	q^2

<q,q> = 1 + 2qq' + q^2(q')^2
	  = (1 + qq')^2 = Kernel

-----------------------------

K(q,q') = (1 + qq')^K
			polynomial

K(q,q') = exp(-||q-q'||^2 *lambda)
				Radial basis

C -> how much you care about slack variables
-if you make it zero it means you don't care about slack variables
-try .1, 1, 10
-make it bigger if you want fewer slack variables



-------------------------------------------------------

Data Mining Lecture 10 Notes


Assignment 2 Due Saturday 

create logistic regression model and complex vector support model that classifies your model
-discuss advantages of 1 model over the other by analyzing results
-separate your data into testing and training
	-80% training, 20% testing
-go in and look at the weights from logistic regression and determine the importance of different features
-go in and determine where the support vectors are, and what they look like
-determine how useful your model is to interested parties
	-how would they use it


Demo Notes---------------------------------

import pandas as pd
import numpy as np

df = pd.read_csv('data/titanic.csv')

#use numbers for labels so that it creates an ordinal 
#attribute
df_imputed['age_range'] = pd.cut(df_imputed.Age,[0,16,24,65,1e6],4,labels=[0,1,2,3]) 

df_imputed.age_range = df_imputed.age_range.astype(np.int)

#objects in a pandas dataframe - categorical

#set up 1 hot encoding for categorical data with more
# than 2 unique
tmp_df = pd.get_dummies(df_imputed.Embarked,prefix='Embarked')
df_imputed = pd.concat((df_imputed,tmp_df),axis=1)

#make sure this is also numerical
df_imputed['IsMale'] = df_imputed.Sex=='male'
df_imputed.IsMale = df_imputed.IsMale.astype(np.int)

#clear out old Sex and Embarked since we replaced them

#create new variable
df_imputed['FamilySize'] = df_imputed.Parch + df_imputed.SibSp

#Training and Testing Split
#randomly go in and grab 80% of data

from numpy import random as rd
 y = df_imputed['Survived'].values
 del df_imputed['Survived']
 X = df_imputed.values

#total number of instances
 N = len(df_imputed)
permuted_indices = rd.permutations(N)

N_eighty_percent = int(0.8*N)

x_train = X[permuted_indices[:N_eighty_percent]]
x_test = X[permuted_indices[N_eighty_percent:]]

y_train = y[permuted_indices[:N_eighty_percent]]
y_test = y[permuted_indices[N_eighty_percent:]]


from sklearn.linear_model import LogisticRegression

#get object
#l2 is the mean squared error of our output
lr_elf = LogisticRegression(penalty='l2', c=1.0, class_weight=None)

#train object
lr_elf.fit(x_train,y_train)

#get test set 
#gives output on test data from the model we trained
y_hat = lr_elf.predict(x_test)

from sklearn import metrics as mt

#wanna test our predicted y values (y_hat) against y
acc = mt.accuracy_score(y_test,y_hat)

#gives accuracy and tendency of confusing 2 classes
#with eachother
#scoring and precision recall
#for later assignments we should do this many times 
#as the permutation matrix will vary each time
conf = mt.confusion_matrix(y_test,y_hat)
print acc
print conf


#interpret the weights
for coef, name in zip(lr_elf.coef_.T, df_imputed.columns):
	print name, 'has an importance of ', coef[0]

#the largest magnitude of weight is the most important
#attribute to the model
#if negative weight then it is negatively associated to survival

from sklearn.preprocessing import StandardScaler

#scale attribute by the training set
scl_obj = StandardScaler()
scl_obj.fit(x_train)
x_train_scaled = scl_obj.transform(x_train)
x_test_scaled = scl_obj.transform(x_test)

#train the model just as before
#can play around with c value
lr_elf = LogisticRegression(penalty='12',c=0.05)
lr_elf.fit(x_train_scaled,y_train)

y_hat = lr_elf.predict(x_test_scaled)

from sklearn import metrics as mt

acc = mt.accuracy_score(y_test,y_hat)

#gives accuracy and tendency of confusing 2 classes
#with eachother
#scoring and precision recall
#for later assignments we should do this many times 
#as the permutation matrix will vary each time
conf = mt.confusion_matrix(y_test,y_hat)
print 'accuracy', acc
print conf

#sort these attributes and spit them out
zip_vars = zip(lr_elf.conf_.T,df_imputed.columns)
zip_vars.sort(key = lambda t: np.abs(t[0]))
for conf, name is zip_vars:
	print name, "has importance of", coef[0]

from matplotlib import pyplot as plt
%matplotlib inline

weights = pd.Series(lr_elf.conf_[0],index=df_imputed.columns)
weights.plot(kind='bar')


Support Vector Machines

from sklearn.svm import SVC

#cost multiplied the sum of Zs (slack variables)
#kernel= linear means no kernel
#higher kernel means transform data into higher degree
svm_elf = SVC(C=1.0, kernel='linear', degree=1, gamme=0.0)
svm_elf.fit(x_train_scaed, y_train)

y_hat = svm_elf.predict(x_test_scaled)

acc = mt.accuracy_score(y_test, y_hat)
conf = mt.confusion_matrix(y_test,y_hat)
print 'accuracy', acc
print conf

print svm_elf.support_vectors_.shape
print svm_elf.support_.shape
print svm_elf.n_support_

print svm_elf.coef_
weights = pd.Series(svm_elf.coef_[0],index=df_imputed.columns)
weights.plot(kind='bar')

df_support = df_imputed.iloc(svm_elf.support_,:)
df_support['Survived'] = y(svm_clf.support_)
df_imputed['Survived'] = y
df_support.info()

from pandas.tools.plotting import boxplot

#group support data set by survived and original data set by survived
df_grouped_support = df_support.groupby(['Survived'])
df_grouped = df_imputed.groupby(['Survived'])

vars_to_plot = ['Age','PClass','IsMale','FamilySize']

for v in vars_to_plot:
	plt.figure(figsize=(10,4))
	#plot support vector stats
	plt.subplot(1,2,1)
	ax = df_grouped_support[v].plot(kind='kds')
	plt.legend(['Perished', 'Survived'])
	plt.title(v+' (Support)')

	#plot original distribution
	plt.subplot(1,2,2)
	ax = df_grouped[v].plot(kind='kds')
	plt.legend(['Perished','Survived'])
	plt.title(v+' (Original)')

#this says there is a lot of variablity in the original data, but not a lot b.w. classes 
-the tough cases where there isn't a lot of variability where you are trying to predict

Gradient Based Alternatives
lecture at 1 hour


-------------------------------------------------------

Data Mining Flipped Module 2 Lecture Notes

Decision Trees
-just another machine learning algorithm
-will take features and attribtues and try to come up with a predictive algorithm
-table data
	-rows are instances, cols are attributes ()

-decision trees help with classification
	-look at an attribute and based on value decide if you will go left or right
	-for binary it would be yes or no

							preg
				-yes 				-no
			pos. diagnosis				eye color
									blue 		hazel
								BMI				brown
							<30		>30		-neg. diag.
							Neg.	Pos.


Create a machine learning algorithm that goes through tree
-then can predict with model

-start at root of tree and compare attribute of instance
-there's more than one way to fit the same training data
-finding the optimal model is very difficult
	-NP complete problem
-you'd have to look at every possible tree

Use Hueristics

Self Test
-what makes an optimal decision tree (from the following)?
	-accuracy
	-easy to interpret by a human
	-minimum tree depth
	-extensible to large datasets
-# of leafnodes is not as important
-many leaf nodes on branches near the root (generally not worried about)

Decision tree Classification task
-use induction
-most stem from Hunt's algorithm

General Structure of Hunt's Algorithm
-have a Node t -> D_t = (2,4,7)
-let D_t be the set of training records that reach a node t
1. if all records in D_t are the same class y_t then t is a leaf node labeled as y_t
2. If D_t is an empty set, then t is a leaf node labeled by the default class y_d
3.  IF D_t contains records that belong to more than one class, use an attribute test to split the data into smaller subsets from D_t
4.  Recursively apply the procedure to each subset

Self test
-splitting the data into smaller subsets 
-given you are in a node with these attributes, should you split the node or make it a leaf node?
	-there area only 2 unique rows
	-splitting it would have every row go either left or right, so make a leaf node

Tree Induction: Splitting on Attributes
-Greedy strategy.
	-split the records based on an attribute test taht optimizes certain condition
-issues 
	-determine how to split
		-how to specify the test condition?
		-how to determine the best split?
	Determine to stop splitting

How to specify test condition?
-depends on attribute types 
	-nominal 
	-ordinal
	-continuous
-depends on number of ways to split
	-2-way split
	-multi-way split

Splitting based on nominal and ordinal
-multi-way split: use many partitions as distinct values
	-cartype-family
			-sports
			-luxury
	-not usually done
-binary split: Divides values into 2 subsets.
			-need to find optimal partitioning
		Car type 						cartype
-luxury 		-family 		-family 		sports
 sports 						 luxury 


Self test
-draw the different ways to split this attribute?
	-attribute size: small, medium, large

small/medium 						large
small 								medium/large
small  			medium 				large
Don't do this next one
-keep the ordinals ordered
small/large 						medium

-come up with binary split for all nominal or ordinal attributes

Splitting based on continuous variables
-different ways of handling
	-discretization to form an ordinal categorical attribute
		-static - discretize once at the beginning
		-dynamic- ranges can be found by equal interval bucketing, equal frequency bucketing (percentiles), or clustering
	-binary decision: (A < v) or (A>=v)
		-consider all possible splits and finds the best cut 
		-can be more compute intensive

Taxable income
-binary split ex
	-most common
	Taxable income is >80k or not
-multiway split
	-<10 k, 10k-25k,25k-50k,50k-80k,>80k
	-this is better done by discretizing the data and making it ordinal

How to determine the best split
-which test condition is most pure?
	- node with 20 attributes
				C0: 10
				C1: 10
				- 10 in class zero, 10 in class 1
	-could split by own car splitting it into 2 groups of  C0: 6      					C0: 4
		C1: 4 						 C1: 6

	-could split by car type with 3 groups
	C0:1 				C0:8 			CO:1
	C1:3 				C1:0 			C1:7

	-could split by student ID
		-never do this

How to determine best split?
-greedy approach:
	-we prefer nodes that are pure, homogeneous
-need a measure of node impurity
	C0: 5 							C0:9
	C1: 5 							C1:1
	not homogeneous					homogeneous
	-not pure=highly impure 		very pure = not
												impure

How to Find the Best Split
-  								classes 	#records
											in class
			Before Splitting:    C0 		N00
 								 C1 		N01

 Look at each attribute as a candidate split
 M is measure of impurity

 Self test
 -if you have 3 different binary attributes and a node that you want to split, how many "impurities" do you need to calculat
 	-you have 3 attributes each get split into 2
 		-6 nodes, so 6 impurities


Measures of Node impurity
-gini index
-entropy
-information gain
-gain ratio

Measure of impurity: GINI
-looks at distribution of classes
-if I randomly grab a record from this node, what is the probability it is not correct?
-Gini index for a give node t:
		GINI(t) = sum_j( p(j | t)*( 1- p(j | t) )
					   )
			     -at node t, multiple the odds of selecting class and not selecting class
			     -sum this up for all classes
	-also 
		GINI(t) = 1 - sum_j ( (p(j|t)^2 )

	Example:
				C1 	1  P(C1) = 1/6
				C2  5  P(C2) = 5/6
				Gini = 1 - (1/6)^2 - (5/6)^2 = .278

Measure of Impurity: GINI
-If I randomly grab a record from this node, what is the probability it is not correct?
-Gini index for a given node t:
		GINI(t) = 1 - sum_j ( (p(j|t)^2 )
	Maximum (1-1/n_c)
	-records are equally distributed among all classes
	Minimum (0.0)
	-all records belong to one class
	-more impure, the higher the GINI

self test: examples for computing GINI
		C1 0    Gini = 1 - P(C1)^2 - P(C2)^2 = 0
		C2 6

		C1 1 	Gini = 1 - P(C1)^2 - P(C2)^2
		C2 5		 = 1- (1/6)^2 - (5/6)^2 = .278

		C1 2    Gini = 1 - P(C1)^2 - P(C2)^2
		C2 4		 = 1 - (2/6)^2 - (4/6)^2 = .444

Splitting Based on GINI
-how to combine Gini indices from children nodes?
-when a node p is split into k partitions(children), the quality of split is computed as

	Gini_split = Sum_i=1 (n_i/n * GINI(i))

	n_i is number of instances inside the child
	-n = total # of instances at node t
-weighted summation

Binary Attribtues: Computing Gini index
-splits into two partitions
-effect of weighing partitions
	-larger, purer partitions

		Parent 
			C1 6
			C2 6
				B?
			yes 	No
		Node N1 	  Node N2
				N1 | N2
			C1  5  | 1
			C2  2  | 4

Gini N1
=1 -(5/7)^2 - (2/7)^2
=.408

Gini N2
= 1 - (1/5)^2 - (4/5)^2
= .320

Gini(children) = 7/12*.408 + 5/12*.320
			   = .37

Categorical Attribtues: Computing Gini Index
-for each distinct value, gather counts for each class in the data set
-use the count matrix to make decisions
-can calculate Gini from two-way split and use the lower value as the better split (more impure)
-multi-way split always has lower Gini

Continuous Attributes: Computing Gini Index
-Use binary decisions based on one value
-many choices for splitting
	number of possible splitting valus
	= number of distinct values
-each splitting value has a count matrix associated with it
	-class counts in each of the patitions
			A >v A<=v
-simple method to choose best v
	-for each v, scan the database to gather count matrix and compute its Gini index
	-computationally inefficient! repetition of work


-efficient computation: sort the attribute
	-take attribute, transpose it to be horizontal and sort it
	-split positions
	-determine how many are in each class for each potential split position and calculate corresponding gini
		-choose where the minimum gini is to be the split position
	-really only have to calculate the Gini where the class value changes (local minimum) to find which is the extrema

Alternative Splitting Criteria based on INFO
-Entropy is a measure of randomness
-want something with low entropy - not very chaotic

Entropy at a give node t:
	Entropy(t) = sum_j ( p(j|t)*log( 1/( p(j|t) ) )
					   )
(Note: p(j|t) is the relative frequency of class j at node t)

-expected number of bits needed to encode a randomly drawn value efficiently
-entropy == uncertainty, which we want to minimize
-if something happens more frequently, we want to use fewer bits to represent it
	-if it happens more rarely we can use more bits
-if all characters occur around the same amount then there is a high level of entropy and there is not a whole lot of reduction that can be done

	Entropy(t) = -1*sum_j ( p(j|t)*log( p(j|t) )
					   )

	Ex: C1  1
		C2  5 	P(C1) = 1/6 	P(C2) = 5/6
				Entropy = -(1/6)log_2(1/6)
						  -(5/6)log_2(5/6) = .65

-Measures homogeneity of node
	-maximum (log n) when records are equally distributed among classes (implying least information)
	-minimum (0.0) when all records belong to one class, implying most information

-Entropy based computations give similar splits as the Gini impurity
	-when we combine entropy it becomes better

Self test:
	Entropy(t) 
-it will always be positive
	log(p(j|t)) is always negative
	p(j|t) is always positive
	-then you take the negative

self test: examples for computing Entropy
		C1 0    Entropy = -P(C1)*log(P(C1)) 
		C2 6			  - P(C2)*log(P(C2)) = -0-0=0

		C1 1    Entropy = -P(C1)*log(P(C1)) 
		C2 5			  - P(C2)*log(P(C2)) 
						= -(1/6)*log(1/6) 
						  - (5/6)*log(5/6) = .65

		C1 2    Entropy = -P(C1)*log(P(C1)) 
		C2 4			  - P(C2)*log(P(C2)) 
						= -(2/6)*log(2/6) 
						  - (4/6)*log(4/6) = .92


Splitting Based on INFO...
-Information Gain:
	GAIN_split = Entropy(p) - (sum_i ( (n_i/n) * 	
											Entropy(i)
							   		 )
							  )
-measures reduction in entropy achieved because of the split.  Choose the split that achieves most reduction (maximizes GAIN)
-Disadvantage: Tends to prefer splits that result in large # of partitions, each being small but pure

Gain Ratio
				     GAIN_(split)
GainRatio_(split) =	 ____________
					 SplitINFO

SpitINFO = -1* Sum_i( (n_i/n) * log( (n_i/n) )
					)
-normalizes Gain_(split) by SplitINFO
-parent node, p is split into k partitions n_i is the number of records in partition i

-Adjusts information gain by the entropy of the partitioning (SplitINFO)
-Large number of small partitions is penalized
-Designed to overcome the disadvantage of information gain 			

Decision boundaries for a decision tree
-graph 2 attribute (one on x and one on y)
-split on attribute 1 draws a line perpendicular to attribute 1's axis
-after doing this several times we have leaf nodes that encompass a box in the feature space
-when do we stop splitting
	-maybe by size of box (# of instances), or the range has to be large 

Stopping Criteria for Tree Induction
-creating new nodes is a recursive operation
	-base case 1:
		-stop when all the records in box belong to same class
	-base case 2:
		-stop when all the records have identical attribute values
		-if two instances were overlapping a lot

Self Test
-is this a good base case?
	-Base Case 3:
		-stop if all attributes have zero information gain
		-meaning entropy didn't decrease or GAIN ratio is zero
		-No! don't stop
			-if you split again after this happens then the classes will be separated perfectly

Common Decision Tree Algorithms
-ID3: Iterative Dichotomizer 3
		-uses information gain for binary splits
		-difficult to use on continuous data
			-requires data be discrete
-CART: classification and regression tree
		-uses Gini or Entropy for binary splits
			-only allows you to make binary splits
			-uses information gain- determines 1 binary split with dynamic programming algorithm
		-Scans sorted continuous values efficiently
-C4.5 (or J48 or C5.0)
		-uses gain ratio
			-allows for multiway splits
		-allows missing data (leaves out of entropy)
			-when using gain ratio it leaves missing data out
			-can still compare splits to each other
		-after creation, prunes tree to prevent overfitting

Example: C4.5
	-Simple Depth-first construction
	-uses information Gain and Gain Ratio
	-Sorts Continuous attributes at each node
	-needs entire data to fit in memory
		-how could we fix this?
		-could do memory mapping
		-could make histograms or KDEs of all the features
		-use it to approximate values for entropy

-------------------------------------------------------

Data Mining Lecture 11 Notes
(in class assignment)


First part is describe data set
Second part is using the Gini coefficient

gini_r is the gini index for the right side of split
gini_l is the gini index for the left side of split

combine the left and right with a weighted sum

-------------------------------------------------------

Data Mining Lecture 12 Notes

Demo Decision Trees--------------

rows in x is number of instances
-columns in x is attributes

StratifiedShuffleSplit - says use all of classes and 50 percent of data

multiclass log loss
	-use decision tree to score the probability of each class (p from class 1, class 2, etc.)
	-grab the correct one given the label, take the log of it, and add it
	-need to be correct, and as close to a probability of 1 as possible
-predict_proba returns the prediction probabilites of each class
-log loss is 26.467
	-want it to be smaller
Randomized PCA
-use this to transform the training data
	-dimensionality reduction
-log loss will get better as accuracy goes up

-alter DecisionTreeClassifier to not overfit
	-need to have a leaf node for each attribute
		-set max_leaf_nodes=121 (at least 1 per attribute)
		-setting this ignores max depth
		-can change min_samples_split to be like 300
		-max_depth
-log loss reflects how confident we are in our model being correct
-if you aren't worried about raw average maybe use log loss

Feature importances
-want to figure out which attributes are more important
	-decrease the gini the most
feature_importances_ shows average drop in gini of each
-can line up these feature importances back together as an image- will show which parts of image were most important part
-since its hard to see take the log to normalize the values

Everything scikit-learn implements is used for pre-pruning

-now do post-pruning
-sci-kit learn has not fully implemented pruning


Demo over----------------------

Decision Tree Overfitting
-overfitting results in decision trees that are more complex than necessary
-training error no longer provides a good estimate of how well the tree will perform on previously unseen records
-need new ways for estimating errors

Occam's Razor
-given 2 models of similar generalization errors, one should prefer the simpler model over the more complex model
-for complex models, there is a greater chance that it is fitting noise
-soooo.....we should include model complexity when evaluating splits of the decision tree

How to address overfitting
-post-pruning
	-grow decision tree to its entirety
	-trim the nodes of the decision tree in a bottom-up fashion
		-only look at sub-trees with 2 leaf nodes (for binary split)
	-if generalization error improves after trimming, replace sub-tree by a leaf node
	-class label of leaf node is determined from majority class of instances in the sub-tree
	-can use pessimistic error of chi-sq. for post pruning

Pessimistic Error for Pruning
-Re-substitution errors: # of errors on training set
	- sum (e(t))
-Generalization errors: # of errors on testing set
	- sum (e'(t))
-pessimistic approach to estimate e':
	-for each leaf node: e'(t) = (e(t) + .5)
	-total errors: e(T) + N*.5 where N is the total # of leaf nodes
	-For a tree w/ 30 leaf nodes and 10 errors on training (out of 1000 instances):
		Training error = 10/1000
		Pessimistic error = (10 + 30*.5)/1000 = 2.5%
		30*.5 is the penalty for a complex tree
-if cutting a branch off improves the pessimistic error then it gets better

The Chi Square Test (chi^2): Complex Trees

					MPG=Good   17
					MPG=Bad    4
					Gini 	   .308

split 3 ways
USA 
MPG=Good 	10
MPG=Bad 	0
Gini=0.0

Asia
MPG=Good 	5
MPG=Bad 	2
Gini=.408

Europe
MPG=Good 	2
MPG=Bad 	2
Gini=.5

Gini split = .301

Would we have seen this level of association "by chance" or "because the attribute is correlated with the class"?

	Test null hypothesis based on expected counts:
Null: split is by chance with XX% confidence (usually 95%)

The Chi Square Test 

							 (E_p(x=j)([x_k]-x_k,j))^2)
T=Sum_(j in C)Sum_(k in S)= ___________________________
						     E_p(x=j)([x_k])

-normalized sum of (count difference)^2
-S: all splits
-C: all classes
-Denominator =>Expected count for jth class in the kth split, based on the parent node
-Numerator X_k,j is actual count for the j_th class in the k_th split

parent node 		USA    Asia   Europe
	MPG=Good 		10 	   5 	  2 	 = 17
	MPG=Bad 		0 	   2      2 	 = 4
					=10    =7     =4 	 =21

Example:

	E_(p(x=good))([x_USA]) = x_USA * p(x=good) = 8.09
-expected value of the class being good for the USA attributed
			= 10 * 17/21 = 8.09
-actual value of USA being good is 10
-subtract these 2 numbers from each other, square them, and divide it by actual

if T > chi^2
-reject null hypothesis split is not by chance
if t < chi^2 
-accept null hypothesis split might be by chance

Example

		Exp(USA,Good) = 8.09
		Exp(Asia,Good) = 7 x17/21 = 5.66
-do this for all attributes and all classes
			USA 		ASIA  		EUROPE
MPG=Good 	10(obs) 	5(obs) 		2(obs)
			8.09(exp)   5.66(exp)   3.23(exp)

MPG=Bad 	0(obs) 		2(obs) 		2(obs)
			1.91(exp)   1.33(exp)   .76(exp)

Chi square test = 5.25
-can look up chi-squared based on how many degrees of freedom are allowed for chi-squared of .050
df = (rows-1)*(cols-1)
   = 1*2 = 2

   df    x^2 .050
   1 	 3.841
   2 	 5.991
   3     7.815

5.25 < 5.991

Not 95% confident split is not by chance


Self Test: Example of post-pruning

A
Class=yes 	20
Class=No 	10
Error = 10/30
Pesimisitc Error: (10+.5)/30 = 10.5/30

				
Mulit split
A1
Class=yes 	8
Class=No 	4
Error = 10/30

A2
Class=yes 	3
Class=No 	4
Error = 10/30

A3
Class=yes 	4
Class=No 	1
Error = 10/30

A4
Class=yes 	5
Class=No 	1
Error = 10/30

Pessimistic Error after splitting:
		=(9+4*.5)/30 = 11/30

if a split causes the pessimistic error to go up then prune it

Chi-square test = T = 2.793 < 7.815 while we want it to be > than so we prune it

Decision Tree Summary
-Advantages:
	-Inexpensive to construct
	-Extremely fast at classifying unknown records
	-variable importance through Gini reduction
	-Easy to interpret for small-sized trees
	-Accuracy is comparable to other classification techniques for many data sets

-------------------------------------------------------

Data Mining Lecture 13 Notes

Model Evaluation Measures

Metrics for Performanace Evaluation
-focus on the predictive capability of a model
 -not how fast it takes to classify or build models, scalibility, etc.
-confusion matrix
					Predicted Class
						Class=yes 		class=no
			 class=yes 		a 				b
Actual Class 
			 class=no 		c 				d

a: true positive
b: false negative
c: false positive
d: true negative

Metrics for performance evaluation
-most widely used metric:
					
				a+d 			TP + TN
	accuracy= __________ = ___________________
			   a+b+c+d 		TP + TN + FP + FN


confusion matrix is good because you might care about one of the squares more than the other ones.

Limitation of the accuracy
-ignores the cost of misclassifications
-consider an imbalanced 2-class problem
	-number of Class 0 examples = 9990
	-number of class 1 examples = 10
-if model predicts everything to be class 0, accuracy is 9990/10000 = 99.9%
	-accuracy is misleading because model does not detect any class 1 example

Superimpose cost matrix on top of confusion


Cost MAtrix

					Predicted Class
			 C(i|j)		Class=yes 		class=no
			 class=yes 	 C(Yes|Yes) 	C(NO|Yes)
Actual Class 
			 class=no 	 C(Yes|No) 		C(NO|No)

-define a cost function based on your expertise with problem:
	C(i|j): cost of misclassifying class j example as class i

Cost Matrix Example
					Predicted Class
			 C(i|j)		 	+ 		-
			  +		 	  	-1		100
Actual Class 
			  - 	 		1 		0

i.e. medical diagnosis costs?

What are the accuracy and cost of these two confusion matrices?
which classifier is "better" as derived by the accuracy and by cost?


Model M_1			Predicted Class
			 C(i|j)		 	+ 		-
			  +		 	  	150		40
Actual Class 
			  - 	 		60 		250

Accuracy = 80%
Cost = 3910

Model M_2			Predicted Class
			 C(i|j)		 	+ 		-
			  +		 	  	250		45
Actual Class 
			  - 	 		5		200

Accuracy = 90%
Cost = 4255

So if you go by accuracy you go by M_2, if you were going by the costs you would say M_1 in this case

-how do we know that the cost matrix is good?
	-verify with a Doctor if this is a good cost matrix
	-cost matrix could be economics, cost of someone dieing, etc.  

Cost-Sensitive Measures
				  a
Precision (p) = _____
				 a+c

-higher precision == lower false positives

			   a
Recall (r) = _____
			  a+b

-higher Recall == lower false negatives

				 2rp    	2a
F - measure(F) = ____ = ______________
 				 r+p 	 2a + b + c
higher F1 == lower FN and FP

precision is biased towards 
	C(p=yes|a=yes)& C(p=yes|a=no)
Recall is biased towards
	C(p=yes|a=yes) & C(p=no|a=yes)
F-measure is biased towards all except C(p=No|a=No)

							w_1*a + w_4*d
Weighted Accuracy = ______________________________
					 w_1*a + w_2*b + w_3*c + w_4*d



Medical diagnosis->Recall probably better because you'd want fewer false negatives

Retrieving similar documents from an online database
->b.c. if we say its similar, it better be similar


Model Evaluation
-how reliable are the estimates for performance?

Methods for Performance Evaluation
-how to obtain a reliable estimate of performance?
-performance of a model may depend on other factors besides the learning algorithm
	-class distribution
	-cost of misclassification
	-size of training and test sets

Learning Curve: Number of samples
-learning curve shows how accuracy changes with varying sample size
-effect of small sample size:
	-bias in the estimate
	-variance of estimate
-you cannot estimate this curve without collecting the data. Some bounds exist, but they are too loose to be useful
-to make one you train a bunch of different models with each sample size
	-average accuracy given sample size is the point in the curve

Bias Variance Tradeoff
-complex models can really fit the training data, giving lower bias 
-simpler models have trouble fitting data, resulting in higher bias
-but complex models can have high variance in their decision

How do we ensure our model is not overfitting to the data?

Methods of estimating generalization
-solution: use tesing set, and never, never, never, let the model see it
	-holdout
		-reserve 2/3 for training and 1/3 for testing
	-random subsampling
		-repeated holdout, with replacement
	-cross validation
		-partition the data in k disjoint subset
		-k-fold: train on k-1 partitions, test on the remaining one
		-Leave-one-out: k=n
	-stratified cross validation
		-select samples, keeping overall class distribution same for each fold
		-make sure that classes keep same distribution that they had from the whole sample in each inidividual fold
-if you use 10 fold cross validation then you are training your model on 90% of the data
	-if you don't expect to have that much training data in the future this would be bad

-still don't look at test data with cross validation
-take out test data to be used later
	-then do cross validation on the remaining training data
	-this is because you wouldn't know if the parameters generalize if you use too much of your data for training

Evaluating Binary Classification : ROC

ROC (Receiver Operating Characteristic)
-characterize trade-off b.w. positive hits and false alarms
-ROC curve plots TP (on the y-axis) against FP (on the x-axis)
-Performance of each classifier represented as a point on the ROC curve
	-changing the threshold of algorithm, sample distribution or cost matrix changes the location of the point

graph false positive rate vs true positive rate as we adjust some threshold
-if binary dat then chance would be x=y line

(TP,FP)
	(0,0): declare everything to be negative class-> means theres never a positive either way
	(1,1): declare everything to be positive class->
	on a graph of positive vs. positive then its always true
	(1,0): ideal-> always got the true positives correctly and never had a false positive
-Diagonal line:
	-random guessing for equal number of classes
	-below diagonal line:
		-prediction is opposite of the true class

Using ROC for model comparison
-no model consistently outperforms the other
	-M1 is better for small FPR
	-M2 is better for large FPR
-Area under the ROC curve
	ideal: Area = 1.0

How to Construct an ROC curve
	classifier score
instance# 		P(+|A) 		True Class
	6 			  .95 			+
	2 			  .93 			+
	5 			  .87 			-
	4 			  .85 			-
	9 			  .85 			-
	1 			  .85 			+
	10 			  .76			-
	8 			  .53 			+
	3 			  .43 			-
	7 			  .25 			+

-Use classifier that produces probability score for each test instance P(+|A)
-sort the instances according to P(+|A) in decreasing order
-Apply threshold, T, at each unique value of P(+|A)
-P(+|A) < T, is a negative class, else it is a positive class
-count the number of TP,FP, TN, FN at each threshold
- TP rate, TPR = TP/Positives
- FP rate, FPR = FP/negatives

Test of Significance
-Given 2 models
	-model M1: accuracy = 85% tested on 30 instances
	-model M2: accuracy = 75%, tested on 5000 instances
-can we say M1 is better than M2?
	-how much confidence can we place on accuracy of M1 and M2?
	-can the difference in performance measure be explained as a result of random fluctuations in the test set?

Comparing performance of 2 models
-given 2 models, M1 and M2, which is better?
	M1 is tested on D1 (size = n1), found error rate=e1
	M2 is tested on D2 (size = n2), found error rate=e2
	-assume D1 and D2 are independent
	-if n1 and n2 are sufficiently large, then
		e1 ~ N(mu_1, sig_1)
		e2 ~ N(mu_2, sig_2)
	approximate: 
									 e_i(1-e_i)
			variance=(sighat_i)^2 = ______________
										n_i
-probably comes form binomial distrib.
 which is approximate well by normal distr.

-To test if performance difference is statistically significant: d_t = e1-e2 <-estimate of mean diff.
				-d_t is the true mean
	- d ~ N(d_t,sig_t) where d_t is the true diff.
	-since D1 and D2 are independent, their variance adds up:
								 ~
		sig1^2 = sig1^2 + sig2^2 = sig1_hat^2 + 	
		^									sig2_hat^2
		|
		|	     e1(1-e1) 	e2(1-e2)
		|	   = ________ + _________
		|	   	  	n1 			n2
estimate of the variance in subtracted error rates
-sig1_hats is the variance as predicted by binomial distribution

	-at (1-alpha) confidence level, 
				d_t = d +- Z_(alpha/2)*sigt_hat


Z is a confidence interval
-use this to adjust confidence in mean
-does this interval include 0?
	-means error rates between 2 might not be that different from each
	-if it doesn't include 0 we can say statistically with some confidence level, one is better than the other

An illustrative example

-Given: M1: n1 = 30, e1=.15
		   M2: n2 = 5000, e2=.25

d = |e2 - e1| = .1 (2-sided test)

		   			.15(1-.15) 		.25(1-.25)
	(sighat_d)^2 =  ___________  +  ___________ =.0043
						30 				5000

-at 95% condience level, Z_(alpha/2) = 1.96

	d_t = .1 +- 1.96*sqrt(.0043) = .1 +- .128

=> interval contains 0 => difference may not be statistically significant

-"There are lies, damned lies and statistics."
	-Mark Twain

M1: n1 = 30, e1 = .15
M2: n2 = 5000, e2 = .25

Comparing Performance of 2 algorithms
-each learning algorithm may produce k models:
	L1 may produce M11, M12, ...., M1k
	L2 may produce M21, M22, ...., M2k
-if models are generated on the same test sets D1,D2,
....,Dk (e.g., via cross-validation)
	for  each set: compute d_j = e1j - e2j the jth difference

	dj has mean d and variance sig_t
				1
	sigt^2 = _______ SUM_j..k (dbar - dj)^2
			   k-1
	    ^
		|
		-now we can bound to get a better idea about how the accurance varies
					1
	d_t = dbar +- ________ *t_( 1-alpha, k-1) * sigt
				   sqrt(k)

	t(95%,k=10) = 2.26

Comparing Performance of 2 algorithms
-what about the fact that each model is computed on the same dataset?
-even if accuracies are similar, what if the errors are on different instances in the data?
-always a good idea to check the confusions b.w. the classifiers:

	Count 				Model M2
						Incorrect 		Correct
			
			incorrect 		a 				b

Model 		
M1 			correct 		c 				d

-------------------------------
	Prob. 				C2
						Incorrect(0) 	Correct(1)
			
		incorrect(0) 		.04 			.16

 		
C1 		correct (1)			.16 			.64


	
--------------------------------
	Prob. 				 C2
						Incorrect(0) 	Correct(1)
			
		incorrect(0) 		.18 			.02

 		
C1 		correct(1)	 		.02 			.78

its not ok to compare their accuracies 

Before Moving on...
-Is there any reason to prefer one learning method over another, if we are only interested in how the algorithm generalizes?

-should any one algorithm be superior to another in all situations?
	
	No, it depends on the data you have
	-no, there is no free lunch.. (there is always a trade off)

-------------------------------------------------------

Data Mining Lecture 14 Notes








