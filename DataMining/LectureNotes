CSE 7331 Data Mining Lecture Notes

------------------------------------------------------

Data Mining Lecture 1 Notes

Labs = 75% of grade
-when labs are due, no class that week, but you have 2 periods to just work on the lab
-high expectations
-comments, analysis, conclusion
-no late labs
In Class Assignment = 25% of grade
-5 assignments at 5% each

-Dr Larson, Professor Larson, prefers Eric

-Email Name, department, grad/ugrad
-something true/false

What is Data Mining?
-non-trivial extraction of implicit, previously unknown, and potentially useful information from data
-Exploration and analysis by automatic or semi-automatic means over large quantities of data in order to discover meaningful patterns

What is not Data Mining?
-looking up phone number in directory, or querying a DB or web search engine

What is Data Mining?
-certain names are more prevalent in certain US locations (O'brien, O'Rurke, O'Reilly... in Boston area)
-group together similar documents returned by search engine according to their context (e.g. Amazon rainforest)
-an algorithm that can group articles by looking at text and throwing them in a different bin accordingly


Data Mining Tasks
-Prediction
	-use some variables to predict unknown or future values of other variables
-Description
	-find human-interpretable patterns that describe the data
-Tasks
	-Classification (predictive)
	-Regression (predictive)
	-Deviation Detection (predictive)
	-Clustering (descriptive)
	-Association Rule Discovery (descriptive)
	-Sequential Pattern Discovery (descriptive)

icaggle

Classification
-Given a collection of records (training set)
	-each record contains a set of attributes, one of the attributes is the class
-find a model for class attribute as a function of the values of other attributes
-Goal: previously unseen records should be assigned a class as accurately as possible

Classification: Application 1
-Direct Marketing
-Goal: Reduce cost of mailing by targeting a set of consumers likely to buy a new cell-phone product
Approach:
	-use the data for a similar product introduced before
	-(buy, dont buy) decision forms the class attribute
	-Collect various demographic, lifestyle, and company-interaction related information about all such customers
-More data the better

Classification: Application 2
-Sky Survery Cataloging
-Goal: To predict class (star or galaxy) of sky objects, especially visually faint ones, based on the telescopic survey images (from Palomar observatory)
	-3000 images with 23,040 x 23,040 pixel/image
Approach:
	-segment the image
	-measure image attributes (features) - 40/object
	-model this class based on these features
	-success story: could find 16 new high red-shift quasars, some of the farthest objects that are difficult to find

Regression
-predict a value of a given continuous valued variable based on the values of other variables
-Examples:
	-predicting sales amounts of new product based on advertising expenditure
	-predicting wind velocities as a function of temperature, humidity, air pressure, etc.
	-predicting lung function as a function of gender, weight, height

Clustering Definition
-given a set of data points, each having a set of attributes, and a similarity measure among them, find clusters such that
	-data points in one cluster are more similar to one another
	-data points in separate clusters are less similar to one another
-similarity measures:
	-Euclidean Distance if attributes are continuous.
	-Other Problem: specific measures

Clustering: Application
-Document Clustering:
	-Goal: To find groups of documents that are similar to each other based on the important terms appearing in them
	-Approach: to identify frequently occurring terms in each document. Form a similarity measure based on the frequencies of different terms.
	-Gain: Information retrieval can utilize the clusters to relate a new document or search term to clustered documents
-Clustering Points:
	-3204 Articles of Los Angeles Times
-Similarity Measure
	-How many words are common in these documents (after some word filtering)

Association Rule Discovery: Definition
-Given a set of records each of which contain some number of items from a given collection:
	-produce dependency rules which will predict occurrence of an item based on occurrences of other items
-supermarket shelf management
	-Goal: Identify items that are bought together by sufficiently many customers .
	-Approach: Process the point-of-sale data collection with barcode scanners to find dependencies among items.
		-if a customer buys diapers and milk, then is very likely to buy beer
		-so, don't be surprised if you find six-packs stacked next to diapers

Types of Data and Categorization

What is data?
-Collection of objects and their attributes
-an attribute is a property or characteristic of an object
	Examples: eye color of a person, temperature, etc.
-a collection of attributes describe an object

Table Data
Rows=> referred to as objects, records, points, samples, cases, entities, instances, etc.

Cols=> referred to as Attributes, variables, fields, characteristics, features

Types of Attributes
-Different types of attributes
	Nominal
		Examples: ID numbers, eye color, zip codes
		-like categories, there isn't an order
	Ordinal
		Examples: rankings (e.g. taste of potato chips on a scale from 1-10), grades, height in (tall, medium, short)
		-order
	-Interval
		-Examples: calendar dates, temperatures in Celsius or Farenheit
	-Ratio
		-Examples: temperature in Kelvin, length, time, counts

Properties of Attribute Values
-the type of an attribute depends on which of the following properties it posseses
	-Distinctness:  =, !=
	-Order: 		< >
	-Addition: 		+ -
	-Multiplication: */

	Nominal- distinctness
	Ordinal- distinctness and order
	Interval- distinctness, order, and addition
	Ratio- all 4 properties

			Attribute
			Level		Tranformation		Comments
		 	
		  _	Nominal		Any Permutation		Would 
		 |				of values 			reassigning
Discrete-								all employee ID
		 |								# make a diff.?
		 |								
		  - Ordinal		an order of 	An attribute
		  				preserving 		encompassing
		  				change of 		notion of good,
		  				values. i.e.	better, best can
		  				new = f(old)	be represented
		  				where f is a 	equally well by
		  				monotonic 		the values (1,2,
		  				function		3) or (.5,1,10)

		  	_Interval	new_value = a*	Thus, farenheit
		   |			old_value + b 	and Celsius 
Continuous-				where a,b are 	temperature 
		   |			constants		scales differ in
		   |							terms of where
		   |							their zero value
		   |							is and the size
		   |							of a unit (
		   |							degree)
		   |
		   	-Ratio		new_value=a*	Length can be 
		   				old_value		measured in 
		   								meters or feet

Types of data sets
-Record
	-tables, frequency lists
-Graph
-Ordered
	-spatial data, temporal data, sequential data

----------------------------------------------------------------------------------------------------------------

Data Mining Lecture 2 Notes

1.5 hours for in class assignment.  Turn in what you got

Labels and Feature Vectors-> MachineLearning Algorithm

Data Quality Problems
-noise and outliers
	-remove as much as possible
-missing values
	-replace or ignore
-duplicate data
	-clean entries or merge

Outliers
	-outliers are data objects with characteristics that are considerably different than most of the other data objects in the data set

Missing Values
	-Reasons for missing values
		-information is not collected (e.g. people decline to give their age and weight)
		-Attributes may not be applicable to all cases (e.g. annual income for children)
		-UCI ML Repository: 90% of repositories have missing data
	-Handling missing values
		-eliminate Data objects
		-impute missing values
			-Stats-means,median,mode
			-cluster data without values and then take means,medians,and modes of the clusters
		-ignore the missing value during analysis
		-replace with all possible values (talk about later)

Data Representation
-intervals -> floats
-ratio -> float
-ordinal -> integer
-nominal -> boolean/integer (binary nominal),
			dictionary, one hot encoding

Bag of Words model
-separate out words as their own separate column and include term frequency, or inverse doc frequency

Feature hashing
-have a hashing function h(x) = y
-multiple words mapped to one feature (want to minimize collisions)
-example- take sume of all letters and modulo it by 8

-slide 43 lecture 2 has Panda tutorials

Make sure to have these installed
-pandas, anaconda or whatever
-matplotlib
-seaborn
-mpld3

--------------------------------------------------------

Data Mining Lecture 3 Notes

What is data exploration?
-A preliminary exploration of the data to better understand its characteristics
-key motivations of data exploration include
	-Helping to select the right tool for preprocessing or analysis
	-making use of human's abilities to recognize patterns
		-fact: people can recognize patterns not captured by data analysis tools. Visualization is the key

Techniques used in Data exploration
-in Exploratory Data Analysis (John Tukey)
	-the focus was visualization 
	-Clustering and anomaly detection were viewed as exploratory techniques 
	-In data mining, clustering anomaly detection are major areas of interest, and not thought of as just exploratory
-in our discussion of data exp, focus on 
	1. Summary Statistics
	2. Visualizations

Summary Statistics
	-numbers that summarize data
		mean -location
		standard deviation - spread
	-most summary stats can be calculated in a single pass

Measures of location
-mean and median
-mean is most common used
-mean is sensitive to outliers
-thus, median or trimmed mean is also commonly used

Measures of Spread: Range and Variance
-Range is the difference b.w. the max and min
-variance or st. dev. is the most common measure of spread
	variance(x) = (s_x)^2 = 1/(m-1)SUM(x_i - x_bar)^2
-however, this is also sensitive to outliers, so that other measures are often used
	AAD(x) = 1/m * SUM(abs_val(x_i - x_bar))

Skewness, kurtosis
-Comparison of the tails of a distribution
-negative skew is skew left
-positive skew is skew right

skewness(x) = 1/N * SUM((x_i - x_bar)/st.dev)^3

kurtosis -> skewness but the 4th power instead of the third
		 -> how peaked your data is

Frequency and Mode
-the frequency of an attribute value is the percentage of time the value occurs in the data set
	attribute gender, and a representative population,
	the gender female occurs 48.2% of the time
-the mode of an attribute is the most frequent attribute value
-the notions of frequency and mode are typically used with categorical data

Percentiles
-for continuous data percentiles is usually more meaningful
-give an ordinal or continuous attribute 
	the pth percentile is x_p means that p percent of the data is < x_p
-50th percentile x_50% is the median
-how efficient is one percentile calculation
	sorted array = O(1) unsorted array = O(N*logN)

Percentiles: approximate
-many times it is more useful to use an approximation of the percentile to avoid sorting time
-for data that fits in memory this usually is overkill: just sort
-1 solt-> sub sample the data
	-not good for percentiles close to 0 or 100
-for data that is too large to fit in memory, look at T-Digest

Data Preprocessing
-aggregation
	-combining 2 or more attributes (or objects) into a single attribute (or object)
-purpose
	-data reduction
		-reduce the # of attributs or objects
	-change of scale
		-cities aggregated into regions, states, countries, etc.
	-more "stable" data
		-aggregated data tends to have less variability

Mapping data to a new space
	-fourier transform
	-wavelet transform
-map to a different space 
	-take it to a space with different representation on the axes
	-instead of time vs. value, change to freq. vs value

Discretization using class labels
-minimize within interval entropy of class distributes
-break up data based on groupings and assign a new attribute to each group -- probably nominal if no order

Discretization without using Class labels
-can use intervals, which would turn continuous data into discrete data
-can do this in pandas with cut command
	cut(dataframe,var,[intervals])

Attribute Transformation
-a function that maps the entire set of values of a given attribute to a new set of replacement values s.t. each old value can be identified with one of the new values
	-simple functions x^k, log(x), e^x, abs_val(x)
	-standardization and normalization
	-polynomial and interaction variables

attribute transformation in python slide 21 lecture 3

Data visualization
-Visualization
	-conversion of data into a visual or tabular format so that the characteristics of the data and the relationships among data items or attributes can be analyzed or reported
-visualization of data is one of the most powerful and appealing techniques for data exploration
	-humans have developed ability to analyze large amounts of information that is presented visually
	-can detect general patterns and trends
	-can detect outliers and unusual patterns

Representation
-need to map information to a visual format 
-data objects, their attribtues, and the relationships among data objects are translated into graphical elements such as points, lines, shapes, and colors.
-Example:
	-objects are often represented as points
	-their attribute values can be represented as the position of the points or the characteristics of the points (e.g. color, size, shape)
	-if position is used, then the relationships of points, i.e. whether they form groups or a point is an outlier, is easily perceived

Arrangement is really important for humans
-is the placement of visual elements within a display
-can make a large difference in how easy it is to understand the data

Selection (people do not think in > 3D)
-need to eliminate or de-emphasize certain objects or attribtues
-selection may involve choosing a subset of attributes
	-dimensionality reduction is often used to reduce the number of dimensions to 2 or 3 
	-alternatively, pairs of attribtues can be aggregated
-selection may also involve choosing a subset of objects
	-a region of the screen can only show so many points
	-can sample, but want to preserve points in sparse areas

Visualization Techniques: Histograms
-Histogram
	-distributions of values of a single variable
	-divide the values into bins and show a bar plot of the # of objects in each bin
	-the height of each bar indicates the number of objects
	-shape of histogram depends on the number of the bins
	-basically a frequency plot

Two-dimensional histograms
-estimate the joint distribution of the values of 2 attributes
-ex: petal width and petal length
-tells you correlation

Visualization Techniques: Box plots
-box plots
	-John Tukey
	-another way of displaying the distribution of data
	-outliers as dots, whiskers at 10th and 90th percentiles, bottom box at 25th percentile, line in box at 50th percentile, and top box at 75th percentile
	-connected to violin plot
	-box plot will hide bimodality
-can have multiple box plots on same graph
	-allows you to compare attributes

Visualization Techniques: Scatter Plots
-Scatter plots
	-2D scatter plots most common, but can have 3D scatter plots
	-often additional attributes can be displayed by using the size, shape, and color of the markers that represent the objects
	-it is useful to have arrays of scatter plots can compactly summarize the relationships of several pairs of attributes

Edward Tufty
Hans Rosling

Matplotlib
-python plotting utility
	-has low level plotting functionality
	-highly similar to Matlab and R for plotting
-extended for visually be more beautiful by 
	-Seaborn: standard data visualization group

-read about PCA in your book (appendix B)


--------------------------------------------------------

Data Mining Lecture 4 Notes

groupings used in lecture 3
-class (1,2,3)
-age (child, adult, senior)
-gender( male, female)

seaborn demo
-correlation plot b.w. siblings and spouses
-violin plot -shows distribution
-box plot - survival with each plot being a category
-pair grid - smooth version of histogram, joint distribution 
-facet grid
-correlation plot organized by survival

integrating with D3
-makes plots html interactive
	-zoom in, move around in plot
	-javascript code generated in python
-if you can use mplD3 then use it

Curse of Dimensionality
-when dimensionality increases (# of attributes increases), data becomes increasingly sparse in the space that it occupies
-definitions of density and distance b.w. points, which is critical for clustering and outlier detection, becomes less meaningful
	-more features, means distance b.w. points is less meaningful
-exponential more points as you increase dimensions

Dimensionality Reduction
-purpose: -avoid curse of dimensionality
		  -reduce amount of time and memory required by data mining algorithms
		  -allow data to be more easily visualized
-techniques: -principle component analysis (Karl Pearson)
			 -discriminant analysis
			 -Others: supervised and non-linear techniques

Dimensionality Reduction: PCA
-goal is to find a projection that captures the largest amount of variation in data
-plot 1 attribute vs. other
-find an attribute that would follow closely to something like a best fit line of the plot
-eigenvectors of covariance matrix
-make data set zero mean and then take covariance between attributes
-higher covariance means more correlation
-take first eigenvector of covariance matrix (corresponds to the highest eigen value)
-then project the data points onto the eigen vector
	-represent the data as if it had one attribute
	-if EigenVector = [.85 .52] then .85*att1 + .52*att2

Karhunen-Loeve Transform
-take data points and multiply them by eigenvectors in PCA

Dimensionality Reduction: Randomized PCA
-problem: PCA on all that data can take a while to compute 
	-what if the # of dimensions is gigantic?
		-Actually that's okay: there are iterative algorithms for finding the largest eigenvalues that scales well with the number of data dimensions, but not the number of instances
		-except iterative solutions doesn't scale if the amount of data you have is huge
	-what if the # of instances is gigantic?
-what if we construct the covariance matrix with a subsample of the data?
	-by randomly sampling from the dataset, most of the time we get something representative of the PCA for the entire dataset

Dimensionality Reduction: LDA
-PCA tell us variance explained by the data in different directions, but it ignores class labels
-is there a way to find "components" that will help with discriminate b.w. the classes?

	arg max  SUM differences b.w. classes
	   comp. ----------------------------
			 SUM variance w.in classes

-called Fisher's discriminant
-...but we need to solve this using Lagrange multipliers and gradient-based optimization
-which we wont get to for 2-3 weeks

-PCA tries to give you a component in the direction of the greatest variance
-LDA tries to look at the classes
-differences b.w. classes is calculated by trying to separate the mean value of each feature in each class
-linear discriminant analysis:
	-assume the covariance in each class is the same
-Quadrature DA:
	-estimate the covariance for each class

Dimensionality Reduction: non-linear
-sometimes a linear transform is not enough
-but nonlinear algorithms tend to be pretty unstable or slow
-though many exist: non-linear PCA, ISOMAP, Manifold
-A powerful non-linear transform has seen a resurgence in past decade: kernel PCA
	-apply a transformation on each of your data points, take them into this higher dimensional space, and then do PCA on it

-------------------------------------------------------

Reviewed this material a second time for Assignment 1
-mostly syntax from demos


Assignment 1 Notes

Summary Statistics using Pandas

//import pandas

import pandas as pd

//load in the data to the data frame
df = pd.read_csv('csv path')

//grabs first 5 rows 
df.head()

//grabs last 5 rows
df.tail()

//can also load in from a database
import sqlite3
con = sqlite3.connect('data/heart_disease_sql')
df = pd.read_sql('SELECT * FROM heart_disease', con)
df.head()

//to access a column in dataframe
df.age
df('age')

//to remove a column
del df('site')

df.describe()
-summary statistics
	-count (# of records)
	-unique (# of unique)
	-top (most frequent thing occurring)
	-freq (freq of most common)
-treats all of these as nominal values

//to replace any ? marks in the data set with NaN (not a number)
//use numpy here just to get that nan value
import numpy as np
df = df.replace(to_replace = '?', value=np.nan)

//impute the values for everything that was missing
//fillina means fill everything thats NaN
//df.median creates a new dataframe with the median of each column
//so this replaces all NaNs with the corresponding median
df_imputed = df.fillina(df.median())

//some of these are numeric
numeric_features = ('age','rest_blood_press','cholesterol')

//go through each numeric feature columns
//get the values in matrix form and then return them as a float
for feat in numeric_features:
	df[feat] = df[feat].values.astype(np.float)

//if there is numeric data then it will take precedence over categorical for describe
df.describe()

//can now do something similar with categorical data
//this will take number data and make sure it is categorical
categ_features = ['is_male','chest_pain', 'high_blood_sugar'(etc)]

for feat in categ_features:
	df[feat] = pd.Categorical(df[feat].values.astype(np.float))


//make one series nbased on the data
//df[numeric_features] will create a new data frame 
//with only those attributes
//then get the mean of all those and save it to 
//series_mean

series_mean = df[numeric_features].mean()

//do same with categorical for median

series_median = df[categ_features].median()
cat_series = dp.concat((series_median,series_mean))

cat_series

//now it will replace the categ attributes with the median and the numeric values will be replaced with the mean
df_imputed = df.fillna(value=cat_series)

//can grab particular data really easy
//if class value is missing throw that record out
//can get some very quick summary statistics for someone who doesn't have heart disease
df_imputed[df_imputed.has_heart_disease==0].describe()

//can also group by with pandas
//says when someone has heart disease value = x then the median of the other attributes are y
df_imputed.groupby(by='has_heart_disease').median()

//this basically groups 1-4 together
//either has heart disease or doesn't and find the averages of all the other values based on if they do or don't have heart disease
df_imputed.groupby(by=df_imputed.has_heart_disease>0).mean()

//could do a group by major blood vessels then look at has_heart_disease to see if it is a classifier for someone who has heart_disease
df_imputed.groupby(by=df_imputed.major_vessels>2).mean()

//one hot encoded variables using get_dummies
tmpdf = pd.get_dummies(df_imputed['chest_pain'],prefix='chest')
tmpdf.head()

//get one hot encoding for all categorical variables
//pd.concat([*]) will concatenate all values inside *
//[** for col in categ_features] ->says for each col in categ_features do **
one_hot_df = pd.concat([pd.get_dummies(df_imputed[col],prefix=col) for col in categ_features], axis=1)


Data Visualization using Pandas


Histograms
-petal width, petal length, sepal width
-split continuous variables into bins on x-axis

2D Histograms
-have one attribute on x, another on z, and count on y
-shows correlation between 2 attributes

Box Plots
-have a plot for each attribute on x axis and values on y

violin plots
-estimation of distribution
-more meaningful for data if data has 2 values with high frequency
-box plot hides bimodality

Scatter plots
-2D scatter plots most common
-could also do scatter plot matrices
-show scatter plot of 5 attributes vs. 5 attributes

Visualization Iris Correlation Matrix
-arranged by class

Parallel Coordinates 
-take attributes on the x axis and values on y axis and connect between classes with a line
-then can have each class use a different color
-can see how attributes are correlated to their class and how each class is correlated to the other classes

//get all data types
df.types
df.info

//percent of individuals that died on the titanic
float(len(df[df.Survived==0]))/len(df)*100

//gets number of people that survived in each class
// number of people in each class
// percentage of people that survived in each class
df_grouped = df.groupedby(by='Pclass')
print df_grouped.Survived.sum()
print '------------------------'
print df_grouped.Survived.count()
print '------------------------'
print df_grouped.Survived.sum() / df_grouped.Survived.count()

//break up a continuous variable into intervals
//creates a new attribute in df called age_range
//rows with NaN will get skipped by aggregate stats
 df['age_range'] = pd.cut(df.Age,[0,16,65,1e6],3,labels=['child','adult','senior'])

//can do df.age_range.describe()

//group by class and age range
//print out % of people that survived based on their class and age range
df_grouped = df.groupby(by=['Pclass','age_range'])
print "Percentage of survivors in each group:"
print df_grouped.Survived.sum() / df_grouped.Survived.count() * 100

//use aggregation to impute values

df_grouped = df.grouby(by=['Pclass','SibSp'])

//takes the median value for each attribute when it is grouped by class and sibling and fills in the NaN with that value
df_imputed = df_grouped.transform(lambda grp: grp.fillina(grp.median()))
df_imputed[['Pclass','SibSp']] = df[['Pclass','SibSp']]

%matplotlib inline


BAR GRAPH
//plot the survival rate based on groups
import matplotlib.pyplot as plt
df_grouped = df_imputed.grouby(by=['Pclass','age_range'])
survival_rate = df_grouped.Survived.sum() / df_grouped.Survived.count()
survival_rate.plot(kind='barh')

//cross-tabulate is good to get groupings
//cross tabulate the df_imputed by the class and 
//age_range
//breaks down class and age range based on if you 
//survived or not
survival = pd.crosstab([df_imputed['Pclass'],df_imputed['age_range']],df_imputed.Survived.astype(bool))

//Can see a stacked bar graph
//divides count by sum to get rate
survival_rate = survival.div(survival.sum(1).astype(float),axis=0)
survival_rate.plot(kind='barh', stacked=True, color=['black','gold'])

//above was for the survived rate
//can do count of people that survived instead with sex 
//and class groupings
//if your yellow bar is big then a lot of your group 
//survived
survival_counts = pd.crosstab([df_imputed['Pclass'],df_imputed['Sex']],df_imputed.Survived.astype(bool))
survival_counts.plot(kind='bar',stacked=True, color=['black','gold'])


BOX PLOT

//plot price of fare based on class
//have a box plot for each value of class, with the range of fares shown on the y axis
df_imputed.boxplot(column='Fare', by= 'Pclass')

//subplots
vars_to_plot_separate = [['Survived','SibSp','Pclass'],['Age'],['Fare']]
plt.figure(figsize=(10,6))

for index, plot_vars in enumerate(vars_to_plot_separate):
	plt.subplot(len(vars_to_plot_separate)/2,2,index+1)
	ax = df_imputed.boxplot(column=plot_vars)
plt.show()


Scatter Matrix

from pandas.tools.plotting import scatter_matrix

ax = scatter_matrix(df,figsize=(15,10))
-diagonal is histogram

Parallel Coordinates

from pandas.tools.plotting import parallel_coordinates

df_sub = df[['Is_Grtr_50','Age','Hrs_Per_Week']]
df_normalized = (df_sub-df_sub.min())/(df_sub.max()-df_sub.min())
parallel_coordinates(df_normalized,'Is_Grtr_50')

Simplifying with Seaborn

import seaborn as sns
cmap = sns.diverging_palette(220, 10, as_cmap=True)

Correlation Plot
sns.set(style='darkgrid')

f, ax = plt.subplots(figsize=(9,9))

sns.corrplot(df,
			annot=True,
			sig_stars=True,
			diag_names=True,
			cmap=cmap,
			ax=ax)
f.tight_layout()

Violin Plot

pal = sns.cubehelix_palette(40, rot=-.5, dark=.3)
sns.violinplot(df_imputed[['Age','Hrs_Per_Week']], color=pal, groupby=df.Is_Grtr_50.sdfadaf)

Factor Plot

sns.factorplot('Age_range','Fare','Survived',df,
				kind="box",
				palette="PRGn")

Pair Grid

sns.set(style="white")

-uses kernel density estimation
g = sns.PairGrid(df,diag_sharey=False)
g.map_lower(sns.kdsplot, cmap="Blues_d")
g.map_upper(plt.scatter)
g.map_diag(sns.kdsplot,lw=3)

Facet Grid

sns.set(styles='darkgrid')
 
g= sns.FacetGrid(df, col="age_range", row="Sex", margin_titles=True)
g.map(plt.hist, "Survived", color="steelblue", lw=0)


Integrating with D3

import mpld3

sns.set(style="darkgrid")

g = sns.FacetGrid(df, col="age_range", row="Sex", margin_titles=True)
g.map(plt.hist, "Survived", color="steelblue", lw=0)

mpld3.display()
-don't use it for box plots or violin plots

-------------------------------------------------------

Data Mining Flipped Module 1 notes

Linear Regression

set of instances X used to predict output y

vectors
X(1)	X(2)	 X(3)  .... X(M)
x(1)_1						x(M)_1
x(1)_2 ...
x(1)_3						.
.							.
.
.
x(1)_N 						x(M)_N

trying to predict y(1) y(2)...y(M)
-try to find eq. that maps y to x
-in linear regression this is a linear eq.

yhat(1) = W'X(1) -> dot product of these 2 variables

W = w_1
	w_2
	w_3
	w_4

add in bias term  w_0
-data becomes 

X(1)	X(2)	 X(3)  .... X(M)
x(1)_0						x(M)_0
x(1)_1						x(M)_1
x(1)_2 ...
x(1)_3						.
.							.
.
.
x(1)_N 						x(M)_N

W = w_0
	w_1
	w_2
	w_3
	w_4

set all of x(.)_0 = 1

want to make yhat(1) as close to y(1) as possible

sum_i(y(i) - yhat(i))^2 = J -> objective function
-want to minimize this
-adding squared difference minimizes the error if we assume it only has Gaussian error

J(w) = sum_i(y(i) - W'x(i))^2

y(i) = W'x(i) + eps_x
				(error term)
				assume its Gaussian

if Gaussian it will have normal distribution 
-0 mean and some variance sigma^2
	N(0,sigma^2)

if we add a Gaussian distribution on then y(i) now also follows a Gaussian distribution with
N(W'x(i),sigma^2)
-average is increased variance stays the same

P(y(i) | x(i),W,sigma) = 
	
		  1 					(y(i) - W'x(i))^2
  __________________  * exp(-  ___________________ )
  sqrt(2*pi*sigma^2) 				2* sigma^2

Tot Prob = Prod_i(P(y(i) | x(i),W,sigma))

-want to maximize the Tot Prob	
-take ln of tot prob. It will change tot prob, but W will stay the same

					  	1
ArgMax_W( sum_i ( ln( _______))                
                )
 			
 ArgMax_W(
 		Sum_i(
 		
 		    1 	 			  (y(i) - W'x(i))^2
 ln( __________________ ) -  ___________________ 
      sqrt(2*pi*sigma^2) 		2* sigma^2

      		 )
      	)

Argmax_W does not depend on term with ln so we can just ignore that term completely

 		  		   1			
 ArgMax_W( -  ____________ * sum_i (y(i) - W'x(i))^2
      		   2* sigma^2
      	 )

      	 or 
      	 	   1
ArgMin_W( ____________ * sum_i (y(i) - W'x(i))^2
      	   2* sigma^2
      	 )

Maximizing total probability is Max likelihood estimation (MLE)
-assumes noise source is Gaussian distributed noise

Regression Objective Function Solution

Added the 1/2 in that we took from ArgMin 
	-we don't know anything about sigma so ignore it?

J(w) = 1/2*sum_i(y(i) - W'x(i))^2

y = y_1    X = <------(x(1))'-------->
	y_2		   <------(x(2))'-------->
	y_2		   <------(x(3))'-------->
	.		   			.
	.		   			.
	.		   			.
	y_M		   <------(x(M))'-------->
					N+1 columns
	
	W = w_0
		w_1
		.
		.
		.
		w_m

		XW = W'x(1)  = yhat
			 W'x(2)
			 W'x(3)
			 .
			 .
			 .
			 W'x(M)

remember want to minimize error between y and yhat

rewrite J(w) as

	J(w) = .5 * ( XW - y)'(XW - y)

normally to minimize a function you take derivative and set to 0

matrix equivalent of taking derivative is taking gradient
	-partial derivatives of J w.r.t w

Aside---------
d.dw((alpha*W -y)(alpha*W-y))
	-use product rule
	= 2*alpha*(alphaW - y)

matrix equivalent

grad.W(XW -y)'(XW-y) = 2X'(XW-y)

End Aside-------

grad.W (J(W)) = .5*2*X'(XW-y)

set = to 0

	0  = X'XW - X'y
	X'XW = X'y
	multiple both sides by inverse of X'XW

	W = (X'X)^-1 * X'y

	-this is the optimal sol't for our W vector for linear regression


X is matrix M x (N+1)

X'X  is matrix N+1 x N+1
-as # of instances grows, the complexity of taking inverse doesn't matter (still have a matrix of N+1 x N+1)
-as N gets really big this calculation gets really complicated
-N is attributes, M is rows

Steepest Descent

W = w_0
	w_1
	.
	.
	.
	.
	w_N
	a W for each attribute 

	lets say W = w_0
				 w_1

	draw a 2d representation of what a 3d vector would look like in space

Steepest Descent
	dot with circles around it
	-from the dot anywhere is uphill
	-we have a guess outside the circles
	-we want to make a decision s.t. each jump is the steepest direction down in relation to next inner circle
	-iteratively update w to be in the steepest direction

W_j := W_j + alpha d.dW_j (J(w))

-alpha adjusts size of each jump in valley
-want to calculate d.dW_j (J(w))

d.dW_j (J(w)) = .5 * sum_i ( d.dW_j((y(i) - W'x(i))^2))
-take derivate ...

	= sum_i (y(i)-W'x(i))x(i)_j

insert into W_j

W_j := W_j - alpha * sum_i (y(i)-W'x(i))x(i)_j

alpha = learning rate
-in code
W = W - alpha * sum_i (y(i)-W'x(i))x(i)
	 
-can use steepest descent to optimize J(W) to solve for coefficients of W

if W starts overlearning
	-meaning highly dependent on 1 or 2 parameters of X
	-overlearning based on sample and it won't generalize as well as it could
	-can adjust by also minimizing sum_j((W_j)^2)

Ridge Regression
J(w) = 1/2*sum_i(y(i) - W'x(i))^2 + roe*sum_j((W_j)^2)

slightly different minimum value by using smaller W


Linear Regression to Classification

-redefine y values from continuous numbers to a set
	y = {0,1} binary

binary classification problem

now
	yhat(i) = g(W'x(i)) => {0,1}
	-g just maps those values into 0 and 1
	-dirac delta

	if g(W'x) < 0 then yhat = 0
	   g(W'x) >= 0 then yhat = 1

J(w) = 1/2*sum_i(y(i) - g(W'x(i)))^2

-update function is same

W_j := W_j + alpha d.dW_j (J(w))


d.dW_j (J(w)) = sum_i((y(i) - g(W'x(i))) 
									*d.dW_j(g(W'x(i)))
					 )
	= sum_i((y(i) - g(W'x(i))) * 			   |
						x(i)_j(d.dthet(g(thet))|      )
			)								   |
											thet=W'x(i)
-saying find the derivate of the dirac delta function
-normally just ignore the d.dthet in this function
	-which is the dirac delta

d.dW_j (J(w)) = sum_i((y(i) - g(W'x(i))) * x(i)_j


Logistic Function

yhat(i) = g(W'x(i))
-use a better equation for g in g(theta)
				1
g(theta) = ________________
			1+ exp(-theta)
-logistic function
-theta is a scalar

				1
g(W'x) = ________________
			1+ exp(-W'x)

d.dW_j(g(W'x)) = -g(W'x(i))(1 - g(W'x(i)))x(i)_j


				learning rule
			 |---------------------|	

W_j := W_j - sum_i((y(i) - g(W'x(i))) *

				g(W'x(i))(1-g(W'x(i)))x(i)_j
				|--------------------------|
					back propagation

optimal values of W when g is sigmoid function
-easier to use with Steepest Desecent


Logistic Regression
-Classification algorithm
-not regression

still got a binary classification problem
yhat = {0,1}
						1
P(yhat =1 | x,W) = _______________
					1 + exp(-W'x)

							1
P(yhat=0 | x,W) = 1 -  _______________
						1 + exp(-W'x)

Want to maximize probabilities given instances we have
-similar to before

Define total probability given our instances and the model parameters

L(W) = prod_for all y=1 (P(y(i)=1 | x(i),W)) * 
	   prod_for all y=0 (P(y(i)=0 | x(i),W))

MLE

ARGMAX_W (L(W))



-------------------------------------------------------

Data Mining Lecture 8 Notes

-lec 5,6 he was gone
-lec 7 was jus inclass assignment


W' is the line mapping x(1) to y

then we used g(W'x(i)) > alpha
with alpha being the splitting point- median maybe

-how do we figure out what weights are so that this is a good classification

Demo Logistic Regression

import numpy as np

#vector from -5 to 5 with 100 points
x1 = np.linspace(-5,5,100)
x2 = np.linspace(-5,5,100)

#turnthese into a grid so you can graph them
x1, x2 = np.meshgrid(x1,x2)

# w for this case is [4 2]
yreg = 1 / (1 + np.exp(-(4*x1+2*x2)))

#when features are axes you're in the feature space
#drawing a line -> < line is class 0, > is class 1
#alpha here is .5

-------------------------
-the weights tell you where the cliff is 

L(W) = Prod_y(i)=1 (P (y(i)=1 | x(i),W)) *
	   Prod_y(i)=0 (P (y(i)=0 | x(i),W))

MLE	 = Prod_i ( P(y(i)=1| x(i),W)^y(i) *
	 			P(y(i)=0| x(i),W)^(1-y(i))
	 		  )

ln(L(W)) = l(W) = sum_i( y(i)*ln(P(y(i)=1)) + 
						(1-y(i))*ln(P(y(i)=0))
						)


l(W) is a contour plot
-choose initial value for w and choose direction of steepest descent (gradient)
-don't need to worry about local minima with logistic regression - there is 1 optimal value

grad_W (l(W)) = d.dW_1(l)
				d.dW_2(l)

:= is preceded by
W := W + alpha * grad_W (l(W))

l(W) = sum_i (y(i)* ln(g(x(i))) + (1-y(i))*ln(1-g(x(i))
											 )
			 )
			 			   1
d.dW_j(l) = sum_i(y(i)* _______ * d.dW_j(g(x(i))) +
						g(x(i))

				  	  (1-y(i))
					____________ *d.dW_j(1-g(x(i)))

				 	 (1-g(x(i)))
				 )
			    1
g(x(i)) = ______________  
		  1+exp(-W'x(i))

d.dW_j(g(x(i))) = g(x(i))*(1-g(x(i)))*(-x(i)_j)

plug these into above equation and cancel

d.dW_j(l) = sum_i ( y(i)*(1-g(x(i)))*(-x(i)_j) + 
					(1 - y(i))*g(x(i))*x(i)_j
				  )

= sum_i( (g(x(i)) - y(i))x(i)_j )

-this is the direction we need to move with each jump

-now plug into update equation

W := W + alpha sum_i( (g(x(i)) - y(i))x(i)_1 )
			   sum_i( (g(x(i)) - y(i))x(i)_2 )
			   			.
			   			.
			   			.
			   sum_i( (g(x(i)) - y(i))x(i)_N )

W := W + alpha* X'(g - y)

g is the vector of outputs when x(i) is used

y is what you are trying to predict (target)

g and y are column vectors

X' is the stacked instances (each instance is a row)
-> X'(g-y) is a column vector


c = {1 ..... C_T}

saying c can go from 1 to C_T -1
P(y(i)=c | x(i),W) =  exp(W'_c x(i))
					 _________________
					  1 + sum_(c'->C_T-1)of 
					  			(W'_c'*x(i))
					  		  )

					W_(c=1)
					W_(c=2)
				W =	.			each W_c_i is N long
					.
					.W_(c=C_T-1)

p(y(i)=C_T | x(i),W) = 				1
						___________________________
						1+ sum_(c=1->c_T-1) of
						exp(W'_c * x(i))

l_new(W) = l(W) - b*sum_j( (W_j)^2)
					|--------------|
					  Regularize


W := W + alpha*X'*(g(x) - y) 		BATCH

W := W + alpha*(g(x(i)) - y(i))x(i)

Stochastic gradient descent
-keep trying jumps till you get a correct one, and will slowly zero in on minimum
-combine with batch to make more stable

Mini-Batch
 W:= W + alpha* X'_sub(g(x_sub) - y(sub))

More Demo-------------------

#class weight is how important a class is
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import load_iris

ds = load_iris()

#default c = 1
obj = LogisticRegression()

obj.fit(ds.data[:75],ds.target[:75])

ysub = ds.target[76:len(ds.data)
feat_sub = ds.data[76:len(ds.data)
yhat = obj.predict(feat_sub)

sum(yhat == ysub)




-----------------------------

Assignment 2 Notes
-take data set we already had 
-use logistic regression use fort vector machine on it
-vary parameters in each and see how well you can classify the data and interpret it


-------------------------------------------------------

Data Mining Lecture 9 Notes


Support Vector Machines

x(i) = x(i)_1
	   x(i)_2

Want to find the equation for the line that falls in the middle of the 2 clusters

Have W that will be perpendicular to this line and will go through the data

-take the dot product of a data point and W and if the scalar value is < C then 0, if it is >= C then 1
w . x(i) >= c then class + 
			  else class -

w . x(i) + b >= 0
y(i) = + => 1
y(i) = - => -1

w . x(i) + b >= 0, when  y(i) = 1
			 < 	0, when y(i) = -1

y(i)*(w . x(i) + b) >= 0

w has 1 entry for each attribute 

w = w_1
	w_2
						   W
Margin = (x(+) - x(-)) . _____ 
						 ||W||


x(+)W + b >= 0
x(+)W + b >= alpha
-(x(-)W + b) = alpha
x(+)W = alpha -b
-x(-)W = alpha + b

Plug into margin
				  1
(x(+)W - x(-)W)* ____
				 ||W||

alpha-b + alpha + b

  						1
(alpha-b + alpha + b)* _____
				 	  ||W||

 2*alpha
_________ = MARGIN
 ||w||

 MINIMIZE ||W|| 

 Want this MINIMIZE .5||W||^2 to be true s.t. 
 -y(i)*(Wx(i) + b) <= 0


min f(W) s.t. g_i(w) <= 0

Primal

min_W Max_alpha_i f(W) + sum_i (alpha_i*g_i(W))
-maximize with respect to alpha_i and then minimize w.r.t W


DUAL
max_alpha_i Min_W f(W) + sum_i (alpha_i*g_i(W))
-minimize w.r.t W and then maximize w.r.t alpha_i


Dual = max_alpha_i Min_W .5||W||^2 - 
					sum_i(alpha_i*y(i)*(W*x(i) + b))

grad_b(duel) = sum_i(alpha_i * y(i)) = 0
grad_w(duel) = 0 = W - sum_i(alpha_i * y(i) * x(i))
				W = sum_i(alpha_i * y(i) * x(i))

DUAL = .5(sum_i(alpha_i * y(i) * x(i)))*
		 (sum_j(alpha_j * y(j) * x(j))) - 
		 (
		  (sum_i(alpha_i * y(i)*b)) +
		  (sum_i(alpha_i * y(i) * x(i))) *
		  ((sum_j(alpha_j * y(j) * x(j))))
		 )

	 = .5 sum_i (sum_j( 
	 			  alpha_i*alpha_j*y(i)*y(j)*x(i).x(j))
	 			      )
	 			)
	 	- sum_i (sum_j( 
	 			  alpha_i*alpha_j*y(i)*y(j)*x(i).x(j))
	 			      )
	 			) 
	 = - .5sum_i (sum_j( 
	 			  alpha_i*alpha_j*y(i)*y(j)*x(i).x(j))
	 			      )
	 			) - sum_i( alpha_i^2)

-most of the alphas = 0
-once you know alphas you can figure out W and then you can figure out b

W*x(test) + b = sum_i (alpha_i * y(i)*<x(i),x(test)>)+b
-don't need to know x's
-just need to know what the <x(i),x(test)> is 

zeta(i) is distance from W that a data point is

Dual + C* sum_i ( (zeta_i^2) )
					|
				  slack
want to minimize slack as much as possible

zeta(i)*(x(i)W + b) >= 0
-set the value of C -> says how much you value the zetas
-zeta is some value greater than 0 for each instance


phi(x(i)) takes x(i) from one dimension and puts it in another

= -.5 sum_i(sum_j(alpha_i*alpha_j*y(i)*y(j)*
					<phi(x(i)),phi(x(j))> + epsilon

DEC = sum_i(alpha_i*y(i)*<phi(x(i)),phi(x(test))> + b)
Func

Aside----------------
q = 1
	sqrt(2)*q
	q^2

<q,q> = 1 + 2qq' + q^2(q')^2
	  = (1 + qq')^2 = Kernel

-----------------------------

K(q,q') = (1 + qq')^K
			polynomial

K(q,q') = exp(-||q-q'||^2 *lambda)
				Radial basis

C -> how much you care about slack variables
-if you make it zero it means you don't care about slack variables
-try .1, 1, 10
-make it bigger if you want fewer slack variables



-------------------------------------------------------

Data Mining Lecture 10 Notes


Assignment 2 Due Saturday 

create logistic regression model and complex vector support model that classifies your model
-discuss advantages of 1 model over the other by analyzing results
-separate your data into testing and training
	-80% training, 20% testing
-go in and look at the weights from logistic regression and determine the importance of different features
-go in and determine where the support vectors are, and what they look like
-determine how useful your model is to interested parties
	-how would they use it


Demo Notes---------------------------------

import pandas as pd
import numpy as np

df = pd.read_csv('data/titanic.csv')

#use numbers for labels so that it creates an ordinal 
#attribute
df_imputed['age_range'] = pd.cut(df_imputed.Age,[0,16,24,65,1e6],4,labels=[0,1,2,3]) 

df_imputed.age_range = df_imputed.age_range.astype(np.int)

#objects in a pandas dataframe - categorical

#set up 1 hot encoding for categorical data with more
# than 2 unique
tmp_df = pd.get_dummies(df_imputed.Embarked,prefix='Embarked')
df_imputed = pd.concat((df_imputed,tmp_df),axis=1)

#make sure this is also numerical
df_imputed['IsMale'] = df_imputed.Sex=='male'
df_imputed.IsMale = df_imputed.IsMale.astype(np.int)

#clear out old Sex and Embarked since we replaced them

#create new variable
df_imputed['FamilySize'] = df_imputed.Parch + df_imputed.SibSp

#Training and Testing Split
#randomly go in and grab 80% of data

from numpy import random as rd
 y = df_imputed['Survived'].values
 del df_imputed['Survived']
 X = df_imputed.values

#total number of instances
 N = len(df_imputed)
permuted_indices = rd.permutations(N)

N_eighty_percent = int(0.8*N)

x_train = X[permuted_indices[:N_eighty_percent]]
x_test = X[permuted_indices[N_eighty_percent:]]

y_train = y[permuted_indices[:N_eighty_percent]]
y_test = y[permuted_indices[N_eighty_percent:]]


from sklearn.linear_model import LogisticRegression

#get object
#l2 is the mean squared error of our output
lr_elf = LogisticRegression(penalty='l2', c=1.0, class_weight=None)

#train object
lr_elf.fit(x_train,y_train)

#get test set 
#gives output on test data from the model we trained
y_hat = lr_elf.predict(x_test)

from sklearn import metrics as mt

#wanna test our predicted y values (y_hat) against y
acc = mt.accuracy_score(y_test,y_hat)

#gives accuracy and tendency of confusing 2 classes
#with eachother
#scoring and precision recall
#for later assignments we should do this many times 
#as the permutation matrix will vary each time
conf = mt.confusion_matrix(y_test,y_hat)
print acc
print conf


#interpret the weights
for coef, name in zip(lr_elf.coef_.T, df_imputed.columns):
	print name, 'has an importance of ', coef[0]

#the largest magnitude of weight is the most important
#attribute to the model
#if negative weight then it is negatively associated to survival

from sklearn.preprocessing import StandardScaler

#scale attribute by the training set
scl_obj = StandardScaler()
scl_obj.fit(x_train)
x_train_scaled = scl_obj.transform(x_train)
x_test_scaled = scl_obj.transform(x_test)

#train the model just as before
#can play around with c value
lr_elf = LogisticRegression(penalty='12',c=0.05)
lr_elf.fit(x_train_scaled,y_train)

y_hat = lr_elf.predict(x_test_scaled)

from sklearn import metrics as mt

acc = mt.accuracy_score(y_test,y_hat)

#gives accuracy and tendency of confusing 2 classes
#with eachother
#scoring and precision recall
#for later assignments we should do this many times 
#as the permutation matrix will vary each time
conf = mt.confusion_matrix(y_test,y_hat)
print 'accuracy', acc
print conf

#sort these attributes and spit them out
zip_vars = zip(lr_elf.conf_.T,df_imputed.columns)
zip_vars.sort(key = lambda t: np.abs(t[0]))
for conf, name is zip_vars:
	print name, "has importance of", coef[0]

from matplotlib import pyplot as plt
%matplotlib inline

weights = pd.Series(lr_elf.conf_[0],index=df_imputed.columns)
weights.plot(kind='bar')


Support Vector Machines

from sklearn.svm import SVC

#cost multiplied the sum of Zs (slack variables)
#kernel= linear means no kernel
#higher kernel means transform data into higher degree
svm_elf = SVC(C=1.0, kernel='linear', degree=1, gamme=0.0)
svm_elf.fit(x_train_scaed, y_train)

y_hat = svm_elf.predict(x_test_scaled)

acc = mt.accuracy_score(y_test, y_hat)
conf = mt.confusion_matrix(y_test,y_hat)
print 'accuracy', acc
print conf

print svm_elf.support_vectors_.shape
print svm_elf.support_.shape
print svm_elf.n_support_

print svm_elf.coef_
weights = pd.Series(svm_elf.coef_[0],index=df_imputed.columns)
weights.plot(kind='bar')

df_support = df_imputed.iloc(svm_elf.support_,:)
df_support['Survived'] = y(svm_clf.support_)
df_imputed['Survived'] = y
df_support.info()

from pandas.tools.plotting import boxplot

#group support data set by survived and original data set by survived
df_grouped_support = df_support.groupby(['Survived'])
df_grouped = df_imputed.groupby(['Survived'])

vars_to_plot = ['Age','PClass','IsMale','FamilySize']

for v in vars_to_plot:
	plt.figure(figsize=(10,4))
	#plot support vector stats
	plt.subplot(1,2,1)
	ax = df_grouped_support[v].plot(kind='kds')
	plt.legend(['Perished', 'Survived'])
	plt.title(v+' (Support)')

	#plot original distribution
	plt.subplot(1,2,2)
	ax = df_grouped[v].plot(kind='kds')
	plt.legend(['Perished','Survived'])
	plt.title(v+' (Original)')

#this says there is a lot of variablity in the original data, but not a lot b.w. classes 
-the tough cases where there isn't a lot of variability where you are trying to predict

Gradient Based Alternatives
lecture at 1 hour


-------------------------------------------------------

Data Mining Flipped Module 2 Lecture Notes

Decision Trees
-just another machine learning algorithm
-will take features and attribtues and try to come up with a predictive algorithm
-table data
	-rows are instances, cols are attributes ()

-decision trees help with classification
	-look at an attribute and based on value decide if you will go left or right
	-for binary it would be yes or no

							preg
				-yes 				-no
			pos. diagnosis				eye color
									blue 		hazel
								BMI				brown
							<30		>30		-neg. diag.
							Neg.	Pos.


Create a machine learning algorithm that goes through tree
-then can predict with model

-start at root of tree and compare attribute of instance
-there's more than one way to fit the same training data
-finding the optimal model is very difficult
	-NP complete problem
-you'd have to look at every possible tree

Use Hueristics

Self Test
-what makes an optimal decision tree (from the following)?
	-accuracy
	-easy to interpret by a human
	-minimum tree depth
	-extensible to large datasets
-# of leafnodes is not as important
-many leaf nodes on branches near the root (generally not worried about)

Decision tree Classification task
-use induction
-most stem from Hunt's algorithm

General Structure of Hunt's Algorithm
-have a Node t -> D_t = (2,4,7)
-let D_t be the set of training records that reach a node t
1. if all records in D_t are the same class y_t then t is a leaf node labeled as y_t
2. If D_t is an empty set, then t is a leaf node labeled by the default class y_d
3.  IF D_t contains records that belong to more than one class, use an attribute test to split the data into smaller subsets from D_t
4.  Recursively apply the procedure to each subset

Self test
-splitting the data into smaller subsets 
-given you are in a node with these attributes, should you split the node or make it a leaf node?
	-there area only 2 unique rows
	-splitting it would have every row go either left or right, so make a leaf node

Tree Induction: Splitting on Attributes
-Greedy strategy.
	-split the records based on an attribute test taht optimizes certain condition
-issues 
	-determine how to split
		-how to specify the test condition?
		-how to determine the best split?
	Determine to stop splitting

How to specify test condition?
-depends on attribute types 
	-nominal 
	-ordinal
	-continuous
-depends on number of ways to split
	-2-way split
	-multi-way split

Splitting based on nominal and ordinal
-multi-way split: use many partitions as distinct values
	-cartype-family
			-sports
			-luxury
	-not usually done
-binary split: Divides values into 2 subsets.
			-need to find optimal partitioning
		Car type 						cartype
-luxury 		-family 		-family 		sports
 sports 						 luxury 


Self test
-draw the different ways to split this attribute?
	-attribute size: small, medium, large

small/medium 						large
small 								medium/large
small  			medium 				large
Don't do this next one
-keep the ordinals ordered
small/large 						medium

-come up with binary split for all nominal or ordinal attributes

Splitting based on continuous variables
-different ways of handling
	-discretization to form an ordinal categorical attribute
		-static - discretize once at the beginning
		-dynamic- ranges can be found by equal interval bucketing, equal frequency bucketing (percentiles), or clustering
	-binary decision: (A < v) or (A>=v)
		-consider all possible splits and finds the best cut 
		-can be more compute intensive

Taxable income
-binary split ex
	-most common
	Taxable income is >80k or not
-multiway split
	-<10 k, 10k-25k,25k-50k,50k-80k,>80k
	-this is better done by discretizing the data and making it ordinal

How to determine the best split
-which test condition is most pure?
	- node with 20 attributes
				C0: 10
				C1: 10
				- 10 in class zero, 10 in class 1
	-could split by own car splitting it into 2 groups of  C0: 6      					C0: 4
		C1: 4 						 C1: 6

	-could split by car type with 3 groups
	C0:1 				C0:8 			CO:1
	C1:3 				C1:0 			C1:7

	-could split by student ID
		-never do this

How to determine best split?
-greedy approach:
	-we prefer nodes that are pure, homogeneous
-need a measure of node impurity
	C0: 5 							C0:9
	C1: 5 							C1:1
	not homogeneous					homogeneous
	-not pure=highly impure 		very pure = not
												impure

How to Find the Best Split
-  								classes 	#records
											in class
			Before Splitting:    C0 		N00
 								 C1 		N01

 Look at each attribute as a candidate split
 M is measure of impurity

 Self test
 -if you have 3 different binary attributes and a node that you want to split, how many "impurities" do you need to calculat
 	-you have 3 attributes each get split into 2
 		-6 nodes, so 6 impurities


Measures of Node impurity
-gini index
-entropy
-information gain
-gain ratio

Measure of impurity: GINI
-looks at distribution of classes
-if I randomly grab a record from this node, what is the probability it is not correct?
-Gini index for a give node t:
		GINI(t) = sum_j( p(j | t)*( 1- p(j | t) )
					   )
			     -at node t, multiple the odds of selecting class and not selecting class
			     -sum this up for all classes
	-also 
		GINI(t) = 1 - sum_j ( (p(j|t)^2 )

	Example:
				C1 	1  P(C1) = 1/6
				C2  5  P(C2) = 5/6
				Gini = 1 - (1/6)^2 - (5/6)^2 = .278

Measure of Impurity: GINI
-If I randomly grab a record from this node, what is the probability it is not correct?
-Gini index for a given node t:
		GINI(t) = 1 - sum_j ( (p(j|t)^2 )
	Maximum (1-1/n_c)
	-records are equally distributed among all classes
	Minimum (0.0)
	-all records belong to one class
	-more impure, the higher the GINI

self test: examples for computing GINI
		C1 0    Gini = 1 - P(C1)^2 - P(C2)^2 = 0
		C2 6

		C1 1 	Gini = 1 - P(C1)^2 - P(C2)^2
		C2 5		 = 1- (1/6)^2 - (5/6)^2 = .278

		C1 2    Gini = 1 - P(C1)^2 - P(C2)^2
		C2 4		 = 1 - (2/6)^2 - (4/6)^2 = .444

Splitting Based on GINI
-how to combine Gini indices from children nodes?
-when a node p is split into k partitions(children), the quality of split is computed as

	Gini_split = Sum_i=1 (n_i/n * GINI(i))

	n_i is number of instances inside the child
	-n = total # of instances at node t
-weighted summation

Binary Attribtues: Computing Gini index
-splits into two partitions
-effect of weighing partitions
	-larger, purer partitions

		Parent 
			C1 6
			C2 6
				B?
			yes 	No
		Node N1 	  Node N2
				N1 | N2
			C1  5  | 1
			C2  2  | 4

Gini N1
=1 -(5/7)^2 - (2/7)^2
=.408

Gini N2
= 1 - (1/5)^2 - (4/5)^2
= .320

Gini(children) = 7/12*.408 + 5/12*.320
			   = .37

Categorical Attribtues: Computing Gini Index
-for each distinct value, gather counts for each class in the data set
-use the count matrix to make decisions
-can calculate Gini from two-way split and use the lower value as the better split (more impure)
-multi-way split always has lower Gini

Continuous Attributes: Computing Gini Index
-Use binary decisions based on one value
-many choices for splitting
	number of possible splitting valus
	= number of distinct values
-each splitting value has a count matrix associated with it
	-class counts in each of the patitions
			A >v A<=v
-simple method to choose best v
	-for each v, scan the database to gather count matrix and compute its Gini index
	-computationally inefficient! repetition of work


-efficient computation: sort the attribute
	-take attribute, transpose it to be horizontal and sort it
	-split positions
	-determine how many are in each class for each potential split position and calculate corresponding gini
		-choose where the minimum gini is to be the split position
	-really only have to calculate the Gini where the class value changes (local minimum) to find which is the extrema

Alternative Splitting Criteria based on INFO
-Entropy is a measure of randomness
-want something with low entropy - not very chaotic

Entropy at a give node t:
	Entropy(t) = sum_j ( p(j|t)*log( 1/( p(j|t) ) )
					   )
(Note: p(j|t) is the relative frequency of class j at node t)

-expected number of bits needed to encode a randomly drawn value efficiently
-entropy == uncertainty, which we want to minimize
-if something happens more frequently, we want to use fewer bits to represent it
	-if it happens more rarely we can use more bits
-if all characters occur around the same amount then there is a high level of entropy and there is not a whole lot of reduction that can be done

	Entropy(t) = -1*sum_j ( p(j|t)*log( p(j|t) )
					   )

	Ex: C1  1
		C2  5 	P(C1) = 1/6 	P(C2) = 5/6
				Entropy = -(1/6)log_2(1/6)
						  -(5/6)log_2(5/6) = .65

-Measures homogeneity of node
	-maximum (log n) when records are equally distributed among classes (implying least information)
	-minimum (0.0) when all records belong to one class, implying most information

-Entropy based computations give similar splits as the Gini impurity
	-when we combine entropy it becomes better

Self test:
	Entropy(t) 
-it will always be positive
	log(p(j|t)) is always negative
	p(j|t) is always positive
	-then you take the negative

self test: examples for computing Entropy
		C1 0    Entropy = -P(C1)*log(P(C1)) 
		C2 6			  - P(C2)*log(P(C2)) = -0-0=0

		C1 1    Entropy = -P(C1)*log(P(C1)) 
		C2 5			  - P(C2)*log(P(C2)) 
						= -(1/6)*log(1/6) 
						  - (5/6)*log(5/6) = .65

		C1 2    Entropy = -P(C1)*log(P(C1)) 
		C2 4			  - P(C2)*log(P(C2)) 
						= -(2/6)*log(2/6) 
						  - (4/6)*log(4/6) = .92


Splitting Based on INFO...
-Information Gain:
	GAIN_split = Entropy(p) - (sum_i ( (n_i/n) * 	
											Entropy(i)
							   		 )
							  )
-measures reduction in entropy achieved because of the split.  Choose the split that achieves most reduction (maximizes GAIN)
-Disadvantage: Tends to prefer splits that result in large # of partitions, each being small but pure

Gain Ratio
				     GAIN_(split)
GainRatio_(split) =	 ____________
					 SplitINFO

SpitINFO = -1* Sum_i( (n_i/n) * log( (n_i/n) )
					)
-normalizes Gain_(split) by SplitINFO
-parent node, p is split into k partitions n_i is the number of records in partition i

-Adjusts information gain by the entropy of the partitioning (SplitINFO)
-Large number of small partitions is penalized
-Designed to overcome the disadvantage of information gain 			

Decision boundaries for a decision tree
-graph 2 attribute (one on x and one on y)
-split on attribute 1 draws a line perpendicular to attribute 1's axis
-after doing this several times we have leaf nodes that encompass a box in the feature space
-when do we stop splitting
	-maybe by size of box (# of instances), or the range has to be large 

Stopping Criteria for Tree Induction
-creating new nodes is a recursive operation
	-base case 1:
		-stop when all the records in box belong to same class
	-base case 2:
		-stop when all the records have identical attribute values
		-if two instances were overlapping a lot

Self Test
-is this a good base case?
	-Base Case 3:
		-stop if all attributes have zero information gain
		-meaning entropy didn't decrease or GAIN ratio is zero
		-No! don't stop
			-if you split again after this happens then the classes will be separated perfectly

Common Decision Tree Algorithms
-ID3: Iterative Dichotomizer 3
		-uses information gain for binary splits
		-difficult to use on continuous data
			-requires data be discrete
-CART: classification and regression tree
		-uses Gini or Entropy for binary splits
			-only allows you to make binary splits
			-uses information gain- determines 1 binary split with dynamic programming algorithm
		-Scans sorted continuous values efficiently
-C4.5 (or J48 or C5.0)
		-uses gain ratio
			-allows for multiway splits
		-allows missing data (leaves out of entropy)
			-when using gain ratio it leaves missing data out
			-can still compare splits to each other
		-after creation, prunes tree to prevent overfitting

Example: C4.5
	-Simple Depth-first construction
	-uses information Gain and Gain Ratio
	-Sorts Continuous attributes at each node
	-needs entire data to fit in memory
		-how could we fix this?
		-could do memory mapping
		-could make histograms or KDEs of all the features
		-use it to approximate values for entropy

-------------------------------------------------------

Data Mining Lecture 11 Notes
(in class assignment)


First part is describe data set
Second part is using the Gini coefficient

gini_r is the gini index for the right side of split
gini_l is the gini index for the left side of split

combine the left and right with a weighted sum

-------------------------------------------------------

Data Mining Lecture 12 Notes

Demo Decision Trees--------------

rows in x is number of instances
-columns in x is attributes

StratifiedShuffleSplit - says use all of classes and 50 percent of data

multiclass log loss
	-use decision tree to score the probability of each class (p from class 1, class 2, etc.)
	-grab the correct one given the label, take the log of it, and add it
	-need to be correct, and as close to a probability of 1 as possible
-predict_proba returns the prediction probabilites of each class
-log loss is 26.467
	-want it to be smaller
Randomized PCA
-use this to transform the training data
	-dimensionality reduction
-log loss will get better as accuracy goes up

-alter DecisionTreeClassifier to not overfit
	-need to have a leaf node for each attribute
		-set max_leaf_nodes=121 (at least 1 per attribute)
		-setting this ignores max depth
		-can change min_samples_split to be like 300
		-max_depth
-log loss reflects how confident we are in our model being correct
-if you aren't worried about raw average maybe use log loss

Feature importances
-want to figure out which attributes are more important
	-decrease the gini the most
feature_importances_ shows average drop in gini of each
-can line up these feature importances back together as an image- will show which parts of image were most important part
-since its hard to see take the log to normalize the values

Everything scikit-learn implements is used for pre-pruning

-now do post-pruning
-sci-kit learn has not fully implemented pruning


Demo over----------------------

Decision Tree Overfitting
-overfitting results in decision trees that are more complex than necessary
-training error no longer provides a good estimate of how well the tree will perform on previously unseen records
-need new ways for estimating errors

Occam's Razor
-given 2 models of similar generalization errors, one should prefer the simpler model over the more complex model
-for complex models, there is a greater chance that it is fitting noise
-soooo.....we should include model complexity when evaluating splits of the decision tree

How to address overfitting
-post-pruning
	-grow decision tree to its entirety
	-trim the nodes of the decision tree in a bottom-up fashion
		-only look at sub-trees with 2 leaf nodes (for binary split)
	-if generalization error improves after trimming, replace sub-tree by a leaf node
	-class label of leaf node is determined from majority class of instances in the sub-tree
	-can use pessimistic error of chi-sq. for post pruning

Pessimistic Error for Pruning
-Re-substitution errors: # of errors on training set
	- sum (e(t))
-Generalization errors: # of errors on testing set
	- sum (e'(t))
-pessimistic approach to estimate e':
	-for each leaf node: e'(t) = (e(t) + .5)
	-total errors: e(T) + N*.5 where N is the total # of leaf nodes
	-For a tree w/ 30 leaf nodes and 10 errors on training (out of 1000 instances):
		Training error = 10/1000
		Pessimistic error = (10 + 30*.5)/1000 = 2.5%
		30*.5 is the penalty for a complex tree
-if cutting a branch off improves the pessimistic error then it gets better

The Chi Square Test (chi^2): Complex Trees

					MPG=Good   17
					MPG=Bad    4
					Gini 	   .308

split 3 ways
USA 
MPG=Good 	10
MPG=Bad 	0
Gini=0.0

Asia
MPG=Good 	5
MPG=Bad 	2
Gini=.408

Europe
MPG=Good 	2
MPG=Bad 	2
Gini=.5

Gini split = .301

Would we have seen this level of association "by chance" or "because the attribute is correlated with the class"?

	Test null hypothesis based on expected counts:
Null: split is by chance with XX% confidence (usually 95%)

The Chi Square Test 

							 (E_p(x=j)([x_k]-x_k,j))^2)
T=Sum_(j in C)Sum_(k in S)= ___________________________
						     E_p(x=j)([x_k])

-normalized sum of (count difference)^2
-S: all splits
-C: all classes
-Denominator =>Expected count for jth class in the kth split, based on the parent node
-Numerator X_k,j is actual count for the j_th class in the k_th split

parent node 		USA    Asia   Europe
	MPG=Good 		10 	   5 	  2 	 = 17
	MPG=Bad 		0 	   2      2 	 = 4
					=10    =7     =4 	 =21

Example:

	E_(p(x=good))([x_USA]) = x_USA * p(x=good) = 8.09
-expected value of the class being good for the USA attributed
			= 10 * 17/21 = 8.09
-actual value of USA being good is 10
-subtract these 2 numbers from each other, square them, and divide it by actual

if T > chi^2
-reject null hypothesis split is not by chance
if t < chi^2 
-accept null hypothesis split might be by chance

Example

		Exp(USA,Good) = 8.09
		Exp(Asia,Good) = 7 x17/21 = 5.66
-do this for all attributes and all classes
			USA 		ASIA  		EUROPE
MPG=Good 	10(obs) 	5(obs) 		2(obs)
			8.09(exp)   5.66(exp)   3.23(exp)

MPG=Bad 	0(obs) 		2(obs) 		2(obs)
			1.91(exp)   1.33(exp)   .76(exp)

Chi square test = 5.25
-can look up chi-squared based on how many degrees of freedom are allowed for chi-squared of .050
df = (rows-1)*(cols-1)
   = 1*2 = 2

   df    x^2 .050
   1 	 3.841
   2 	 5.991
   3     7.815

5.25 < 5.991

Not 95% confident split is not by chance


Self Test: Example of post-pruning

A
Class=yes 	20
Class=No 	10
Error = 10/30
Pesimisitc Error: (10+.5)/30 = 10.5/30

				
Mulit split
A1
Class=yes 	8
Class=No 	4
Error = 10/30

A2
Class=yes 	3
Class=No 	4
Error = 10/30

A3
Class=yes 	4
Class=No 	1
Error = 10/30

A4
Class=yes 	5
Class=No 	1
Error = 10/30

Pessimistic Error after splitting:
		=(9+4*.5)/30 = 11/30

if a split causes the pessimistic error to go up then prune it

Chi-square test = T = 2.793 < 7.815 while we want it to be > than so we prune it

Decision Tree Summary
-Advantages:
	-Inexpensive to construct
	-Extremely fast at classifying unknown records
	-variable importance through Gini reduction
	-Easy to interpret for small-sized trees
	-Accuracy is comparable to other classification techniques for many data sets

-------------------------------------------------------

Data Mining Lecture 13 Notes

Model Evaluation Measures

Metrics for Performanace Evaluation
-focus on the predictive capability of a model
 -not how fast it takes to classify or build models, scalibility, etc.
-confusion matrix
					Predicted Class
						Class=yes 		class=no
			 class=yes 		a 				b
Actual Class 
			 class=no 		c 				d

a: true positive
b: false negative
c: false positive
d: true negative

Metrics for performance evaluation
-most widely used metric:
					
				a+d 			TP + TN
	accuracy= __________ = ___________________
			   a+b+c+d 		TP + TN + FP + FN


confusion matrix is good because you might care about one of the squares more than the other ones.

Limitation of the accuracy
-ignores the cost of misclassifications
-consider an imbalanced 2-class problem
	-number of Class 0 examples = 9990
	-number of class 1 examples = 10
-if model predicts everything to be class 0, accuracy is 9990/10000 = 99.9%
	-accuracy is misleading because model does not detect any class 1 example

Superimpose cost matrix on top of confusion


Cost MAtrix

					Predicted Class
			 C(i|j)		Class=yes 		class=no
			 class=yes 	 C(Yes|Yes) 	C(NO|Yes)
Actual Class 
			 class=no 	 C(Yes|No) 		C(NO|No)

-define a cost function based on your expertise with problem:
	C(i|j): cost of misclassifying class j example as class i

Cost Matrix Example
					Predicted Class
			 C(i|j)		 	+ 		-
			  +		 	  	-1		100
Actual Class 
			  - 	 		1 		0

i.e. medical diagnosis costs?

What are the accuracy and cost of these two confusion matrices?
which classifier is "better" as derived by the accuracy and by cost?


Model M_1			Predicted Class
			 C(i|j)		 	+ 		-
			  +		 	  	150		40
Actual Class 
			  - 	 		60 		250

Accuracy = 80%
Cost = 3910

Model M_2			Predicted Class
			 C(i|j)		 	+ 		-
			  +		 	  	250		45
Actual Class 
			  - 	 		5		200

Accuracy = 90%
Cost = 4255

So if you go by accuracy you go by M_2, if you were going by the costs you would say M_1 in this case

-how do we know that the cost matrix is good?
	-verify with a Doctor if this is a good cost matrix
	-cost matrix could be economics, cost of someone dieing, etc.  

Cost-Sensitive Measures
				  a
Precision (p) = _____
				 a+c

-higher precision == lower false positives

			   a
Recall (r) = _____
			  a+b

-higher Recall == lower false negatives

				 2rp    	2a
F - measure(F) = ____ = ______________
 				 r+p 	 2a + b + c
higher F1 == lower FN and FP

precision is biased towards 
	C(p=yes|a=yes)& C(p=yes|a=no)
Recall is biased towards
	C(p=yes|a=yes) & C(p=no|a=yes)
F-measure is biased towards all except C(p=No|a=No)

							w_1*a + w_4*d
Weighted Accuracy = ______________________________
					 w_1*a + w_2*b + w_3*c + w_4*d



Medical diagnosis->Recall probably better because you'd want fewer false negatives

Retrieving similar documents from an online database
->b.c. if we say its similar, it better be similar


Model Evaluation
-how reliable are the estimates for performance?

Methods for Performance Evaluation
-how to obtain a reliable estimate of performance?
-performance of a model may depend on other factors besides the learning algorithm
	-class distribution
	-cost of misclassification
	-size of training and test sets

Learning Curve: Number of samples
-learning curve shows how accuracy changes with varying sample size
-effect of small sample size:
	-bias in the estimate
	-variance of estimate
-you cannot estimate this curve without collecting the data. Some bounds exist, but they are too loose to be useful
-to make one you train a bunch of different models with each sample size
	-average accuracy given sample size is the point in the curve

Bias Variance Tradeoff
-complex models can really fit the training data, giving lower bias 
-simpler models have trouble fitting data, resulting in higher bias
-but complex models can have high variance in their decision

How do we ensure our model is not overfitting to the data?

Methods of estimating generalization
-solution: use tesing set, and never, never, never, let the model see it
	-holdout
		-reserve 2/3 for training and 1/3 for testing
	-random subsampling
		-repeated holdout, with replacement
	-cross validation
		-partition the data in k disjoint subset
		-k-fold: train on k-1 partitions, test on the remaining one
		-Leave-one-out: k=n
	-stratified cross validation
		-select samples, keeping overall class distribution same for each fold
		-make sure that classes keep same distribution that they had from the whole sample in each inidividual fold
-if you use 10 fold cross validation then you are training your model on 90% of the data
	-if you don't expect to have that much training data in the future this would be bad

-still don't look at test data with cross validation
-take out test data to be used later
	-then do cross validation on the remaining training data
	-this is because you wouldn't know if the parameters generalize if you use too much of your data for training

Evaluating Binary Classification : ROC

ROC (Receiver Operating Characteristic)
-characterize trade-off b.w. positive hits and false alarms
-ROC curve plots TP (on the y-axis) against FP (on the x-axis)
-Performance of each classifier represented as a point on the ROC curve
	-changing the threshold of algorithm, sample distribution or cost matrix changes the location of the point

graph false positive rate vs true positive rate as we adjust some threshold
-if binary dat then chance would be x=y line

(TP,FP)
	(0,0): declare everything to be negative class-> means theres never a positive either way
	(1,1): declare everything to be positive class->
	on a graph of positive vs. positive then its always true
	(1,0): ideal-> always got the true positives correctly and never had a false positive
-Diagonal line:
	-random guessing for equal number of classes
	-below diagonal line:
		-prediction is opposite of the true class

Using ROC for model comparison
-no model consistently outperforms the other
	-M1 is better for small FPR
	-M2 is better for large FPR
-Area under the ROC curve
	ideal: Area = 1.0

How to Construct an ROC curve
	classifier score
instance# 		P(+|A) 		True Class
	6 			  .95 			+
	2 			  .93 			+
	5 			  .87 			-
	4 			  .85 			-
	9 			  .85 			-
	1 			  .85 			+
	10 			  .76			-
	8 			  .53 			+
	3 			  .43 			-
	7 			  .25 			+

-Use classifier that produces probability score for each test instance P(+|A)
-sort the instances according to P(+|A) in decreasing order
-Apply threshold, T, at each unique value of P(+|A)
-P(+|A) < T, is a negative class, else it is a positive class
-count the number of TP,FP, TN, FN at each threshold
- TP rate, TPR = TP/Positives
- FP rate, FPR = FP/negatives

Test of Significance
-Given 2 models
	-model M1: accuracy = 85% tested on 30 instances
	-model M2: accuracy = 75%, tested on 5000 instances
-can we say M1 is better than M2?
	-how much confidence can we place on accuracy of M1 and M2?
	-can the difference in performance measure be explained as a result of random fluctuations in the test set?

Comparing performance of 2 models
-given 2 models, M1 and M2, which is better?
	M1 is tested on D1 (size = n1), found error rate=e1
	M2 is tested on D2 (size = n2), found error rate=e2
	-assume D1 and D2 are independent
	-if n1 and n2 are sufficiently large, then
		e1 ~ N(mu_1, sig_1)
		e2 ~ N(mu_2, sig_2)
	approximate: 
									 e_i(1-e_i)
			variance=(sighat_i)^2 = ______________
										n_i
-probably comes form binomial distrib.
 which is approximate well by normal distr.

-To test if performance difference is statistically significant: d_t = e1-e2 <-estimate of mean diff.
				-d_t is the true mean
	- d ~ N(d_t,sig_t) where d_t is the true diff.
	-since D1 and D2 are independent, their variance adds up:
								 ~
		sig1^2 = sig1^2 + sig2^2 = sig1_hat^2 + 	
		^									sig2_hat^2
		|
		|	     e1(1-e1) 	e2(1-e2)
		|	   = ________ + _________
		|	   	  	n1 			n2
estimate of the variance in subtracted error rates
-sig1_hats is the variance as predicted by binomial distribution

	-at (1-alpha) confidence level, 
				d_t = d +- Z_(alpha/2)*sigt_hat


Z is a confidence interval
-use this to adjust confidence in mean
-does this interval include 0?
	-means error rates between 2 might not be that different from each
	-if it doesn't include 0 we can say statistically with some confidence level, one is better than the other

An illustrative example

-Given: M1: n1 = 30, e1=.15
		   M2: n2 = 5000, e2=.25

d = |e2 - e1| = .1 (2-sided test)

		   			.15(1-.15) 		.25(1-.25)
	(sighat_d)^2 =  ___________  +  ___________ =.0043
						30 				5000

-at 95% condience level, Z_(alpha/2) = 1.96

	d_t = .1 +- 1.96*sqrt(.0043) = .1 +- .128

=> interval contains 0 => difference may not be statistically significant

-"There are lies, damned lies and statistics."
	-Mark Twain

M1: n1 = 30, e1 = .15
M2: n2 = 5000, e2 = .25

Comparing Performance of 2 algorithms
-each learning algorithm may produce k models:
	L1 may produce M11, M12, ...., M1k
	L2 may produce M21, M22, ...., M2k
-if models are generated on the same test sets D1,D2,
....,Dk (e.g., via cross-validation)
	for  each set: compute d_j = e1j - e2j the jth difference

	dj has mean d and variance sig_t
				1
	sigt^2 = _______ SUM_j..k (dbar - dj)^2
			   k-1
	    ^
		|
		-now we can bound to get a better idea about how the accurance varies
					1
	d_t = dbar +- ________ *t_( 1-alpha, k-1) * sigt
				   sqrt(k)

	t(95%,k=10) = 2.26

Comparing Performance of 2 algorithms
-what about the fact that each model is computed on the same dataset?
-even if accuracies are similar, what if the errors are on different instances in the data?
-always a good idea to check the confusions b.w. the classifiers:

	Count 				Model M2
						Incorrect 		Correct
			
			incorrect 		a 				b

Model 		
M1 			correct 		c 				d

-------------------------------
	Prob. 				C2
						Incorrect(0) 	Correct(1)
			
		incorrect(0) 		.04 			.16

 		
C1 		correct (1)			.16 			.64


	
--------------------------------
	Prob. 				 C2
						Incorrect(0) 	Correct(1)
			
		incorrect(0) 		.18 			.02

 		
C1 		correct(1)	 		.02 			.78

its not ok to compare their accuracies 

Before Moving on...
-Is there any reason to prefer one learning method over another, if we are only interested in how the algorithm generalizes?

-should any one algorithm be superior to another in all situations?
	
	No, it depends on the data you have
	-no, there is no free lunch.. (there is always a trade off)

-------------------------------------------------------

Data Mining Lecture 14 Notes --- snow day

-------------------------------------------------------

Data Mining Lecture 15 Notes

For the ROC curve
-a good classifier has a curve that is almost horizontal
-that means the true positive rate stays high while the false positive rate can be decreased

ensemble methods

-Construct a set of classifiers from the training data
-predict class label of previously unseen records by aggregating predictions made by multiple classifiers

General Idea
-Original training data D
1. create multiple data sets D1...Dt from D
2. Build multiple classifiers on the data sets
C1 .. Ct
3. Combine classifiers into C*

Why does it work?
-why is this better than SVM 
-suppose we have 25 base classifiers
	-each classifier has error rate, e = .35
	-assume classifiers are independent so they make errors on different samples from dataset
	-probability that the ensemble classifier makes a wrong prediction
Sum(i = 13 to 25) of 25Ci * e^i * (1-e)^(25 - i) = .06
-thats the error rate of the ensemble

-these classifiers arent independent
-in practice our classifiers are correlated, so it does not work this well

Why does it work?
-how much does this horse weigh?
	-average of the guesses from many people is close to the true value
	-average of many people is better than AN expert's guess

-best classification methods use ensembles

Examples of Ensemble methods
-how to generate an ensemble of classifiers
	-bagging
	-boosting
	-stacking
	-cascading

Bagging
-Bag = bootstrap aggregation
Ex:
	-10 instances of data
	-will build 3 classifiers
-Build classifiers from random subsamples of data
-take majority vote of the resulting classifiers
	-thats it... more models is better
	-like 1000s
	-maybe we could be smarter than just random subsamples from the dataset
-can run mapreduce in parallel

Self test
-what is the probability of selecting a particular instance exactly twice when you have a sample size of 2000, 40 rounds of bagging, and selecting 100 instances for each bag?
	-you will be selecting 40 rounds x 100 instances =
	4000 times
	-Bernoulli trial with p = 1/2000, q = 1999/2000, n=4000, k=2

	P(2) = 67.7% = n_C_k * p^k * q^(n-k)

	Boosting and Bagging
	-weak learner: a learner with better than chance accuracy (error < 50% for 2 class problem)
		i.e. classifier has high bias
		like a pruned decision tree
	-strong learner: arbitrarily small error rate


Boosting
-an iterative procedure to adaptively change distribution of training data by focusing more on previously misclassified records
	-initially, all N records are assigned equal weights
	-unlike bagging, weights may change at the end of boosting round
	-samples with a higher weight are more likely to be chosen
	-if an instance starts showing up more that means it is harder to classify
		-its weight gets higher as it goes on

Example: AdaBoost
-base classifiers: C1,...,CT
-weighted error rate of Ci is:
e_i = 1/N * Sum(j=1 to N) of W_j*delta(Ci(x_j)!=y_j)

delta = 1 when error, 0 when not error
1/N is for average 
W_j is jth instance weight
Take jth instance (x_j) and put through ith classifier Ci(x_j) and if those are not equal (y_j) then add up the weight of that instance
	-weighted error rate

-use this weighted error rate to come up with an importance

alpha_i = 1/2*ln( (1-e_i)/e_i )
-alpha gets bigger as e gets smaller


Weight update:
- if you were correct don't adjust weight, but if you were wrong
			| 1 				if Ci(x_j) = y_i
wj <- wj *  |
 			|(1-e_i)/e_i 		if Ci(x_j)!= y_i

-Rescale weights to sum up to 1
-Classification
	C*(x) = argmax sum(j=1..T) of 
			   y				alpha_j*delta(Cj(x)=y)
-so you boost the weights where you were wrong
-so by not adjusting the weights you were correct at this dilutes their weight
	-this is much faster than bagging

Illustrating
-Original Data (sorted on x):
	 +  +  +  -  -  -  -  -  +  +
	.1 .1 .1 .1 .1 .1 .1 .1 .1 .1

-try to find a threshold where on 1 side it is positive, and on the other its a negative class
-we have 10 data points, so each data point gets initial weight 1/10
-suppose we sample six points
-which makes 2 errors with weight .1

	-threshold with lowest error rate is after 3rd instance
	-2 errors on the 9th and 10th instance with weight of .1
	-go back to equations
	- so e = 2*.1 = .2, alpha = ln( (1-.2)/.2 ) = ln4
												~ 1.38
	-so weights of incorrect answers get multiplied by 4
	-then weights are rescaled to 1

	+ + + - - - - - + +
	|--.0625-------| .25
	-now those 8 have a weight of .0625 and the 2 that had been wrong have a weight of .25

	take 6 new samples

		+ - - - + +
		so the 2 with the high weights got chosen again and there were 4 random others

	-repeat process
	-choose threshold with smallest error

	-after 4th instance
		+ - - - + +
		   -   | +

	-sometimes error stump might be on an edge
-so after performing a few classifiers
-if the importances are all right around each other then you take the majority vote for an instance

Stacking and Cascading 
-train an intial classifier (or ensemble)
-use the output probabilities of each class (from each classifier, if ensemble), as features for another classifier
-train final classifier on inferences from previous classes
	-typical this last classifer is logistic regression

Common types of ensemble classifiers
-random forests
	-select random subset of samples
	-select random subset of the features
	-build a tree
	-build many trees
	-actually a whole forest of trees

Random Forest
-decision trees are built
-but at each stage a random subset of the features is selected (random subspace)
	-if "f" features, look at "np.sqrt(f)" features at each iteration
	-approximately probably correct
-generalization built in :out-of-bag
-variable importance:
	-random feature permutation
	-look at out-of-bag samples
	-randomly permute the values of nth feature
	-see how performance degrades

-outback error rate on random forests

features of random forests
-produce high accuracy on many real world datasets
-run efficiently on large databases (each tree is an easy prediction, easily extensible to map reduce)
-can handle 1000s of input variables w.o variable deletion
-give estimates of what variables are important in the classification
-generate an internal unbiased estimate of the generalization error as the forest building progresses
-have effective method for estimating missing data
-have methods for balancing error in class population for unbalanced sets

self test
-how many features (Attributes) does scikit-learn use in each iteration of bagging for a random forest?
	max_features = sqrt(n_features)
-which best completes this statement : in scikit, the number of features considered changes:
	a: once for each tree in the forest
	b: once for each split in each tree
		-B

Assignment 3: Classification Finale
-modeling and eval
	-whatever you choose for part 1 just back it up
	-part 2 use all of your data
	-use at least 3 models for 3
	-discuss the advantages of each model and use the statistics to discuss this

-------------------------------------------------------

Flipped Module 3

Classification using k nearest neighbors

Nearest Neighbor Classifiers
-Basic idea:
	-if it walks like a duck, quacks like a duck, then its probably a duck
-if it has about the same values, and matches something in our dataset then that's probably what it is

example:
	-we have an unknown record (Red dot)
	-its three nearest neighbors are + so 
-requires 3 things
	-the set of stored records
	-distance metric to compute distance b.w. records
	-the value of k, the number of nearest neighbors to retrieve
-to classify an unknown record:
	-compute distance to other training records
	-identify k nearest neighbors 
	-use class labels of nearest neighbors to determine the class label of unknown record
	(e.g., by taking majority vote)

definition of nearest neighbor
-1-nearest neighbor
	-doesn't account for noise
-2-nearest neigbor
	-could have a tie and no majority class
-3-nearest neighbor
	-accounts for noise and has majority

-compute the euclidean distance b.w. 2 points
	-d(p,q) = sqrt(sum(i) of ((p_i-q_i)^2))
-determine the class from nearest neighbor list
	-take the majority vote of class labels among the k-nearest neighbors
	-weigh the vote according to distance
		-weight factor, w= 1/(d^2)
-decision boundaries can be intricate and complicated
-increasing k makes the boundary more and more linear

-self test
	-how would KNN classify the test point for 
		k=1 - green
		k=3 - green
		k=5 - pink
-choosing the value for k:
	-if k is too small then sensitive to noise points
	-k too large then neighborhood may include points from other classes
		-use grid search to determine good value for k

problem with euclidean measure:
	-high dimensional data
		-curse of dimensionality
	-can produce counter-intuitive results

Ex: binary data

	1 1 1 1 1 1 1 1 1 1 1 0 vs 1 0 0 0 0 0 0 0 0 0 0
	0 1 1 1 1 1 1 1 1 1 1 1    0 0 0 0 0 0 0 0 0 0 1
	the distance between the 2 sets is identical which we can tell they are not even close to the same distances apart

	-semi-solution: normalize the vectors to unit length
	-semi-solution: scale features

scaling issues
	-attributes may have to be scaled to prevent distance measures from being dominated by one of the attributes
		-height of a person may vary from 1.5m to 1.8m
			-so value < 1
		-weight of a person may vary from 90lb to 300lb
			-value in 10s
		-income of a person may vary from $10k to $1M
			-value in 1000s

	-so the income will have the highest importance and our data will be most sensitive to income
	-you must scale the data dimensions
	-otherwise large attributes appear more important
		-must give 0-mean and st. dev.

self test
	-would decision trees or KNN be better for the following dataset?
	-answer for this dataset is decision trees
	-all the noise attributes will skew the distance score for KNN

self test 2
	-would decision trees or KNN be better for the following dataset?
	-answer: KNN!
	-the arbitrary decision lines will be better for the KNN classifier

	-the decision tree will have lots of boundaries to make and likely get stuck in a local optimum

KNN distances and similarities

similarity/dissimilarity for simple attributes
-p and q are the attribute values for a data object
Attribute Type 		Dissimilarity 		Similarity
					| 0 if p=q            | 1 if p=q 
Nominal 	     d= |                  s= |
					| 1 if p!= q          | 0 if p!= q


Ordinal 		d = (| p-q |)/ (n-1)
				(values mapped to      s=1-(| p-q |)/ 
				integers 0 to n-1,  			  (n-1)
				where n is the # of 
				values)

Interval or       d = |p-q|            s=-d, s= 1/(1+d)
Ratio 								   or 
									s=1- d-min(d)/
									  (max(d) - min(d))

Euclidean Distance
	dist = sqrt(sum(k=1..n) of ((p_k-q_k)^2))

	where n is the # of dimensions (attributes) and p_k
	and q_k are, respectively the kth attributes (components) or data objects p and q
-standardization is necessary if scales differ

Minkowski Distance
-a generalization of euclidean distance

	dist = (sum(k=1..n) of ((|p_k-q_k|)^r))^(1/r)
-so if r = 2 it is euclidean 
- r is a parameter n is the # of dimensions (attributes) and p_k and q_k are respectively the kth attributs (components) or data objecst p and q

r = 1 City block (manhattan, taxicab, L1 norm) distance
	-a common example of this is the Hamming distance, which is just the # of bits that are different b.w. two binary vectors
r = 2 Euclidean distance
r -> infinity. "supremum" (L_max norm, L_inf norm) 
				distance.
	-this is the maximum difference b.w. any component of the vectors
-do not confuse r with n, i.e. all these distances are defined for the number of dimensions

-when you really care about one attribute being really far away, you might use infinity norm
-l1 and l2 are somewhat interchangeable

Mahalanobis Distance

mahalanobis(p,q) = (p-q)* sig^-1 * (p-q)'

sig is the covariance matrix of the input data X

-scale data in dimension that corresponds to the variance

-common properties of a similarity
	-similarities also have some well known properties 1. s(p,q)=1 (or maximum similarity) only if p=q
	2. s(p,q)=s(q,p) for all p and q (symmetry)

	where s(p,q) is the similarity b.w. points (data objects), p and q

Similarity b.w. binary vectors
-common situation is that objects, p and q, have only binary attributes
-compute similarities using the following quantities
	M01 = the # of attributes where p was 0 and q was 1
	M10 = the # of attributes where p was 1 and q was 0
	M00 = the # of attributes where p was 0 and q was 0
	M11 = the # of attributes where p was 1 and q was 1
-simple matching and jaccard coefficients
	-SMC = # of matches / # of attributs

	J = # of 11 matches / # of not-both zero attributes values

SMC versus Jaccard: example

p = 1000000000
q = 0000001001

M01 = 2
M10 = 1
M00 = 7
M11 = 0
	SMC = (M11+M00)/(M01+M10+M11+M00) = (0+7)/(2+1+0+7)
									  = .7
	J = M11/(M01 + M10 + M11) = 0/(2+1+0) = 0
-can still use jaccard for continuous data

Cosine Similarity
-want to take 2 vectors and look at angle b.w. vectors
-used a lot in document classification
	-like summarize a document based on words in document
-saying each sentence has a document frequence we can associate with it
	-can say how similar are these 2 sentences to each other
	-can look at the angles of each sentence
		-if they use the same words they will have angles close to each other
if d1 and d2 ar 2 document vectors, then 
	cos(d1, d2) = (d1 . d2)/||d1|| ||d2||
	where . indicates vector dot proecut and ||d|| is the length of vector d

example:
	d1 = 3 2 0 5 0 0 0 2 0 0
	d2 = 1 0 0 0 0 0 0 1 0 2

	d1 . d2 = 3*1 + 2*0 +... 0*2 = 5
	||d1|| = (3*3 + 2*2 + 0*0 ... + 0*0)^.5 = 42^.5=6.4
	||d2|| = (1*1 + 0*0+...+2*2)^.5 = 6^.5 = 2.245

	cos(d1,d2) = .315

Correlation
-correlation measures the linear relationship b.w. objects
-to compute correlation, we standardize data objects, p and q, and then take their dot product
	p'_k = (p_k - mean(p))/std(p)
	q'_k = (q_k - mean(q))/std(q)
	correlation(p,q) = p' . q'

general approach for combining similarities
	-sometimes attributes are of many different types, but an overall similarity is needed
1. for the kth attribute, compute a similarity s_k in the range [0,1]
2. define an indicator variable, delta_k, for the kth attribute as follows
		  | 0 if the kth attribute is a binary 
		  |	  asymmetric attribute and both objects 
delta_k = |   have a value of 0, or if 1 of the objects
		  |   has a missing values for the kth attribu.
		  |
		  | 1 otherwise

3. Compute the overall similarity b.w. the 2 objects using the following formula

		similarity(p,q) = sum(k=1..n) of delta_k*s_k /
						  sum(k=1..n) of delta_k

Using weights to combine similarities
-may not wnat to treat all attributes the same.
	-use weights w_k which are b.w. 0 and 1 and sum to 
	 1. 

	similarity(p,q) = sum(k=1..n) of w_k*delta_k*s_k /
					  sum(k=1..n) of delta_k

 	distance(p,q) = (sum(k=1..n) of 
 								w_k*|p_k-q_k|^r)^(1/r)


Regression - k nearest neighbor

KNN regression
-knn can also be used for regression problems where we want to predict a numeric value
-use the avg. y value of the k nearest neighbors as the predicted value
-this estimates E(y|x), the average y value at x, which minimizes mean squared error

KNN Regression: example
-in a clinical study of risk-factors for cardiovascular disease
	-independent variable x is a patient's waist circumference
	-the dependent variable y is a patient's deep abdominal adipose tissue (more complicated to measure)
-researchs want to predic the y variable using the x

graphed waist circumference vs the deep abdominal adipose tissue

KNN regression: alternatives to mean
-we don't need to just take the mean of the points

-find the neighbors in feature space
-use regression of feature subset using only nearest neighbors

-encapsulated knn or knn with a linear fit
-1 of most effective regression methods you can use

self-test
-find the 5-NN points from this data for the given test point?
-draw the linear regression best fit line that estimates the value of the test point from attribute A1

k-NN classifiers are "lazy" learners
	-in pure form, model has no parameters
	-classifying records can get computationally expensive
	-this is unlike decision tree induction and rule-based systems
	-so prediction becomes more costly

bigish data: KD Trees for kNN
-a way to make the space of kNN more tractable
	-brute force
	-kd tree:
		-partition search space using tree
		-examine nearby points

kd tree construction
-start with d-dimensional space
-start with box around data points
-choose to split on x or y
	-lets say x (vertical line) (left branch)
-choose next to split
	-lets say split left (horizontal line on the left side)
-choose next to split
	-lets say left = bottom half (vertical line)

-save the bounding rectangle for each node

kd Tree construction
-which dimesion to split?
	-split on dimension which has higher dynamic range (normalized)
-which value to split?
	-median or mean of everything in that box 
		-typically median
-stopping criteria
	-points in node < threshold
	-minimum dynamic range in box (volume/area)

self test for k = 1
-save the distance to the nearest point
	-sometimes that isn't in the same box you are in
	-need to recursively back track to compare current nearest neigbor distance to the distance b.w. our point and the nodes bounding box
		-then we iterate through all points for that node and update nearest neighbor accordingly

what if k > 1
-same algorithm, save minimum distance to most distant neighbor point in container box
-need to calculate more distances to container boxes, but still better than brute force
-can get this even lower though

Approximate KDTree for KNN (when k >1)
-do not visit nodes when
	-exact algorithm: r < dist. to bounding box
	-new algorithm :r < alpha (dist. to bounding box)
	-suboptimal, but still works well
	-no one point close than r/alpha

	alpha < 1

KDTree for KNN complexity
-query for NN, balanced tree
	O(log N) usual case
	O(N) worst case
-query for kNN
	O(k log N) (approximate almost always this)
	O(N log k) worst case

KNN summary
-get dimensions down
	-choose only relevant features
-normalize features
-use KDTree to optimize query
-high dimensions (if you cannot reduce)
	-ball tree is better, but suffers from curse of dimensionality
	-use locality sensitive hashing functions
	-or choose a different classifier!

Classification: Naive Bayes
-a probability framework for solving classifications
-conditional probability
	P(C | A) = P(A,C)/ P(A)
	P(A | C) = P(A,C)/ P(C)
	A is attributes
	C is Class
	P(A,C) is a joint probability
-bayes thm:
		P(C | A) = P(A,C)*P(C)/ P(A)
	P(A,C) = conditional
	P(C) = prior
	P(C|A) = posterior
	P(A) = marginal

Example:
	P(blue) = 40/60 = 2/3
	blue blocks take up 40 of the 60 peg area (we're still counting the pegs covered by the yellow block)

	P(red) = 20/60 = 1/3

	P(blue) + P(red) = 1

What about the yellow brick
	p(yellow) = 6/60 = 1/10

	P(Y | R) ?
	-split red section off from blue
	-get the area of the remaning red space (2x10)
	-get teh area of the yellow block on th red space(4)
	-divide the area of the yellow block by the are of the red block
	P( y | r ) - 4/20 = 1/20

	P( r | y ) = 4/6 -> 4 of the 6 yellow blocks are on red

	-using formula -> p(yellow)*totalpegs = 1/10*60=6
	#of red pegs = P(red)*totalpegs = 1/3*60=20
	numberofred under yellow = p(y|r)*#ofR = 1/5*20=4

	P(r|y) = # of red under yellow/number of yellow
		   = 4/6 = 2/3

		   = P(y|r)* (#of red Pegs)/ (P(y)*totalpegs)

		   = p(y|r)*p(r)*totalPegs/(p(y)*totalPegs)

		   = p(y|r)*p(r)/p(y)

self-test bayes thm

-given 
	-doctor known meningitis causes a stiff neck in 50% of people
	-prior p(meningitis) = 1/50000
	-prior p(stiff neck) = 1/20

	P(M | stiff neck) ?

	= P(stiff neck | M) * P(M) / P(stiff neck)
	= 1/2*1/50000/(1/20) = 1/5000

Bayesian Classifiers
-consider each attribute and class label as random variables
-given a record with attributes (A1,A2,...,An)
	-goal is to predict class C
	-specifically, we want to find the value of C that maximizes P(C | A1, A2, ... An)
-can we estimate P(C | A1,...An) directly from data?
	-classification decision, whichever is greater:
		P(evade=yes|No refund, married,75k)
		P(evade=no|no refund,married,75k)
		-take the one that is greater

Bayesian Classification
-approach
	-compute the posterior porbability P(C|A1,...An) for all values of C using Bayes thm

	P(C|A1A2....An) = P(A1A2...An|C)P(C)/(P(A1A2..An))

	-choose values of C that maximizes P(C|A1,A2,..An)
	-equivalent to choosing value of C that maximizes
		P(A1,A2,...,An|C)P(C)

-how to estimate P(A1,A2,...,An|C)?

Naive Bayes Classifier
-assume independence among attributes Ai when class is given:
	P(A1,A2,...,An|C) = P(A1|Cj)*P(A2|Cj)*..*P(An|Cj)
	-use training data to estimate P(Aj|Cj) for all Aj and Cj.
	-new point is classified to Cj if P(Cj) prod(P(Aj|Cj)) is maximal
		
		argmax P(Cj) prod (P(A_j | C_j))
		   j

P(C) = N_c/N
P(Aj | Ck) = |Ak|/Nc
where |Ak| is # of instances having attribute A and belongs to class Ck
-this is for ordinal or discrete

Estimate Probabilities for Continuous Data

-for continuous attributes:
	-discretize the range into bins
		-one ordinal attribute per bin
		-violates independence assumption
	-two-way split: (A<v) or (A > v)
		-choose only one of the 2 splits as new attribute
	-probability density estimation:
		-assume attribute follows a distribution (usually Gaussian)
		-use data to estimate parameters of distribution (e.g. mean and standard deviation)
		-once probability distribution is known, can use it to estimate the conditional probability P(Aj|c)
		-could also use kernel density estimation to figure out the probability

how to estimate probabilities from data?
	-normal distribution:
		P(A_i|c_j)=1/(sqrt(2*pi*sigma^2))*
							e^(-1*(A_i-u_ij)^2/
							      (2*sigma^2)
							  )
	example = Ai = Income = x
			Cj = Evading = y
			u_ij = sample mean
			sigma = st. deviation
	P(income = 120 | No) ?
	sample mean = 110
	sample variance = 2975 = sigma^2

  P(income=120|No) = 1/(sqrt(2*pi*2975))*
  						e^(-(120-110)^2/(2*2975))
  				   = .0072

naive bayes classifier, smoothing
-problem: if 1 conditional probability is 0 then the entire expression becomes 0
-probability estimation
	-original, laplace, m-estimate, lidstone
	-all want to remove that 0 appearance

Naive Bayes (summary)
-robust to isolated noise points
-handle missing values by ignoring the instance during probability estimate calculations
-robust to irrelevant attributes
-independence assumption may not hold for some attributes
	-use othe techniques such as Bayesian Belief Networks (BBN)..
	-training and inference get complicated quickly (toolkits exists, i.e. gmtk)
-but that does not really matter! all we need to worry about is whether the probability estimate correctly identifies the class (P(X) > .5)
-similar argument for more than 2 classes, but "margin of error" is smaller
-an example...

-look at some examples in scikit learn

-------------------------------------------------------

Data Mining in class assignment

-most of values are 0

-------------------------------------------------------

Data Mining Lecture 17 Notes

classification notebook demo

-image data for data mining
	-break it into features (pixels)
	-stack rows one after the other 
	-classify the person who's face is there

lfw_people = fetch_lfw_people(min_faces_per_person=20, resize=0.4)
-fetches at least 20 of each person, and makes the faces a little smaller so data set isnt huge

lfw_people.data.shape
	-numpy matrix has .shape property
	-tells rows and columns (instances and features)

lfw_people.target_names
	-target_names are actual unique people

size (3023,1850)
-3023 instances (3023 images)
-1850 features (pixels in a row) (in an image)
	-each value is 0-255 (some level of gray)

plt.hist(lfw_people.target,bins=len(lfw_people.target_names))
	-count how many instances there are of each person and put it in a histogram (each person is a class)

-what metric would be a good measurement of performance for this data set?
	-accuracy - you want to know how accurate you are on the whole dataset
	-low false positive rate - for very frequent faces
		-high precision (on a per class basis)
			-calculate precision on all classes above this threshold of frequency
	-high true positive rate - very infrequent faces
		-no false negatives
		-high recall

-start with a classifier
	-doesn't matter which one

-k neighbors

x = lfw_people.data
y = lfw_people.target
yhat = np.zeros(y.shape)

cv = StratifiedKFold(y, n_folds=10)
-way to pick training data - 10 folds and in each fold it will have the same class distribution as whole data in each fold
-train model using 9 folds and train on 10th
-this will happen 10 times
-use entire data set for both training and testing
-real good measure of performance

clf = KNeighborsClassifier(n_neighbors=1)

for train, test in cv:
	clf.fit(x[train],y[train])
	yhat[test] = clf.predict(x[test])

total_accuracy = mt.accuracy_score(y,yhat)
-use mt for all metrics
-1/3 was accuracy - chance is 1/N classes = 1/62 faces
-try adjusting k neighbors

-use principle component analysis to reduce dimensionality
	-gives you eigen faces to reduce number of features
		-you take less pixels into account to determine who's face it is
		-there might be some noise 

-need to be careful to never using testing data when training - be careful with randomized pca not to do this

-have 10 folds - take 9 folds calculate and fit a pca to that data - then fit a classifier based on these features and fit it on 

clf_pipe = Pipeline(
	[('PCA_TJ',RandomizedPCA(n_componenets=300)),('CLF_TJ',KNeighborsClassifier(n_neighbors=1))])

-takes dimensions from 1800 to 300 weights
	-can tune this
-can also tune # of neighbors

-then use same code as before

for train, test in cv:
	clf_pipe.fit(x[train],y[train])
	yhat[test] = clf_pipe.predict(x[test])

-could also do feature union
	-puts a few things together before putting into a classifier

-this trained faster but not better
-try changing classifier or other parameters
	-because it didn't work so well

clf_pipe = Pipeline(
	[('PCA_TJ',RandomizedPCA(n_componenets=300)),('CLF_TJ',RandomForestClassifier(max_depth=50, n_estimators=150,n_jobs=-1))])

-can tune these again
-n_jobs = -1 means use all cores available

-repeat loop again from above

try just random forest classifier

clf = RandomForestClassifier(max_depth=50, n_estimators=150,n_jobs=-1,oob_score=True);

-out of bag score
	-calculate how well it is generalizing on the data it is not using currently to train the forest
-run loop again
-advantage of not using random pca is we can graph out feature importances

-can grab last time it was fit
	-feature importances calculate by randomly permuting feature values and sticking it back through the classifier to see whether or not it does well
	-if a feature is very important you would expect its score to drop off a lot when you randomly permute the values
plt.barh(range(len(clf.features_importances_)),clf.feature_importances_)
plt.show()

print clf.oob_score_
	-want out of bag score to be very close in accuracy to pipeline
	-this would mean your data is generalizing well
	-a random forest is an unbiased estimator of generalization error if these numbers match
		-underestimate might be okay
			-if a lot of trees then a lot of training iterations
		-overestimate is bad because 

-importance graph
	-could display feature importance as an image instead
	-would expect a weird circle around face 

-could do an adaboostclassifier
	-in python book

-could do nearest centroid to use cosine metric
	-remember cosine takes angle distance between vectors
	-will have a centroid per class

-can play with n_components

-do SVC, logistic regression, SGDClassifer

-do Gaussian Naive Bayes
	-keep doing clf_pipe code
	-using continuous weights b.c. of r pca
	-won't be sensitive to noise

-decision trees are pretty good with noise also

-KNN can be sensitive to noise b.c. distance measure

-now use metrics
	-we already have accuracy on whole data we've been calculating

-create threshold for how many faces is frequent enough

freq_infreq_threshold = 30

-turn into binary classification problem
for class in unique values of y, calculate indices where y = that class
	-set these = to 1 and the rest to 0
	-then get predicted values and wherever you predicted the correct value set predicted = 1

	-then get number of times that you predicted correctly for that class and if it is < threshold
		calculate recall
		-otherwise calculate precision
		-append the value onto array
	-do this for every class (Face)

Total acc = 53 %
number of infrequent faces = 28 with recall of .35
number of frequent faces = 34 with precision of .55
- so when our class was frequent enough we were 55% accurate
-whenever we don't have very many examples we have more trouble
-this makes sense because the more dense, the better gaussian dist will do
	-remember precision mostly takes into account low false positives
	-can adjust threshold

-compute ROC curve
	-receiver operating characteristic
	-want it to be steep up in the beginning and then flatten
	-can't calculate for multiclass dataset
		-no need binary data
		-could do ROC curve for every individual class

-remember with ROC curve you get the probability for every single class
	-for every instance you pull out a record and say what is the probability it came from each class
	-use these probabilities to score roc curve
		-y_hat_score

	-grab score at ith class - positive_label = i
	-will give false positive and true positive rates
		-for every single class
	then get area under roc curve

-get one hot encoding of every class and make it a giant vector
-take thresholds for every class and put that in 1 giant vector
-lets you calculate an ROC curve for all of classes combined
	-like an average ROC across all of your classes


-then randomly grab 6 classifiers and graph them
-also shows micro-average ROC curve area = .91 (not bad)

-get to naive base classifiers on different number of components
-see which is better
	-comparing performance of 2 different algorithms

-we can do cross validation and see accuracy for each individual fold
	-can get overall standard dev. and do +- that quantity and see if it includes 0
	-if it includes 0 then with 95% confidence we don't know they are different

clf1 = Pipeline(
	[('PCA',RandomizePCA(n_components=100)),('CLF',GaussianNB())]
)

clf2 = Pipeline(
	[('PCA',RandomizePCA(n_components=50)),('CLF',GaussianNB())]
)

cv = StratifiedKFold(y, n_folds=10)
acc1 = cross_val_score(clf1,x,y=y,cv=cv)
acc2 = cross_val_score(clf2,x,y=y,cv=cv)
-give test attributes x, labels (y) and cross validation object cv

-give me the accuracy of every single fold as an array
-trains 2 classifiers with Naive Gaussian Bayes and gives accuracy scores


t = 2.26/np.sqrt(10)
-multiply by st. dev.
then do +- dbar (which is just the average)

e = (1-acc1) - (1-acc2)
stdtot = np.std(e)
dbar = np.mean(e)

print 'Range of:', dbar-t*stdtot, dbar+t*stdtot

-doesn't include zero so we can say with 95% confidence they are different
	-there is still a possibility they are from the same distribution

2.26 came from 95% confidence interval with 10 folds

-------------------------------------------------------

Data Mining Lecture 18 Notes -- no class

-------------------------------------------------------

Data Mining Flipped Module 4 Notes

Basics of Clustering Analysis
	Algorithms
		-k-means clustering
		-agglomerative clustering
		-DBSCAN

What is clustering analysis
-clustering
	-finding groups of objects s.t. the objects in a group will be similar (or related) to one another and different from (or unrelated to) the objects in other groups
-want to minimize the intra cluster distances
-want to maximize inter cluster distance

Applications of Cluster Analysis

Prototypes 
	-group related documents for browsing
	-group genes and proteins that have similar functionality

-Summarization
	-reduces the size of large data sets
	-efficient PCA (Reduction)
	-efficient KNN (quantization)

	-Marketing on social media

Self Test
-is this clustering
	-dividing students up alphabetically
		-no this is just ordering
	-paritioning a graph by cutting links
		-most likely, depending on how you decide to cut the links


Types of clusters
-a clustering is a set of clusters

Partitional Clustering
-a division data objects into non-overlapping subsets (clusters) s.t. each data object is in exactly one subset
	-assuming clusters are disjoint from e.o

Hierarchical Clustering
-a set of nested clusters organized as a hierarchical tree

-defining clusters can be ambiguous
	-well-separated clusters
	-conter-based clusters
	-contiguous clusters
	-density based clusters
	-property of conceptual
	-described by an objective function

Types of Clusters: Well-separated
-center-based
	-a cluster is a set of objects s.t. an object in a cluster is closer (more similar) to the "center" of a cluster, than to the center of any other cluster
	-the center of a cluster is often a centroid, the average of all the points in the cluster, or a medoid, the most "representative" points of a cluster

-contiguous cluster (nearest neighbor or transitive)
	-a cluster is a set of points s.t. a point in a cluster is closer (or more similar) to one or more other points in the cluster than to any point not in the cluster

-density-based
	-like a contiguous cluster in noise
	-a cluster to a dense region of points, which is separated by low-density regions, from other regions of high density
	-used when the clusters are irregular or intertwined, and when noise and outliers are present

-Clusters defined by an objective function
	-finds clusters that minimize or maximize an objective function
	-enumerate all possible ways of dividing the points into clusters and evaluate the 'goodness' of each potential set of clusters by using the given objective function (NP Hard)
	-can have global or local objectives
		-hierarchical clustering algorithms typically have local objectives
		-partitional algorithms typically have global objectives
	-a veriation of the global objective function appraoch is to fit the data to a parameterized model
		-parameters for the mdoel are determined from the data
		-mixture models assume that the data is a "mixture" of a number of statistical distributions

-transformed objective
	-map the clustering problem to a different domain and solve a related problem in that domain
		-proximity matrix defines a weighted graph, where the nodes are the points being clustered, and the weighted edges represent the proximities b.w. points
		-clustering is equivalent to breaking the graph into connected components, one for each cluster
		-want to minimize the edge weight b.w. clusters and maximize the edge weight w.in clusters

self test
-how many cluster would contiguous clustering find?
	2
-how many clusters would center based clsutering find?
	1

Common clustering Algorithms
	-k-means and its variants
	-hierarchical clustering
	-density-based clustering

K-means clustering
-partitional clustering approach
	-no overlap b.w. clusters and no hierarchy
-each cluster is center based (there is a centroid)
-each point is assigned to the cluster with the closest centroid
	-euclidean
	-cosine if sparse
	-jaccard if binary inputs

-number of clusters, K, must be specified
-the basic algorithm is very simple

1. Select K points as the initial centroids
2. repeat
3. 		form K clusters by nudging all points to the closest centroid
4. 		recompute the centroid of each cluster
5.  until the centroids don't change

Self-test K-means clustering
-for the k cluster algorithm how many times does each point get accessed in memory
	-k times to comapre it to each centroid to find which it is closest to

K-means clustering - details
-initial centroids are often chosen randomly
	-clusters produced vary from one run to another
-the centroid is (typically) the mean of the points in the cluster
-'Closeness' is measured by Euclidean distance, cosine, similarity, correlation, etc.
-K-means will converge for common similarity measures mentioned above
-most of the convergence happens in the first few iterations
	-often the stopping condition is changed to 'until relatively few points change clusters'
-complexity is O(n*K*I*d)
	-n = nuber of points
	-K = number of clusters
	-I = number of iterations
	-d = number of attributes
-initialize is very key
	-choosing centroids must have some randomness
-want way of evaluating what clusters are better than others


2-different k-means clustering
-suboptimal vs optimal clustering
	-mathematical eq. to quantify this

Comparing K-means clusterings
-most common measure is Sum of Squared Error (SSE)
	-for each point, the error is the distance to the nearest cluster
	-the SSE of a model is given by

		SSE =  sum sum dist^2(m_i,x)

	-x is a data point in Cluster C_i and m_i is the representative point for cluster Ci
		-can show m_i corresponds to the center (mean) of the cluster
	-given 2 clusterings, we can choose the 1 with the smallest error 
	-1 easy way to reduce SSE is to increase K
		-Note: A good clustering w/smaller K can have a lower SSE than a poor clustering w/greater K

K-means optimizes the SSE
-SSE pf the entire clustering
	x: point
	m_i = centroid of Cluster C_i
	K = # of clusters

	if distance is euclidean distance

	SSE = sum sum( (m_i - x)^2)

want to know optimal values of m_k

d/d(m_k) of SSE = 0 = d/d(m_k) sum sum ((m_i - x)^2)


use chain rule for (m_i-x)^2

	2(m_i -x)* d/d(m_k) (m_i - x)
	-this goes away unless k = i 
	because d/d(m_k)(m_i-x) = 0 if k!= i
	-if k = i then it equals 1
	so  this becomes

	0 = 	sum   (m_k -x)
		x inc C_k


	Sum m_k = sum x 

	m_k = 1/abs(C_k) * sum x
	-new m_k = the mean of all the parts we have in the cluster

-k-means will find that optimal clustering even if you choose the wrong initial clustering	
	-hopefully
	-sometimes you get caught in the local optimum
		-local min of SSE

Solutions to initial centroids problem
-multiple runs
	-helps, but probability is not on your side
-Kmeans ++
	-select initial point and select initial centroids w/more probable selection proportional to distance
	-slightly more probable to find optimal clustering
-bisecting k-means
	-not as susceptible to initialization issues

Bisecting K-means
	-variant of k-means that can produce a partitional or a hierarchical clustering

1. Initialize the list of clusters to contain the cluster containing all points
2. repeat
3. 		select a cluster from the list of clusters
4. 		for i = 1 to number of iterations do
5. 			bisect the selected clusters using basic k-
			means
6. 		end for
7.  	Add the 2 clusters from the bisection with the 
		lowest SSE to the list of clusters
8. until Until the list of clusters contains K clusters

will bisecting k-means always produce the same clustering?
	-yes! there is no randomness to it
	-almost guaranteed to find optimal bisection

will bisecting k-means always globally find the minimum SSE solution
	-no, no guarantees, but for most data usually yes

Updating centers incrementally
-an alternative is to update the centroids after each assignment (i.e. stochastic update)
	-each assignment updates zero or two centroids
	-fast but not necessarily accurate
-an alternative: mini-batch k-means
	-randomly select "x" points at each iteration and update the clusters based on sampling

Mini-batch K-means
init centers, c randomly
init counts per center, v[c] = all zeros

for t iterations:
	choose M points from data
	for each x in M
		c = closest center to x
		update per center count:v[c]++
		set learning rate n = 1/v[c]
		update center towards x, c= (1-n)c + nx

-minibatch k-means runs the fastest with objective function values really good
	-good trade off

Limitations of K-means
-has problems when clusters are of differing
	-sizes
	-densities
	-non-globular shapes
-has problems when the data contains outliers

-one solution is to use many clusters
	-find parts of clusters but need to put together

-does bisecting k-means also suffer from globularity?
	-yes, it doesn't change the underlying k-means assumption of center-based clusters

Hierarchical Clustering
-produces a set of nested clusters organizd as a hierarchical tree
-can be visualized as a dendrogram
	-a tree like diagram that records the sequences of merges or splits

strengths of hierarchical clustering
-do not have to assume any particular number of clusters
	-any desired # of clusters can be obtained by 'cutting' the dendrogram at the proper level
-they may correspond to meaningful taxonomies 
	-example in biological sciences (e.g. animal kingdom, phylogeny reconstruction, ...)

agglomerate clustering
	-start with the points as individual clsuters
	-at each step merge the closest pair until only one cluster (or k clusters) left

-traditional hierarchical algorithms use a similarity or distance matrix 
	-merge or split one cluster at a time

Agglomerative clustering algorithm
-most popular hierarchical technique
-basic algorithm is straightforward
	-compute the proximity matrix
	-let each data point be a cluster
	-repeat
		-merge 2 closest clusters 
		-update the proximity matrix
	-until only a single cluster remains
-key operation is the computation of the proximity of two clusters
	-different approaches to defining the distance b.w. clusters distinguish the different algorithms

how to define inter-cluster similarity
-MIN (single link)
	-euclidean distance b.w. 2 individual points (the closest)
-MAX
	-same as min but the 2 farthest points
-group average
-distance b.w. centroids
-other methods driven by an objective function
	-Wards method uses squared error

Cluster Similarity: Ward's Method
-similarity of 2 clusters is based on the increase in squared error when 2 clusters are merged
	-similar to group average if distance b.w. points is distance squared
-less susceptible to noise and outliers
-biased towards globular clusters
-hierarchical analogue of K-means
	-can be used to initialize K-means

Time and Space requirements
-O(n^2) space since it uses the proximity matrix
	-N is the number of points
-O(N^3) time in many cases
	-there are N steps and at each step the size, N^2 proximity matrix must be updated and searched
	-complexity can be reduced to O(N^2log(N)) time by using kd-tree


Problems and Limitations of Hierarchical Clustering
-once a decision is made to combine 2 clusters it cannot be undone
-no objective function is directly minimized
-different schemes have problems with one or more of the following
	-sensitivity to noise and outliers
	-difficulty handling different sized clusters and convex shapes
	-breaking large clusters

Density Based Clustering 

DBSCAN
-density-based algorithm
	-density=number of points w.in a specified radius (Eps)
	-a points is a core point if it has more than a specified number of points (MinPts) w.in Eps
	-a border point has fewer than MinPts w.in Eps, but is in the neighborhood of a core point
	-a noise point is any point that is not a core point or a border point

DBSCAN Algorithm
-eliminate noise points
-perform clustering on the remaining points
	-current_cluster_label = 1
	for al core points do
		if the core point has no cluster label then
			current_cluster_label = ccl + 1
			Label the current core point with cluster
			label current_cluster_label
		end if
		for all points in the Eps-neighborhood except the ith point itself do
			if the point does not have a cluster label then
				label the point with cluster label current_cluster_label
			end if
		end for
	end for

When DBSCAN works well
-resistant to noise
-can handle clusters of different shapes and sizes

When DBSCAN does not work well
-lot of different densities
-high dimensional data
	-notion of distance becomes less and less meaningful

DBSCAN: Determining Eps and Min points
-idea is that for points in a cluster, their kth nearest neighbors are at roughly the same distance
-noise points have the kth nearest neighbor at farther distance
-so, plot sorted distance of every point to its kth nearest neighbor

-if minpoints = 4 then we grab the fourth nearest neighbor for a point
-graph all the points' distances to their 4th nearest neighbor
	-look for the natural drop off in the graph 
	-point where it begins to climb very fast
	-that is what the Eps should be

-look over kmeans and DBSCAN for test



-------------------------------------------------------

Data Mining Lecture 19 Notes

-in class assignment

-------------------------------------------------------

Data Mining Lecture 20 Notes

final assignment
-crisp-dm capstone

cluster validity
-for supervised classifications we ahve a variety of measures to evaluate how good our model is
	-accuracy, precision, recall
-for cluster analysis, the analogous question is how to evaluate the "goodness" of the resulting clusters?
-but "clusters are in the eye of the beholder"!
-then why do we want to evaluate them?

Different aspects of cluster validation
-Question we might want answered:
1. Determining the clustering tendency of a set of data, i.e., distinguishing whether non-random structure actually exists in the data
2. Comparing the results of a cluster analysis to externally known results, e.g. to externally given class labels
3. evaluating how well the results of a cluster analysis fit the data without reference to external information
4. Comparing the results of 2 different sets of cluster analyses to determine which is better
5. Determining the "correct" number of clusters

For 2, 3, and 4, we can further distinguish whether we want to evaluate the entire clustering or just individual clusters

-easy in 2D, but we want something that scales to multiple Ds

Method One: Using Simularity MAtrix
-order the similarity matrix w.r.t cluster labels and inspect visually
	-for each row you take the correlation of one row to every othe row
	-you can then cluster them by the similarity
-clusters in random data are not so crisp
	-if not center based or density based then its not great for clustering (DBSCAN)
	-if K-means it gets a little better
	-hierarachical conglomerative with complete links
-drawback: does not scale well to non-globular density 
	-it assumes some degree of center based data
-similarity looks at distance measure-if a cluster is 2 intertwined Us then it will appear that some points in the clusters are more similar to points in other clusters

Can we do better with visual inspection?

External Measures
-external index: used to measure the goodness of a clustering structure with respect to external information
	-homogeneity
		-percentage of instances where each assigned cluster contains only members of a single class
	-completeness
		-all members of a given class are assigned to the same cluster
	-drawback: if we randomly assign labels, what is the expected value for these?

-a clustering that might have labels could be genetics with phenotypes, stars with stages, etc.

External Measures: Adjusted Rand Index
-external index, but normalized (helps fix the issue of not comparing apples to apples regarding different clusters)
-let a = # of pairs of elements in both the ground truth class and the assigned clustering
-let b = # of pairs in different ground truth classes and in different assigned clusterings
-let c = total # of pairs in the dataset
	- unadjusted RI = (a+b)/c
	- adjusted RI (ARI) = (RI - E[RI])/(max(RI) - E[RI])
	-E[RI] is the expected valude for a random assigned clustering
		-depends on the size of the dataset

External Measures: Adjusted Mutual Information
-let U = ground truth assignment
-let V = clustering assignment
-N = total # of points

	H(U) = sumi=1..|U| of (P(i)log(P(i)))
					- P(i) = |U_i|/N

	H(V) = sumj=1..|V| of (P'(j)log(P'(j)))
					- P(j) = |V_j|/N

	MI(U,V) = sumi=1..|U| sumj=1..|V| of P(i,j)*
								log(P(i,j)/(P(i)P'(j)))
			-P(i,j) = | U_i intersection V_j|/ N

	AMI = (MI- E[MI])/(max(H(U),H(V))-E[MI])
		-this would be an adjusted normalized value

External Measures: tradeoffs
-advantages
	-easy to interpret
	-no assumption about the underlying clusters
-disadvantages
	-requires ground truth

Internal Measures: SSE
-clusters in more complicated figures aren't well separated
-internal index: used to measure the goodness of a clustering structure w.o.r.t. external information
	SSE
-SSE is good for comparing 2 clusterings or 2 clusters (average SSE)
-can also be used to estimate the # of clusters
-really good if we expected our data to be globular we could cluster it with different # of clusters, and check the sum squared error with each value
	-when it evens off that is the ideal clustering

internal measure: SSE
-SSE curve for a more complicated data set
	-it assumes globular data...
	-SSE is not good if the data isn't center based

internal measures: cohesion and separation
-cluster cohesion: measures how closely related are objects in a cluster 
	-ex: SSE
-cluster separation: measure how distinct or well-separated a cluster is from other clusters
-EX: Squared Error
	-cohesion is measured by the w.in cluster sum of squares (SSE)
		WSS = sumi sumx-includedin-C_i of ((x-m_i)^2)
	-separation is measured by the b.w. cluster sum of squares
		BSS = sumi of (|C_i|*( (m-m_i)^2) )
		-where |C_i| is the size of cluster i
		-takes distance of all the clusters normalized by the size of the cluster

-a proximity graph based approach can also be used for cohesion and separation
	-cluster cohesion is the sum of the weight of all links w.in a cluster
	-cluster separation is the sum of the weights b.w. nodes in the cluster and nodes outside the cluster

internal measures: silhouette coefficient
-silhouette coefficient combine ideas of both cohesion and separation, but for individual points, as well as clusters and clusterings
-for an individual point, i
	-Calculate a = avg. dissimilarity of i to the points in its cluster
	-Calculate b = min (average dissimilarity of i to the points in another cluster)
	-the silhouette coefficient for a point is then given by

	s = 1 - a/b if a<b, (or s = b/a -1 if a>=b, not 
										the usual case)
	-if a>=b then throw that clustering algorithm away
	-typically b.w. 0 and 1
	-the closer to 1 the better
-can calculate the avg. silhouette width for a cluster or clustering
-O(n^2)

Demo-----------

cell 2
-gets 2D data - good for visualizations
-make blobs create center based data sets

cell 3
-k-means and .fit
-estimate some different measures of how good it is
	-homogeneity score
	-completeness
	-v-measure
	-adj rand index
	-adjusted mutual info
	-silhouette coeff.
	-take all these and do some plottings
-all of them were 1 (perfect clustering)
	-except silhouette coefficient
		-this is b.c. it just an agglomeration of distances so close to 1 is good - no sense of labels

use agglomerative clustering
-smaller sil coeff and adj rand index and adj mut info
-so k-means would be better

use dbscan
-adj. eps to help it find the coeffs

moons with k-means
-pretty bad

moons with DBSCAN
-good - excepth for sil coeff b.c. non-globular

run similarity matrix 
-take pairwaise data of clusters
-wasn't horrible- some clustering even though non-globular

graph sil coeff by num clusters


grid_to_graph of lena
-vectorize each pixel and stick into matrix
-using k-means it doesn't assume any connectivity
-use agglomerative with connectivity
	-any pixels connected have a graph link of 1
	-if not connected to other pixels then don't merge clusters

-------------------------------------------------------

Data Mining Lecture 21 Notes


Expectation maximization

mixture models
-assumption: data has been generated fromsome underlying statisitical distribution
-we can model that process in order to find the most likely "statistical distributions" that generated the points we are seeing

Mixture models as probabilities
-gaussian distribution multiplied by a weight
-we assume that somehow we can observe x from an unknown (or latent) variable z:
	- z_(0): some unknown class labeling
	- x_(0) the instance from the model

	p(x_(i),z_(i)) = p(x_(i) | z_(i))P(z_(i))
	-assume x_(i) is a gaussian (normal) distribution
	-z_(i) is the class prior
	- p(z_(i) = j) = w_j
-basically we say the underlying distribution is modeled well by drawing x_(i) from one of k gaussian distributions with a prior

sum j=1..k of w_j
-k clusters


Self Test: Mixture models as probabilities
-what is the probability that x came from each mixture if:
	p(x|z=0) = .25 		z = 0: .25*.6 = .15
	p(z=0) = .6

	p(x|z=1) = .36 		z = 1: .36*.4 = .144
	p(z=1) = .4


if no labels...

-so the likelihood of each x is given by marginalization (i.e., the sum of all mixtures)

l(x_(i)) = sum j=1..k of p(x_(i)| z_(i) = j)*p(z_(i)=j)


total likelihood of a set of X = {x_(1)...m}:
	l - product i=1..m of sum j=1..K of
						p(x_(i)| z_(i) = j)*p(z_(i)=j)
	-left side are just gaussians
	-right part are just scalars that sum to 1

Self Test:
What is the probability that x from a previous example was generated by the model?
p(x|z=0) = .25
p(z=0) = .6 		z=0: .25*.6 = .15

p(x|z=1) = /36 		z=1: .36*.4 = .144
p(z=1) = .4

Answer /(x) .15 + .144 = .294


-simplifying from previous notation, we get:
	l = prod i=1..m sum j=1..k p(x_(i)|z_(i) = j)*w_j
-as always we want to maximize this likelihood from the instances we have observed!
-that means estimating parameters of each Gaussian distribution and each prior weight w_i
-bad news: this has no closed form solution
-more bad news: each gaussian might have a bunch of parameters
-good news: it does have an iterative solution

mixture models, multivariate
-for the multivariate case, each Gaussians looks like this
	= 1/(sqrt((2*pi)^k|SIGMA|)
			exp(-.5*(x-mu)'SIGMA^-1(x-mu))

if N is the # of attributes (i.e. the length of each vector x)
	mean: has N parameters
	covariance: has N^2 parameters that you need to estimate

	so if we have K gaussians,
	we have K*N + K*N^2 parameters to estimate

	= 1/(sqrt((2*pi)^k|SIGMA|)
		exp(-.5*(x-mu)'SIGMA^-1(x-mu))
-we can simplify the # of parameters by limiting the expressiveness of each Gaussian:
	spherical
-assume all variances ar the same and all along the diagonal

covariance: has 1 parameter


diagonal
-only variances on diagonal but not necessarily equal
covariance: has N parameters
-ellipse

full covariance
-full covariance matrix need to be estimated 
	-covariance: has N^2 parameters

self test:
-given this 2D data set, what type of covariance would you select?
-answer = it depends
	-there is either one mixture with full covariance
	-or 2 mixtures with spherical (or diagonal) covariance

expectation maximization
-turns out the maximum likelihood solution has no closed form solution, so we need to solve it literally...

-init centers of each gaussian mixture (e.g. run k-means)
-repeat
	-expectation: calculate probability that point belongs to every cluster, re-normalize weights w_ik
	-maximization: given the new weights, find new weighted mean and weighted covariaance that center gaussians over the weighted points, maximizing the probabilities
-until parameters do not change

Tradeoffs for gaussian mixtures
	negative
-scalability is a concern for large dimensional datasets
	-covariance and determinant calculations are time consuming
-can only handle globular clusters
-small clusters of cluster on a line are not easily captured
-spurious singularities can be a problem without limiting the variance
	positive
-positively, more general than many other clustering types
-robest to density difference
-clusters are easy to characterize from parameters
-many processes are the result of a gaussian process

How many gaussians?
-bayesian information criterion
	-use total likelihood to determine goodness
	-penalize complex models
	-smaller is better
		BIC = -2ln(l) + k*ln(m)

	as likelihood goes up BIC becomes more and more negative
	-first ln is total log likelihood
	-second ln is penalize number of gaussians
	-want BIC to be a small value

DEMO
More advanced clustering
	mixture.GMM(n_components=n_components,covariance_type=...)

-remember no cross validation with clustering
-can graph number of components by BIC
	-the lower the BIC the better
-BIC is lower for spherical covariance b.c. it penalizes the complexity (# of gaussians and number of parameters)
-plot gaussian mixtures it found

---demo over

Graph-based clustering
-like agglomerative hierarchical clustering...
-graph based clustering uses the proximity graph
	-start with the proximity graph
	-consider each point as a node in a graph
	-each edge b.w. 2 nodes has a weight which is the proximity b.w. the 2 points
	-initially the proximity graph is fully connected
	-MIN (single-link) and MAX(complete-link) can be viewed as starting with this graph
-in the simplest case, clusters are connected components in the graph

Graph-based clustering: sparsification
-the proximity matrix can be made sparser!
-the amount of data that eneds to be processed is drastically reduced
	-sparsification can eliminate more than 99% of the entries in the proximity matrix
		-as many values to be zero as possible
		-less values to have to make comparisons to and less clusters to merge together
		-the amt. of time required to cluster the data is drastically reduced
		-the size of the problems that can be handled is increased (more room in memory)
-clustering may work better
	-chameleon

easiest method for sparsification is the k-neighbors graph

self-test
-k-neighbors graph
	-make the connections on this graph for a sparse 2-NN connections

Limitations of current merging schemes
-existing merging schemes in hierarchical clustering algorithms are static in nature
	MIN:
		-merge 2 clusters based on their closeness (or minimum distance)
	Group-average:
		-merge 2 clusters based on their average connectivity

Chameleon: clustering using dynamci modeling
-adapt to the characterisitics of the data set to find the natrual clusters
-use a dynamic model to measure the similarity b.w. clusters
	-main property is teh relative closeness and relative interconnectivity of the cluster
	-2 clusters are combined if the resulting cluster shares certain properties with the constituent clusters
	-the merging scheme preserves self-similarity
-1 of the areas of application is spatial data

Aside: Spatial Data Sets
-clusters are defined as densely populated regions of the space
-clusters have arbitrary shape, orientation, and non-uniform sizes
-difference in densities across clusters and variation in densities w.in clusters
-existence of special artifacts (streaks) and noise
	-like sine wave
the clustering algorithm must address the above characteristics and also require minimal supervision

Chameleon: Steps
1. Make it sparse:
Represent the data by a sparse graph
	given a set of points, construct the k-nearest neighbor (k-NN) graph to capture the relationship b.w. a point and its k nearest neighbors
	-concept of neighborhood is captured dynamically (even if region is sparse)
2. Over-cluster: use a multilevel graph partitioning algorithm on the graph to find a large # of clusters of well-connected vertices
	each cluster should contain mostly points from one "true" cluster, i.e. is a sub-cluster of a "real" cluster
3. Merge clusters: use hierarchical agglomerative clustering to merge sub-clusters
	2 clusters are combined if the resulting cluster shares certain properties with the constituent clusters

	2 key properties to model cluster similarity
	3.a - relative interconnectivity: absolute interconnectivity of 2 clusters normalized by the internal connectivity of the clusters
	RI(C_i,C_j) = |EC_{C_i,C_j}|
				 _____________________
				 |EC_(C_i)| + EC_(C_j)
				 _____________________
				 			2
	|EC| = # of edges in a given cluster only
	-tells you how much interconnectivity will change after merging

	3.b - relative closeness: absolute closeness of 2 clusters normalized by the internal closeness of the clusters

	RC(C_i,C_j) = 			

					Sbar_(EC_{C_i,C_j})
 _____________________________________________________
 	|C_i|					  |C_j|
 ________*Sbar_(EC_(C_i))+_____________*Sbar_(EC_(C_j))|C_i|+|C_j| 				|C_i| + |C_j|


4. Get "fused" similarity: merge closest self similar clusters
	RC*(RI^a)

-choose whichever clusters that give you the values as close to 1 as possible


------------------------------------------------------

Data Mining Lecture 22 Notes

Spectral Clustering
-using eigen vectors
-lets treat proximity matrix like an actual matrix
-top down: start with all the data in one giant cluster, then decide how to break the clusters
-the problem of clustering becomes: how do we cut the graph to make separate clusters

Spectral clustering: adjacency
-adjacency matrix: like a proximity matrix with edges indicating a link (i.e. the sparse matrix from k neighbors graph)
-how can we cut this graph such that the resulting clustering are highly interconnected and the cut breaks as few links as possible?

Spectral Clustering: degree
-degree matrix: diagonal matrix with number of edges for each node

Spectral Clustering: Laplacian
-Laplacian Matrix: D-A
	-sum along any row or any column should be zero


Spectral Clustering: Eigenvectors of Laplacian
-now find the eigenvectors of the Laplacian matrix
	-second eignenvector is called the Fiedler Vector
	-tells us where to palce the vertices on a line
-graph on a 1D line where each node falls based on the first eigenvector
-move a vertical line along number line and this tells you potential spots to cut the graph
	-1 is optimal
-this sorting tells you where to make candidate cuts!
-and we can evaluate the quality of each cut: like doing the normalized cut
	Volume: # edges in cluster that do not connect the 2 clusters - want high volume
	Cut: # edges that interconnect the clusters
		-want this low
	Normalized Cut: combines each measure. Fairly common
-could also look at RI and RC to have resulting clusters preserve self-similarity based on where it is cut

Spectral Clustering Eigenvectors of Laplacian
-don't stop at first eigenvector, but grab the second 1 as well
-instead of graphing in 1D, graph in 2D
-run k-means or any clustering method
	can use all the methods of clustering for globular clusters, but the actual clusters are not necessarily globular in the feature space, only in the spectral space
-this is called embedded this large problem into its spectral space to make it easier

spectral clustering: similarity
-but there is not reason to just use the adjacency matrix, we can use proximity matrices based upon any similarity measure that exists!!
-we could even just use kernels that represent the similarity of 2 points...
-this is typically called the "Affinity" of the pairs of points in the spectral cluster

Spectral Clusterin Demo-------
-sklearn has spectral clustering framework

-make sure to have py amg installed to speed up eigen_solver

use fit_predict instead of fit and then predict


-------------------------------------------------------

Flipped Module 5

Association Rule Mining
-given a set of transactions find rules that will predict the occurrence of an item based on the occurrences of other items in the transaction

Example of Association Rules

	(Diaper)-> (Beer)
	(Milk,bread)->(Eggs,Coke)
	(Beer,Bread)->(Milk)

implication means co-occurence not causality
-this is more than "buying power"
(H,BP)->(Stress)

Assocation Rule Mining
-also make many implications for reccomendation systems

Definition: Frequent Itemset
-itemset
	-a collection of one or more items 
		ex: (milk,bread,diaper)
	-k-itemset
		-an itemset that contains k items
-support count (sigma)
	-frequency of occurence of an itemset
		e.g. sigma(Milk,bread,diaper) = 2
-support
	-fractin of transactions that contain an itemset
		e.g. s([milk,bread,diaper]) = 2/5


Definition : Assocation rule
-Association Rule
	-an implication expression of the form X
		-> Y where X and Y are itemsets
	-example:
		(milk,diaper) -> (beer)
-rule evaluation metrics
	-support (S)
		-fraction of transactions that contain both X and Y
	-confidence (c)
		-measures how often items in Y appear in transactions that contain X

Self Test : association rule
....


Assocation Rule Mining Task
-Given a set of transactions T, the goal of association rule mining is to find all rules having
	support >= minsup threshold
	confidence >= minconf threshold
-brute-force approach
	-list all possible association rules
	-compute the support and confidence for each rule 
	-prune rules that fail the minsup and minconf thresholds
	=>computationally prohibitive

Mining Association Rules
-2-step approach
	-frequent itemset generation
		-generate all itemsets whose support >= minsup
	-rule generation
		-generate high confidence rules from each frequent itemset, where each rule is a binary partitioning of a frequent itemset
-frequent itemset generation is still computationally expensive


Frequent Itemset Generation
-given d items, there are 2^d possible candidate itemsets
-brute-force approach:
	-each itemset in the lattice is a candidate frequent itemset
	-count the support of each candidate by scanning the database
	-match each transaction against every candidate
	-complexity - O(NMw)=> expensive since M=2^d


Frequent Itemset generation strategies
-reduce the number of candidates (M)
	-complete search: M=2^d
	-use pruning techniques to reduce M
-reduce the # of transactions (N)
	-reduce size of N as the size of itemset increases
	-used by direct hashing and pruning (DHP) and vertical-based mining algorithms (database storage)
-reduce the # of comparisons (NM)
	-use efficient data structures to store the candidates or transactions
	-no need to match every candidate against every transaction

Reducing the # of candidates
-apriori principle
	-if an itemset is frequent then all of its subsets must also be frequent
-support of an itemset never exceeds the support of its subsets
	-the anti-monotone property of support


Apriori Algorithm
-method
	-Let k = 1
	-generate frequent itemsets of length 1
	-repeat until no new frequent itemsets are identified
		-generate length (k+1) candidate itemsets from length k freqent itemsets
		-prune candidate itemsets containing subsets of length k that are infrequent
		-count the support of each candidate by scanning the DB
		-eliminate candidates that are infrequent, leaving only those that are frequent

Reducing # of comparisons
-candidate counting:
	-scan the db of transactions to determine the support of each candidate itemset
	-to reduce the # of comparisons, store the candidates in a hash structure
		-instead of matching each transaction against every candidate, match it against candidates contained in the hashed buckets

generate hash tree
-suppose you have 15 candidate itemsets of length 3
-you need:
	-hash function
	-max leaf size: max # of itemsets , stored in a leaf node (if # of candidate itemsets exceed max leaf size, split the node)

Factors Affecting Complexity
-choice of minimum support threshold
	-lowering support threshold results in more frequent itemsets
	-this may increase number of candidates and max lenght of frequent itemsets
-dimensionality (# of items) of the data set
	-more space is needed to store support count of each item
	-if # of frequent items also increases, both computation and I/O costs may also increase
-size of database
	-since apriori makes multiple passes, run time of algorithm may increase with # of transactions
-average transactions width
	-transaction width increases with denser data sets
	-this may increase max length of frequent itemsets and traversals of hash tree (number of subsets in a transaction increases with its width)

Association: COmpact representation

compact representation of frequent itemsets
-some itemsets are redundant b.c. they ahve identical support as their supersets
-if we know A1-A10 is frequent we might not care A1-A5 is also frequent
-is there a way to reduce the redundancy of subets?

maximal frequent itemset
-an itemset is maximal frequent if none of its immediate supersets is frequent

Closed itemset
-an itemset is closed if none of its immediate supersets has the same support as the itemset

MAximal vs closed itemsets
-biggest circle is frequent itemsets
	-first nest is closed frequent itemsets
		-second nest is maximal frequent itemsets

-will go through notebook and generate frequent itemsets
-be able to judge what is maximal and what is closed


-------------------------------------------------------

Data Mining Lecture 23 Notes

-in class exam

---------------------------------------------------------

Data Mining Lecture 24 Notes

-to generate rules need to find out what is frequent first and then generate rules

maximal means all of supersets are infrequent

Association Rules Generation

Association Rule
-if you bought milk and diapers you would also buy beer
-Rule evaluation metrics
	-support(s)
		-fraction of transactinos that contain both X and Y
	-Confidence (C)
		-measures how often items in Y appear in transactions that contain X
		-always less than 1

Rules Generation
-given a frequent itemset L, find all non-empty subsets f subset L s.t. f-> L without subset f satisfies the minimum confidence requirement
-for every single itemset we can develop 2^k rules where k = items in itemset
-need confidence for every rule

Rules Generation
-how to efficiently generate rules from frequent itemsets?
	-in general, confidence does not have an anti-monotone property
		c(ABC->D) can be larger or smaller than c(AB->D)
	-but confidence of rules generated from same itemset have anti-monotone property

Rules generation for Apriori Algorithm
-in rules lattice prune subtrees that will also be below confidence level

Self Test
	-if {Male,3rd Class, Did not survive} is a frequent itemset, what does the candidate lattice look like?
-root of tree is the 3 imply nothing
-next level is 3 leaves with each item on the right hand side of the other 2

Merging Rules for Apriori Algorithm
-candidate rule is generated by merging 2 rules that share the same prefix in the rule consequent
-join (CD=>AB,BD=>AC) would produce the candidate rule D=>ABC
-prune rule D=>ABC if its subset AD=>BC does not have high confidence

Pattern Evaluation
-Association rule algorithms tend to produce too many rules
	-many of them are uninteresting or redundant
	-redundant if {A,B,C}->{D} and {A,B}->{D}
		-{1st class, child, female}->{survived}
		-{1st class, child}->{survived}
-interestingness measures can be used to prune/rank the drived rules
-in mining association rules, support and confidence are the only measures used

Contingency Tables
-given a rule X->Y, information needed to compute rule interestingness can be obtained from a contingency table
-confidence doesn't capture relation of conditional probability and the given probability

Statistical-base measures
-measures that take into account statistical dependence
	Lift = P(Y|X)/P(Y) = c(X->Y)/s(Y)
	Interest = P(X,Y)/(P(X)*P(Y)) = N*f_11/(f_(1+)*f(+1))

Example of lift/interest
-assocation rule of tea->coffee

Confidence = P(Coffee|Tea) = .75
but P(Coffee) = .9 so confidence is misleading

=>Lift = .75/.9 = .8333 (<1, therefore is negatively associated)
	-aka the rule is bad

#students<80% -> Pop Quiz
Confidence = P(PopQuiz|Students<80%) = .33
Lift = Confidence/P(PopQuiz) = .33/.25 = 1.65 which is >1 so the rule is good

Interest = P(X,Y)/(P(X)P(Y)) = 1.66 which is good as well b.c. it is not 1

if list = 1 then statistically independent (not a good rule)

bias in lift towards things that are more rare in the dataset

Properties of a good measure
-3 properties a good measure M must satisfy:
	M(A,B) = 0 if A and B ar statistically independent

	M(A,B) increase monotonically with P(A,B) when P(A) and P(B) remian unchanged

	M(A,B) decreases monotonically with P(A) [or P(B)] when P(A,B) and P(B)[or P(A)] remain unchanged

Subjective Interestingness Measure
-objective measure:
	-rank patterns based on statistics computed from data
		e.g. 21 measures of association (support, confidence, Laplace, Gini, Mutual information, Jaccard, etc.)
-subjective measure:
	-rank patterns according to user's interpretation
		-a pattern is subjectively interesting if it contradicts then expectation of a user
		-a pattern is subjectively interesting if it is actionable

Interestingness via unexpectedness
-need to model expectations of users (domain knowledge)
-need to combine expectation of users with evidence from data (i.e. extracted patterns)

Demo


---------------------------------------------------------

Data Mining Lecture 25 Notes


Collaborative Filtering
-much like association rule mining and much like clustering
-except we wnt a potential item to suggest to someone, base on what we know about the past
	-could be past transaction history of the item
	-or history of other users compare to this user

Collaboraitve fltering: user based
-user-based filtering
	-data is stored in a user-item matrix with ratings
	-objective: find similar users and try to fill in the missing ratings from them
		use KNN to get similar users
	-distance measure is important -> Correlation, Manhattan, Cosine
		-cosine sparse
		-correlation user bias
	-doen't scale very well
		-computationally intensive
	-we need to run KNN every time to get a user's recommendation
-item-based matrix
	-data is stored in an item-item matrix, where we computed the item-item similarity in each entry of the matrix
	-(usually only store K most similar items)
	S(x,y) = Freq(x,y)
			 _________
			 Freq(x)
	-precompute the matrix and then...

Collaborative filtering: model based
-problem: user based filtering is more optimal, but too computational to store all teh data and run queries on it...
-solution: can we project a giant user-item matrix down to a smaller matrix (i.e. reduce the number of dimensions)?
	-Matrix Factorization: there are k "hidden" factors

	user-item matrix ~ user-factor matrix * factor-item matrix

	matrix factorization
-other variants on the factorization also exist

Steps: 
	1. Factorize matrix into these smaller matrices
	2. Complete the user-item matrix from low rank representations
	3. Pick recommendations for each user from filled in matrix!
		a. Recommendation becomes very fast
	4. Profit

Collaborative Filtering: evaluationg
-cross validation
-what should folds look like?
	-stratified across users and ratings
-easy to use cross validation with
	-precision, recall, and RMSE
-can look at RMSE per-item, per-user, or total
	Root means square error

Collaborative Filtering Evaluation
-precision and recall:
-need to look "per-user" rating for both user-item and item-item
	-let p_k be a vector of the k highest ranked recommendations for a particular user, and let a be the set of items for that user in the groundtruth dataset
	-per user recall at cutoff "k"
			R(k) = |a n Pk|
				   ________
				   	|a|

	-per user precision at cutoff "k"
					|a n Pk|
			P(k) = ___________
					   k


Dato: easy, production level recommendations
-Problem : user item matrices are absolutely gigantic
-Solution: use data representations that allow the matrices to be stored on the hard drive or completely subsumed from the data analysis itself
	
Demo on collaborative filtering


---------------------------------------------------------

Data Mining Lecture 27 Notes


Assignment 4 - lecture 20 has demo


Big Data
-refers to the exponential size of data creation
-but also should look at the stagnant processor speed

Big data and mining on parallel architecetures
-scalable learning is hard b.c. of
	-programmability
	-failures
	-distributing data to parallel processes

Data Parallel (embarrassingly parallel)
-simplest type of parallelism and many times one of the most useful/easy to exploit
	-problems can be broken into smaller, independent sub-problems
	-like running prediction on a data set of independent instances

Data parallel Map Reduce
-Map
	-data parallel over the elements (like images)
	-emit key/values (like image features)

		input-> list(key,value)
-Reduce
	-aggregate over the keys
	-must be commutative and associative operations
	-data parallel over keys
	-emit reduced output
		
		list(key,value)->output


Map reduce in Data Mining
-much of data mining is iterative
-though not always
	-random forests (or any bagging)
	-multi-class LR and SVM (for large classes)
	-rule mining transaction comparison
	-kNN distance calculations
	-Naive Bayes conditionals estimation
-data getes moved around your cluster, possibly replicated, overhead might eliminate benefits
-mostly, Map Reduce is great for embarrassingly parallel
	-feature extraction
	-grid searching
	-cross validation
	-computing statistics


Grid searching
-try to find the best parameters


Demo on Parallel Parameter Tuning--------





---------------------------------------------------------

End of Class



























