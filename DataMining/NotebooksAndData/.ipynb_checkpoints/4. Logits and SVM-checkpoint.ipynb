{
 "metadata": {
  "name": "",
  "signature": "sha256:5bc59ea9b9b3cbb10f901e3842d15b77dba68abbba9ced5160a4a3873a7e4839"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Logistic Regression, SVMs, and Gradient Optimization\n",
      "*Please address questions to Professor Eric Larson, eclarson@smu.edu*\n",
      "\n",
      "In this notebook we will explore methods of using logistic regression in `scikit-learn` and we will also investigate methods for gradient descent. Finally we will look at using support vector machines and investigate parameters of kernel functions. A basic understanding of `scikit-learn` is required to complete this notebook, but we start very basic. Note also that there are more efficient methods of separating testing and training data, but we will leave that for a later lecture. \n",
      "\n",
      "First let's load a dataset and prepare it for analysis. We will use pandas to load in data, and then prepare it for classification. We will be using the titanic dataset (a very modest sized data set of about 1000 instances) before loading a larger, more complicated dataset for gradient descent methods.\n",
      "\n",
      "______\n",
      "The imputation methods used here are discussed in a previous notebook. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "df = pd.read_csv('data/titanic.csv') # read in the csv file\n",
      "\n",
      "# 1. Remove attributes that just arent useful for us\n",
      "del df['PassengerId']\n",
      "del df['Name']\n",
      "del df['Cabin']\n",
      "del df['Ticket']\n",
      "# 2. Impute some missing values, grouped by their Pclass and SibSp numbers\n",
      "df_grouped = df.groupby(by=['Pclass','SibSp'])\n",
      "# now use this grouping to fill the data set in each group, then transform back\n",
      "df_imputed = df_grouped.transform(lambda grp: grp.fillna(grp.median()))\n",
      "df_imputed[['Pclass','SibSp']] = df[['Pclass','SibSp']]\n",
      "# 3. Computed discrete features agains now with the newest values\n",
      "df_imputed['age_range'] = pd.cut(df_imputed.Age,[0,16,24,65,1e6],4,labels=[0,1,2,3]) # this creates a new variable\n",
      "# 4. drop rows that still had missing values after grouped imputation\n",
      "df_imputed.dropna(inplace=True)\n",
      "df_imputed.age_range = df_imputed.age_range.astype(np.int)\n",
      "# 5. Rearrange the columns\n",
      "df_imputed = df_imputed[['Survived','Age','age_range','Sex','Parch','SibSp','Pclass','Fare','Embarked']]\n",
      "\n",
      "df_imputed.info()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "________\n",
      "Now let's look a little further at each of the categorical objects. Note that age range has already been saved as an ordinal. We need to look at `Sex` and `Embarked` objects. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_imputed[['Sex','Embarked']].describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Because the `Sex`  attribute binary, there is no need to encode it using OneHotEncoding. We can just convert it to an integer. However, we should transform the `Embarked` attribute to take on three different values--one for each possible variable outcome."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tmp_df = pd.get_dummies(df_imputed.Embarked,prefix='Embarked')\n",
      "df_imputed = pd.concat((df_imputed,tmp_df),axis=1)\n",
      "\n",
      "df_imputed['IsMale'] = df_imputed.Sex=='male' \n",
      "df_imputed.IsMale = df_imputed.IsMale.astype(np.int)\n",
      "\n",
      "df_imputed.info()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Now let's clean up the dataset\n",
      "del df_imputed['Sex']\n",
      "del df_imputed['Embarked']\n",
      "df_imputed.info()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Finally, let's create a new variable based on the number of family members\n",
      "# traveling with the passenger\n",
      "\n",
      "df_imputed['FamilySize'] = df_imputed.Parch + df_imputed.SibSp\n",
      "df_imputed.info()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "___\n",
      "# Training and Testing Split\n",
      "For training and testing purposes, let's gather the data we have and randomly grab 80% of the instances for training and the remining 20% for testing."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from numpy import random as rd\n",
      "\n",
      "y = df_imputed['Survived'].values # get the labels\n",
      "del df_imputed['Survived']\n",
      "X = df_imputed.values\n",
      "\n",
      "N = len(df_imputed) # total number of instances\n",
      "permuted_indices = rd.permutation(N) # random permutation of the instances\n",
      "\n",
      "N_eighty_percent = int(0.8*N)\n",
      "\n",
      "X_train = X[:N_eighty_percent]\n",
      "X_test = X[N_eighty_percent:]\n",
      "\n",
      "y_train = y[:N_eighty_percent]\n",
      "y_test = y[N_eighty_percent:]\n",
      "\n",
      "print 'Training sizes, X:', X_train.shape, 'Y:', y_train.shape\n",
      "print 'Testing sizes, X:', X_test.shape, 'Y:', y_test.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "___\n",
      "#Logistic Regression\n",
      "Now let's use Logistic Regression from `scikit-learn`. The documentation can be found here:\n",
      "\n",
      "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# run logistic regression and vary some parameters\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "lr_clf = LogisticRegression(penalty='l2', C=1.0, class_weight=None) # get object\n",
      "lr_clf.fit(X_train,y_train)  # train object\n",
      "\n",
      "y_hat = lr_clf.predict(X_test) # get test set precitions\n",
      "\n",
      "from sklearn import metrics as mt\n",
      "\n",
      "acc = mt.accuracy_score(y_test,y_hat)\n",
      "conf = mt.confusion_matrix(y_test,y_hat)\n",
      "print acc \n",
      "print conf"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# interpret the weights\n",
      "for coef, name in zip(lr_clf.coef_.T,df_imputed.columns):\n",
      "    print name, 'has importance of', coef[0]\n",
      "    \n",
      "# does this look correct? "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These weight interpretations are not neccessarily interpretable because of the values we had. Very large attribute values could just as easily be assigned a higher weight. Instead, let's normalize the feature values so that all the attributes are on the same playing field.\n",
      "___"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "# scale attributes by the training set\n",
      "scl_obj = StandardScaler()\n",
      "scl_obj.fit(X_train) # find points that make this zero mean and unit std\n",
      "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
      "X_test_scaled = scl_obj.transform(X_test) # apply those means and std to the test set (without snooping at the test set values)\n",
      "\n",
      "# train the model just as before\n",
      "lr_clf = LogisticRegression(penalty='l2', C=0.05, class_weight=None) # get object\n",
      "lr_clf.fit(X_train_scaled,y_train)  # train object\n",
      "\n",
      "y_hat = lr_clf.predict(X_test_scaled) # get test set precitions\n",
      "\n",
      "from sklearn import metrics as mt\n",
      "\n",
      "acc = mt.accuracy_score(y_test,y_hat)\n",
      "conf = mt.confusion_matrix(y_test,y_hat)\n",
      "print 'accuracy:', acc \n",
      "print conf\n",
      "\n",
      "# sort these attributes and spit them out\n",
      "zip_vars = zip(lr_clf.coef_.T,df_imputed.columns)\n",
      "zip_vars.sort(key = lambda t: np.abs(t[0]))\n",
      "for coef, name in zip_vars:\n",
      "    print name, 'has importance of', coef[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# now let's make a pandas Series with the names and values, and plot them\n",
      "from matplotlib import pyplot as plt\n",
      "%matplotlib inline\n",
      "\n",
      "weights = pd.Series(lr_clf.coef_[0],index=df_imputed.columns)\n",
      "weights.plot(kind='bar')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "At this point it would make sense to remove variables that are highly related to one another or ones that are irrelevant and keep going with the weights analysis. What variables would you remove?\n",
      "___"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Support Vector Machines\n",
      "We can use the previous training and testing attributes (scaled) to investigate the weights and support vectors in the attributes. SVMs were first hypothesized by Vladmir Vapnik."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import Image\n",
      "# Here he is, in all his glory:\n",
      "# Image(url='http://engineering.columbia.edu/files/engineering/vapnik.jpg')\n",
      "Image(url='http://yann.lecun.com/ex/images/allyourbayes.jpg')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# lets investigate SVMs on the data and play with the parameters and kernels\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "# train the model just as before\n",
      "svm_clf = SVC(C=1.0, kernel='linear', degree=3, gamma=0.0) # get object\n",
      "svm_clf.fit(X_train_scaled, y_train)  # train object\n",
      "\n",
      "y_hat = svm_clf.predict(X_test_scaled) # get test set precitions\n",
      "\n",
      "acc = mt.accuracy_score(y_test,y_hat)\n",
      "conf = mt.confusion_matrix(y_test,y_hat)\n",
      "print 'accuracy:', acc \n",
      "print conf"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# look at the support vectors\n",
      "print svm_clf.support_vectors_.shape\n",
      "print svm_clf.support_.shape\n",
      "print svm_clf.n_support_ "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# if using linear kernal, these make sense to look at (not otherwise, why?)\n",
      "print svm_clf.coef_\n",
      "weights = pd.Series(svm_clf.coef_[0],index=df_imputed.columns)\n",
      "weights.plot(kind='bar')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# now lets look at the support for the vectors and see if we they are indicative of anything\n",
      "# grabe the rows that were selected as support vectors (these are usually instances that are hard to classify)\n",
      "df_support = df_imputed.iloc[svm_clf.support_,:]\n",
      "df_support['Survived'] = y[svm_clf.support_]\n",
      "df_imputed['Survived'] = y\n",
      "df_support.info()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# now lets see the statistics of these attributes\n",
      "from pandas.tools.plotting import boxplot\n",
      "\n",
      "df_grouped_support = df_support.groupby(['Survived'])\n",
      "df_grouped = df_imputed.groupby(['Survived'])\n",
      "\n",
      "# plot KDE of Different variables\n",
      "vars_to_plot = ['Age','Pclass','IsMale','FamilySize']\n",
      "\n",
      "for v in vars_to_plot:\n",
      "    plt.figure(figsize=(10,4))\n",
      "    # plot support vector stats\n",
      "    plt.subplot(1,2,1)\n",
      "    ax = df_grouped_support[v].plot(kind='kde')\n",
      "    plt.legend(['Perished','Survived'])\n",
      "    plt.title(v+' (Support)')\n",
      "    \n",
      "    # plot original distributions\n",
      "    plt.subplot(1,2,2)\n",
      "    ax = df_grouped[v].plot(kind='kde')\n",
      "    plt.legend(['Perished','Survived'])\n",
      "    plt.title(v+' (Original)')\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That's mostly it for using these things! They are really nice analysis tools and provide human interpretable summaries of the data. \n",
      "___\n",
      "\n",
      "# Gradient Based Alternatives\n",
      "So now let's go and find out how we can use these when our data size gets bigger. Like a lot bigger. We will use a kaggle dataset that attempts to classify plankton. We will use some example code to get us started from the tutorial here:\n",
      "http://www.kaggle.com/c/datasciencebowl/details/tutorial "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Image(url='https://kaggle2.blob.core.windows.net/competitions/kaggle/3978/media/plankton%20schmorgasborg.jpg')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# load another dataset (large) and train using various methods of gradient (and mini-batch)\n",
      "import glob\n",
      "import os\n",
      "\n",
      "# get the classnames from the directory structure\n",
      "directory_names = list(set(glob.glob(os.path.join(\"../../../Desktop\",\"kaggle_plank\", \"*\"))\n",
      " ).difference(set(glob.glob(os.path.join(\"../../../Desktop\",\"kaggle_plank\",\"*.*\")))))\n",
      "\n",
      "print 'number of classes:', len(directory_names)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Rescale the images and create the combined metrics and training labels\n",
      "from skimage.transform import resize\n",
      "from skimage.io import imread\n",
      "\n",
      "#get the total training images\n",
      "numberofImages = 0\n",
      "for folder in directory_names:\n",
      "    for fileNameDir in os.walk(folder):   \n",
      "        for fileName in fileNameDir[2]:\n",
      "             # Only read in the images\n",
      "            if fileName[-4:] != \".jpg\":\n",
      "              continue\n",
      "            numberofImages += 1\n",
      "\n",
      "# We'll rescale the images to be 40x40\n",
      "maxPixel = 25\n",
      "imageSize = maxPixel * maxPixel\n",
      "num_rows = numberofImages # one row for each image in the training dataset\n",
      "num_features = imageSize # for our ratio\n",
      "\n",
      "# X is the feature vector with one row of features per image\n",
      "# consisting of the pixel values and our metric\n",
      "X = np.zeros((num_rows, num_features), dtype=float)\n",
      "# y is the numeric class label \n",
      "y = np.zeros((num_rows))\n",
      "\n",
      "files = []\n",
      "# Generate training data\n",
      "i = 0    \n",
      "label = 0\n",
      "# List of string of class names\n",
      "namesClasses = list()\n",
      "\n",
      "print \"Reading images\"\n",
      "# Navigate through the list of directories\n",
      "for folder in directory_names:\n",
      "    # Append the string class name for each class\n",
      "    currentClass = folder.split(os.pathsep)[-1]\n",
      "    namesClasses.append(currentClass)\n",
      "    for fileNameDir in os.walk(folder):   \n",
      "        for fileName in fileNameDir[2]:\n",
      "            # Only read in the images\n",
      "            if fileName[-4:] != \".jpg\":\n",
      "              continue\n",
      "            \n",
      "            # Read in the images and create the features\n",
      "            nameFileImage = \"{0}{1}{2}\".format(fileNameDir[0], os.sep, fileName)            \n",
      "            image = imread(nameFileImage, as_grey=True)\n",
      "            files.append(nameFileImage)\n",
      "            #axisratio = getMinorMajorRatio(image)\n",
      "            image = resize(image, (maxPixel, maxPixel))\n",
      "            \n",
      "            # Store the rescaled image pixels and the axis ratio\n",
      "            X[i, 0:imageSize] = np.reshape(image, (1, imageSize))\n",
      "            #X[i, imageSize] = axisratio\n",
      "            \n",
      "            # Store the classlabel\n",
      "            y[i] = label\n",
      "            i += 1\n",
      "            # report progress for each 5% done  \n",
      "            report = [int((j+1)*num_rows/20.) for j in range(20)]\n",
      "            if i in report: print np.ceil(i *100.0 / num_rows), \"% done\"\n",
      "    label += 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# here is where the online tutorial code stops and my code starts\n",
      "print X.shape\n",
      "print y.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# now divide the data into test and train using scikit learn built-ins\n",
      "from sklearn.cross_validation import StratifiedShuffleSplit \n",
      "\n",
      "cv = StratifiedShuffleSplit(y,n_iter=1,train_size=0.5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# use some compact notation for creating a linear SVM classifier with stichastic descent\n",
      "from sklearn.linear_model import SGDClassifier\n",
      "\n",
      "regularize_const = 0.1\n",
      "iterations = 5\n",
      "svm_sgd = SGDClassifier(alpha=regularize_const,\n",
      "        fit_intercept=True, l1_ratio=0.0, learning_rate='optimal',\n",
      "        loss='hinge', n_iter=iterations, n_jobs=-1, penalty='l2')\n",
      "\n",
      "scl = StandardScaler()\n",
      "for train_idx, test_idx in cv:\n",
      "    svm_sgd.fit(scl.fit_transform(X[train_idx]),y[train_idx])\n",
      "    yhat = svm_sgd.predict(scl.transform(X[test_idx]))\n",
      "    \n",
      "    conf = mt.confusion_matrix(y[test_idx],yhat)\n",
      "    acc = mt.accuracy_score(y[test_idx],yhat)\n",
      "\n",
      "print 'SVM:', acc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# use some compact notation for creating a logistic regression classifier with stochastic descent\n",
      "log_sgd = SGDClassifier(alpha=regularize_const,\n",
      "        fit_intercept=True, l1_ratio=0.0, learning_rate='optimal',\n",
      "        loss='log', n_iter=iterations, n_jobs=-1, penalty='l2')\n",
      "\n",
      "scl = StandardScaler()\n",
      "for train_idx, test_idx in cv:\n",
      "    log_sgd.fit(scl.fit_transform(X[train_idx]),y[train_idx])\n",
      "    yhat = log_sgd.predict(scl.transform(X[test_idx]))\n",
      "    \n",
      "    conf = mt.confusion_matrix(y[test_idx],yhat)\n",
      "    acc = mt.accuracy_score(y[test_idx],yhat)\n",
      "\n",
      "print 'Logistic Regression:', acc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# now lets use some of what we know from this class to reduce the dimensionality of the set\n",
      "from sklearn.decomposition import RandomizedPCA\n",
      "n_components = 50\n",
      "\n",
      "pca = RandomizedPCA(n_components=n_components)\n",
      "\n",
      "iterations = 150\n",
      "log_sgd = SGDClassifier(\n",
      "        fit_intercept=True, l1_ratio=0.0, learning_rate='optimal',\n",
      "        loss='log', n_iter=iterations, n_jobs=-1, penalty='l2')\n",
      "\n",
      "for train_idx, test_idx in cv:\n",
      "    log_sgd.fit(pca.fit_transform(X[train_idx]),y[train_idx])\n",
      "    yhat = log_sgd.predict(pca.transform(X[test_idx]))\n",
      "    \n",
      "    conf = mt.confusion_matrix(y[test_idx],yhat)\n",
      "    acc = mt.accuracy_score(y[test_idx],yhat)\n",
      "\n",
      "print 'Logistic Regression:', acc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The competition does not use \"accuracy\" as the evaluation of the best model; they use the log loss:\n",
      "$$logloss=-\\frac{1}{N}\\sum_{i=1}^m\\sum_{j=1}^C {\\bf 1}_{ij}\\ln(p_{ij})$$\n",
      "\n",
      "Where there are $m$ instances (images) in the dataset, and $C$ is the number of classes. The equation ${\\bf 1}_{ij}$ is an indicator function that ensures we only add log probabilities when the class is correct. That is, it is zero if the predicted class for the $i^{th}$ instance is not equal to $j$ and it is one when the class of the $i^{th}$ instance == $j$. To prevent extremities in the log function they also replace probabilities, $p$, with $p=\\max(\\min(p,1-10^{-15}),10^{-15})$\n",
      "\n",
      "Would this be easy to code in python? `scikit-learn` has an implementation for log loss, but it is not exactly what the competition uses and is only defined for binary classes."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# from the tutorial: http://www.kaggle.com/c/datasciencebowl/details/tutorial \n",
      "def multiclass_log_loss(y_true, y_pred, eps=1e-15):\n",
      "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
      "    https://www.kaggle.com/wiki/MultiClassLogLoss\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    y_true : array, shape = [n_samples]\n",
      "            true class, integers in [0, n_classes - 1)\n",
      "    y_pred : array, shape = [n_samples, n_classes]\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    loss : float\n",
      "    \"\"\"\n",
      "    predictions = np.clip(y_pred, eps, 1 - eps)\n",
      "\n",
      "    # normalize row sums to 1\n",
      "    predictions /= predictions.sum(axis=1)[:, np.newaxis]\n",
      "\n",
      "    actual = np.zeros(y_pred.shape)\n",
      "    n_samples = actual.shape[0]\n",
      "    actual[np.arange(n_samples), y_true.astype(int)] = 1\n",
      "    vectsum = np.sum(actual * np.log(predictions))\n",
      "    loss = -1.0 / n_samples * vectsum\n",
      "    return loss"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "___\n",
      "## How do you think you might increase the accuracy of the classifier(s)?\n",
      "- Search through parameters for models?\n",
      "- Try different classifiers?\n",
      "- Add more features (through better image processing)?\n",
      "\n",
      "___\n",
      "## How do you think we can make the algorithms more efficient for training/testing?\n",
      "- What about mini-batch training? \n",
      "- Sampling?\n",
      "- Map/Reduce (what are advantages/disadvantages)?\n",
      "- Buy a ton of memory on AWS virtual machines?\n",
      "\n",
      "**Note:** For mini-batch calculations (they are not really needed here because the dataset fits in memory) they can be accessed for a number of different classifiers (including SGDClassifier) by managing the sub-samples we send it, $X_{sub}$, and calling the function `partial_fit`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}